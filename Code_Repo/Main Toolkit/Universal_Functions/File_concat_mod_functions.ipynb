{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11612cd0-5706-450a-8c03-883b9f5c7d3c",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>File Read n Concat by Model/Var</h1>\n",
    "By: Kayla Besong, PhD\n",
    "    <br>\n",
    "Last Edited: 12/11/23\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "A suite of functions to return concatenated xarray datasets per variable per model.     \n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5078588a-20ee-4252-bbca-e79d72694182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0acedc5-df28-44a9-8f55-42409973a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_maker(path_name):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        os.mkdir(path_name)\n",
    "\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "        #print(f'{path_name} subdir exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eddc5f15-e3ac-4fee-8c24-f60b5aee0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(output_xr_dataset, output_path, filename):\n",
    "    \n",
    "    \n",
    "    if filename in os.listdir(output_path):\n",
    "\n",
    "        pass\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        output_xr_dataset.to_netcdf(f'{output_path}/{filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59609d5d-a23f-4e50-9c6d-402b7723bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk(model):\n",
    "    \n",
    "    model_chunks = {'CONUS404': {'Time': 1, 'south_north': 1015, 'west_east': 1367},\n",
    "                'ERA5':{'time': 1, 'longitude': 721, 'latitude': 341}, \n",
    "                'HRRR': {'y': 1059, 'x': 1799}, \n",
    "                'NAM': {'y': 428, 'x': 614}, \n",
    "                'NARR': {'x': 349, 'y': 277}, \n",
    "                'NCEP': {'time': 1, 'lat': 45, 'lon': 96}, \n",
    "                'UFS_S2S': {'latitude': 341, 'longitude': 720}}        \n",
    "    \n",
    "    return model_chunks[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57173dab-4f54-4348-8299-504f524b326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_database(model):\n",
    "    \n",
    "    model_chunks = {'CONUS404': {'Time': 1, 'south_north': 1015, 'west_east': 1367},\n",
    "                'ERA5':{'time': 1, 'longitude': 721, 'latitude': 341}, \n",
    "                'HRRR': {'time': 1, 'y': 1059, 'x': 1799}, \n",
    "                'NAM': {'time':1, 'y': 428, 'x': 614}, \n",
    "                'NARR': {'time': 1, 'x': 349, 'y': 277}, \n",
    "                'NCEP': {'time': 1, 'lat': 45, 'lon': 96}, \n",
    "                'UFS_S2S': {'latitude': 341, 'longitude': 720}}        \n",
    "    \n",
    "    return model_chunks[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a152ff22-1265-45a9-83a3-8a3b104bbf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_sFWD(model):\n",
    "    \n",
    "    model_chunks = {'CONUS404': {'time': 1, 'longitude': 2205, 'latitude': 1085},\n",
    "                'ERA5':{'time': 1, 'longitude': 721, 'latitude': 341}, \n",
    "                'HRRR': {'time': 1, 'latitude': 1165, 'longitude': 2710}, \n",
    "                'NAM': {'time':1, 'latitude': 455, 'longitude': 958}, \n",
    "                'NARR': {'time': 1, 'latitude': 281, 'longitude': 695}, \n",
    "                'NCEP': {'time': 1, 'latitude': 35, 'longitude': 96}, \n",
    "                'UFS_S2S': {'latitude': 341, 'longitude': 720}}        \n",
    "    \n",
    "    return model_chunks[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0404aeef-1924-4901-8d3c-0a4338ee1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(model):\n",
    "    \n",
    "    model_names = {'CONUS404': 'CONUS404_ANALYSIS',\n",
    "                'ERA5': 'ERA5_REANALYSIS', \n",
    "                'HRRR': 'HRRR_HISTORICAL', \n",
    "                'NAM': 'NAM_HISTORICAL', \n",
    "                'NARR': 'NARR_REANALYSIS', \n",
    "                'NCEP': 'NCEP_REANALYSIS_V2', \n",
    "                'UFS_S2S': 'UFS_S2S_FORECAST'}        \n",
    "    \n",
    "    return model_names[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e2ff457-dd15-4ba7-9702-e5ba96353e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_plot_colors(model):\n",
    "\n",
    "    line_plot_colors = {\n",
    "    \n",
    "        'CONUS404': 'dodgerblue',\n",
    "        'ERA5': 'mediumblue', \n",
    "        'HRRR': 'darkred', \n",
    "        'NAM': 'darkorange', \n",
    "        'NARR': 'lawngreen', \n",
    "        'NCEP': 'darkviolet', \n",
    "        'UFS_S2S': 'red' \n",
    "        \n",
    "    }\n",
    "\n",
    "    return line_plot_colors[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "530996d6-cf15-403f-9d7f-b4b14adfd5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_lat_lon_vars(model):\n",
    "    \n",
    "    if model == 'CONUS404':\n",
    "        lat_str = 'XLAT'\n",
    "        lon_str = 'XLONG'\n",
    "        \n",
    "    elif model == 'ERA5':\n",
    "        lat_str = 'latitude'\n",
    "        lon_str = 'longitude'\n",
    "        \n",
    "    elif model == 'NAM' or model == 'NARR' or model == 'HRRR':\n",
    "        lat_str = 'y'\n",
    "        lon_str = 'x'\n",
    "        \n",
    "    elif model == 'NCEP':\n",
    "        lat_str == 'lat'\n",
    "        lon_str == 'lon'\n",
    "        \n",
    "    else:\n",
    "        print(f'{model} is an invalid choice')\n",
    "\n",
    "    return lat_str, lon_str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a2e317-f7d3-45a5-b655-53de5e564fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_var(model, variable):    \n",
    "    \n",
    "    data =  {'CONUS404': {'PBL': 'PBLH', 'CAPE': ['SBCAPE', 'MLCAPE'], 'SOILM':'SMOIS', 'WIND': ['U10', 'V10'], 'PRECIP': 'PREC_ACC_NC', 'TEMP': 'T2', 'RH': 'TD2'},\n",
    "             'ERA5': {'PBL': 'blh', 'CAPE': 'cape', 'SOILM':'swvl1', 'WIND': ['u10', 'v10', 'i10fg'], 'PRECIP': ['tp', 'cp', 'lsrr'], 'TEMP': 't2m', 'RH': 'd2m'},\n",
    "             'HRRR': {'PBL': 'blh', 'CAPE': 'cape', 'SOILM':'mstav', 'WIND': ['u10', 'v10', 'gust'], 'PRECIP': ['prate', 'tp'], 'TEMP': 't2m', 'RH': 'd2m'},\n",
    "             'NAM': {'CAPE': 'cape', 'SOILM': ['sm', 'soilw'], 'WIND': ['u10', 'v10', 'gust'], 'PRECIP': 'tp', 'TEMP': 't2m', 'RH': ['r', 'r2'], 'HAINES': 'hindex'},\n",
    "             'NARR': {'PBL': 'Planetary_boundary_layer_height_surface', 'SOILM':'Soil_moisture_content_layer_between_two_depths_below_surface_layer', 'WIND': ['u-component_of_wind_height_above_ground', 'v-component_of_wind_height_above_ground'], 'PRECIP': ['Precipitation_rate_surface', 'Total_precipitation_surface_3_Hour_Accumulation'], 'TEMP': 'Temperature_height_above_ground', 'RH': 'Relative_humidity_height_above_ground'},\n",
    "             'NCEP': {'SOILM':'soilw.0-10cm.gauss', 'WIND': ['uwnd', 'vwnd'], 'PRECIP': 'prate.sfc.gauss', 'TEMP': 'air', 'RH': 'rhum'},\n",
    "             'UFS_S2S': {'PBL': 'hpbl', 'CAPE': 'cape', 'SOILM':'soilw', 'WIND': ['u10', 'v10', 'gust'], 'PRECIP': 'prate', 'TEMP': 't2m', 'RH': 'r2', 'HAINES': 'hindex'}}\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        return data[model][variable]\n",
    "        \n",
    "    except KeyError:\n",
    "        \n",
    "        return 0\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b130ca-733b-4e57-8a83-a25015f18a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_var_database(model, variable):    \n",
    "    \n",
    "    data =  {'CONUS404': {'PBL': 'PBLH', 'CAPE': ['SBCAPE', 'MLCAPE'], 'SOILM':'SMOIS', 'WIND_COMP': ['U10', 'V10'], 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd', 'PRECIP': 'PREC_ACC_NC', 'TEMP': 'T2', 'RH': 'rh'},\n",
    "             'ERA5': {'PBL': 'blh', 'CAPE': 'cape', 'SOILM':'swvl1', 'WIND_COMP': ['u10', 'v10', 'i10fg'], 'PRECIP': ['tp', 'cp', 'lsrr'], 'TEMP': 't2m', 'RH': 'rh', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "             'HRRR': {'PBL': 'blh', 'CAPE': 'cape', 'SOILM':'mstav', 'WIND': ['u10', 'v10', 'gust'], 'PRECIP': ['prate', 'tp'], 'TEMP': 't2m', 'RH': 'd2m', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "             'NAM': {'PBL': 'hpbl', 'CAPE': 'cape', 'SOILM': ['sm', 'soilw'], 'WIND_COMP': ['u10', 'v10', 'gust'], 'PRECIP': 'tp', 'TEMP': 't2m', 'RH': 'r', 'HAINES': 'hindex', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "             'NARR': {'PBL': 'Planetary_boundary_layer_height_surface', 'SOILM':'Soil_moisture_content_layer_between_two_depths_below_surface_layer', 'WIND_COMP': ['u-component_of_wind_height_above_ground', 'v-component_of_wind_height_above_ground'], 'PRECIP': ['Precipitation_rate_surface', 'Total_precipitation_surface_3_Hour_Accumulation'], 'TEMP': 'Temperature_height_above_ground', 'RH': 'Relative_humidity_height_above_ground', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "             'NCEP': {'SOILM':'soilw', 'WIND_COMP': ['uwnd', 'vwnd'], 'PRECIP': 'prate', 'TEMP': 'air', 'RH': 'rhum', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "             'UFS_S2S': {'PBL': 'hpbl', 'CAPE': 'cape', 'SOILM':'soilw', 'WIND_COMP': ['u10', 'v10', 'gust'], 'PRECIP': 'prate', 'TEMP': 't2m', 'RH': 'r2', 'HAINES': 'hindex', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'}}\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        return data[model][variable]\n",
    "        \n",
    "    except KeyError:\n",
    "        \n",
    "        return 0\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9458e10a-18fa-46dc-a9d0-7292095c7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_var_finalize(model, variable):\n",
    "\n",
    "    data = {\n",
    "\n",
    "    'CONUS404': {'PBL': 'PBLH', 'SBCAPE': 'SBCAPE', 'MLCAPE': 'MLCAPE','VSM':'SMOIS', 'U10': 'U10', 'V10': 'V10','WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi','VPD': 'vpd', 'TP': 'PREC_ACC_NC', 'TEMP': 'T2', 'RH': 'rh'},\n",
    "    'ERA5': {'PBL': 'blh', 'CAPE': 'cape', 'VSM':'swvl1','U10': 'u10', 'V10': 'v10', 'GUST': 'i10fg','TP': 'tp', 'TCP': 'cp', 'LSPR': 'lsrr','TEMP': 't2m', 'RH': 'rh', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "    'HRRR': {'PBL': 'blh', 'CAPE': 'cape', 'SOILM':'mstav','U10': 'u10', 'V10': 'v10', 'GUST': 'gust','PRATE': 'prate', 'TP': 'tp','TEMP': 't2m', 'DEWPOINT': 'd2m', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "    'NAM': {'PBL': 'hpbl', 'CAPE': 'cape', 'SOILM': 'sm', 'U10': 'u10', 'V10': 'v10', 'GUST': 'gust', 'TP': 'tp', 'TEMP': 't2m', 'RH': 'r', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "    'NARR': {'PBL': 'Planetary_boundary_layer_height_surface', 'SOILM':'Soil_moisture_content_layer_between_two_depths_below_surface_layer', 'U10': 'u-component_of_wind_height_above_ground', 'V10': 'v-component_of_wind_height_above_ground', 'PRATE': 'Precipitation_rate_surface', 'TP': 'Total_precipitation_surface_3_Hour_Accumulation','TEMP': 'Temperature_height_above_ground', 'RH': 'Relative_humidity_height_above_ground', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "    'NCEP': {'SOILM':'soilw', 'U10': 'uwnd', 'V10': 'vwnd', 'PRATE': 'prate', 'TEMP': 'air', 'RH': 'rhum', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'},\n",
    "    'UFS_S2S': {'PBL': 'hpbl', 'CAPE': 'cape', 'VSM':'soilw', 'U10': 'u10', 'V10': 'v10', 'GUST': 'gust', 'PRATE': 'prate', 'TEMP': 't2m', 'RH': 'r2', 'WINDSPEED': 'wspeed', 'WINDDIR': 'wdir', 'HDWI': 'hdwi', 'VPD': 'vpd'}}\n",
    "\n",
    "    try:\n",
    "        \n",
    "        return data[model][variable]\n",
    "        \n",
    "    except KeyError:\n",
    "        \n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28194f60-359c-4391-8017-8e6041090813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16155db6-8fb9-4207-90e4-e7ce593a896f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a387a4b-c26a-4408-b0eb-a2fcf71526d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_narr_lat_lon_index(northern_most, southern_most, western_most_deg_west, eastern_most_deg_west):\n",
    "\n",
    "    narr_lat_lon = pd.read_csv('NARR_LatLon.txt', skiprows=5, header=None, names=['lambert-i', 'lambert-j', 'lat(degN)', 'lon(degW)'], delim_whitespace=True)\n",
    "    \n",
    "    filtered_df = narr_lat_lon.loc[(narr_lat_lon['lat(degN)'] >= southern_most) & (narr_lat_lon['lat(degN)'] <= northern_most) & (narr_lat_lon['lon(degW)'] <= np.abs(western_most_deg_west)) & (narr_lat_lon['lon(degW)'] >= np.abs(eastern_most_deg_west))]\n",
    "\n",
    "    return filtered_df['lambert-i'].values, filtered_df['lambert-j'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d527d561-bc30-4eb7-bffd-9784b7e51834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nam_lat_lon_index(northern_most, southern_most, western_most_deg_west, eastern_most_deg_west):\n",
    "\n",
    "    nam_lat_lon = pd.read_csv('NAM_LatLon.txt', skiprows=3, header=None, names=['lambert-i', 'lambert-j', 'lat(degN)', 'lon(degW)'], delim_whitespace=True)\n",
    "    \n",
    "    filtered_df = nam_lat_lon.loc[(nam_lat_lon['lat(degN)'] >= southern_most) & (nam_lat_lon['lat(degN)'] <= northern_most) & (nam_lat_lon['lon(degW)'] <= np.abs(western_most_deg_west)) & (nam_lat_lon['lon(degW)'] >= np.abs(eastern_most_deg_west))]\n",
    "\n",
    "    return filtered_df['lambert-i'].values, filtered_df['lambert-j'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b6e910-2b0e-4489-af7d-e24bc26349d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_raw_files_and_resampler_UFS(var_list, output_dir, prototype):\n",
    "    \n",
    "    parent_dirs = glob.glob(os.path.join(f'{model}/{p}/', '*'))\n",
    "    model_save_str = get_filename('UFS_S2S')\n",
    "\n",
    "    if type(var_list) != list:\n",
    "        var_list = [var_list]\n",
    "\n",
    "    for i in parent_dirs:\n",
    "                \n",
    "        dir_maker(os.path.join(f'{output_dir}', f'{i}'))\n",
    "\n",
    "        for v in var_list:\n",
    "            print(f'starting {i} {v}')\n",
    "            \n",
    "            var_date_files = glob.glob(os.path.join(f'{i}/{v}', f'*{v}*'))\n",
    "\n",
    "            try:\n",
    "            \n",
    "                var_file_cc = xr.open_mfdataset(var_date_files, combine = 'nested', concat_dim = 'valid_time', chunks = get_chunk(model))\n",
    "                var_file_cc = var_file_cc.sortby('valid_time')\n",
    "                \n",
    "                model_abs = var_file_cc[v].resample(valid_time='24H').mean(dim='valid_time', skipna = True).to_dataset()\n",
    "                model_min = var_file_cc[v].resample(valid_time='24H').min(dim='valid_time', skipna = True).to_dataset()\n",
    "                model_max = var_file_cc[v].resample(valid_time='24H').max(dim='valid_time', skipna = True).to_dataset()\n",
    "                \n",
    "                \n",
    "                save_file(model_abs, f'{output_dir}/{i}', f'{v}AVG_{model_save_str}_Daily_{i[-8:]}.nc')\n",
    "                save_file(model_min, f'{output_dir}/{i}', f'{v}MIN_{model_save_str}_Daily_{i[-8:]}.nc')\n",
    "                save_file(model_max, f'{output_dir}/{i}', f'{v}MAX_{model_save_str}_Daily_{i[-8:]}.nc')\n",
    "                save_file(var_file_cc, f'{output_dir}/{i}', f'{v}_{model_save_str}_Abs_{i[-8:]}.nc')\n",
    "\n",
    "            except OSError:\n",
    "                print(f'{v} not available for {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c32215e-122e-4cee-9473-5bfea04a9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampler_UFS(v, var_file_cc, output_dir, i, prototype):\n",
    "     \n",
    "    model_save_str = get_filename('UFS_S2S')\n",
    "\n",
    "    try:\n",
    "\n",
    "        print(f'starting {i} {v}')\n",
    "               \n",
    "        model_abs = var_file_cc[v].resample(valid_time='24H').mean(dim='valid_time', skipna = True).to_dataset()\n",
    "        model_min = var_file_cc[v].resample(valid_time='24H').min(dim='valid_time', skipna = True).to_dataset()\n",
    "        model_max = var_file_cc[v].resample(valid_time='24H').max(dim='valid_time', skipna = True).to_dataset()\n",
    "                    \n",
    "        save_file(model_abs, f'{output_dir}/{i}', f'{v}AVG_{model_save_str}_Daily_{i[-8:]}.nc')\n",
    "        save_file(model_min, f'{output_dir}/{i}', f'{v}MIN_{model_save_str}_Daily_{i[-8:]}.nc')\n",
    "        save_file(model_max, f'{output_dir}/{i}', f'{v}MAX_{model_save_str}_Daily_{i[-8:]}.nc')\n",
    "        save_file(var_file_cc, f'{output_dir}/{i}', f'{v}_{model_save_str}_Abs_{i[-8:]}.nc')\n",
    "\n",
    "    except OSError:\n",
    "        print(f'{v} not available for {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682cc07e-61a7-4e43-bf3f-f0ad0a7e0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampler_regular_vars(var_list, model_out_var, output_dir, model):\n",
    "    \n",
    "\n",
    "    model_out_var = model_out_var.sortby('time')\n",
    "    \n",
    "    if type(var_list) != list:\n",
    "        var_list = [var_list]\n",
    "\n",
    "    for v in var_list:\n",
    "\n",
    "        print(f'Resampling {model}, {v}')\n",
    "\n",
    "\n",
    "        if model == 'CONUS404':\n",
    "\n",
    "            model_abs = model_out_var[v].resample(Time='24H').mean(dim='Time', skipna = True).to_dataset()\n",
    "            model_min = model_out_var[v].resample(Time='24H').min(dim='Time', skipna = True).to_dataset()\n",
    "            model_max = model_out_var[v].resample(Time='24H').max(dim='Time', skipna = True).to_dataset()\n",
    "            \n",
    "            years = np.unique(model_out_var['Time.year'].values)\n",
    "            \n",
    "            print(f'Saving {model}, {v}')\n",
    "            model_save_str = get_filename(model)\n",
    "            \n",
    "            for i in years:\n",
    "                \n",
    "                model_abs_year = model_abs.where(model_abs['Time.year'] == i, drop = True)\n",
    "                model_min_year = model_min.where(model_min['Time.year'] == i, drop = True)\n",
    "                model_max_year = model_max.where(model_max['Time.year'] == i, drop = True)\n",
    "                model_abs_all = model_out_var[v].to_dataset().where(model_out_var['Time.year'] == i, drop = True)\n",
    "                \n",
    "                save_file(model_abs_year, f'{output_dir}/{model}', f'{v}AVG_{model_save_str}_Daily_{i}.nc')\n",
    "                save_file(model_min_year, f'{output_dir}/{model}', f'{v}MIN_{model_save_str}_Daily_{i}.nc')\n",
    "                save_file(model_max_year, f'{output_dir}/{model}', f'{v}MAX_{model_save_str}_Daily_{i}.nc')\n",
    "                save_file(model_abs_all, f'{output_dir}/{model}', f'{v}_{model_save_str}_Abs_{i}.nc')                \n",
    "                \n",
    "\n",
    "        else:\n",
    "\n",
    "            model_abs = model_out_var[v].resample(time='24H').mean(dim='time', skipna = True).to_dataset()\n",
    "            model_min = model_out_var[v].resample(time='24H').min(dim='time', skipna = True).to_dataset()\n",
    "            model_max = model_out_var[v].resample(time='24H').max(dim='time', skipna = True).to_dataset()\n",
    "            \n",
    "            \n",
    "            years = np.unique(model_out_var['time.year'].values)\n",
    "            \n",
    "            print(f'Saving {model}, {v}')\n",
    "            model_save_str = get_filename(model)\n",
    "            \n",
    "            for i in years:\n",
    "                \n",
    "                model_abs_year = model_abs.where(model_abs['time.year'] == i, drop = True)\n",
    "                model_min_year = model_min.where(model_min['time.year'] == i, drop = True)\n",
    "                model_max_year = model_max.where(model_max['time.year'] == i, drop = True)\n",
    "                model_abs_all = model_out_var[v].to_dataset().where(model_out_var['time.year'] == i, drop = True)\n",
    "\n",
    "                \n",
    "                save_file(model_abs_year, f'{output_dir}/{model}', f'{v}AVG_{model_save_str}_Daily_{i}.nc')\n",
    "                save_file(model_min_year, f'{output_dir}/{model}', f'{v}MIN_{model_save_str}_Daily_{i}.nc')\n",
    "                save_file(model_max_year, f'{output_dir}/{model}', f'{v}MAX_{model_save_str}_Daily_{i}.nc')\n",
    "                save_file(model_abs_all, f'{output_dir}/{model}', f'{v}_{model_save_str}_Abs_{i}.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79d313b-fae0-4749-a250-4517b09f7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_min_max_vars(model, variable, output_dir):\n",
    "    \n",
    "    dir_maker(f'{output_dir}/{model}')\n",
    "    \n",
    "    print(f'Getting {model}, {variable} data and merging')\n",
    "       \n",
    "    var = get_model_var(model, variable)\n",
    "    \n",
    "    if var == False:\n",
    "        \n",
    "        print(f'{variable} is not available for {model}')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "\n",
    "        model_var_out = return_concat(model, variable)\n",
    "        \n",
    "        \n",
    "        if model == 'NCEP':\n",
    "\n",
    "            if var in ['air.2m.gauss', 'soilw.0-10cm.gauss', 'prate.sfc.gauss']:\n",
    "\n",
    "                var = list(model_var_out.variables)[-1]                                \n",
    "\n",
    "        if type(model_var_out) == list:\n",
    "          \n",
    "            for mvo in model_var_out:\n",
    "\n",
    "                var_list = list(np.intersect1d(var, list(mvo.variables)))\n",
    "\n",
    "                resampler_regular_vars(var_list, mvo, output_dir, model)\n",
    "          \n",
    "        else:\n",
    "            \n",
    "             resampler_regular_vars(var, model_var_out, output_dir, model)           \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b63e7f3-e770-4d89-aa40-430f8e8ed9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vapor_pressure_deficit(temp, rh):\n",
    "    \n",
    "    es = 611.2*np.exp((temp*17.67)/(temp+243.5))\n",
    "    vpd = (es - (rh*es))/100.\n",
    "    vpd.name = 'vpd'\n",
    "\n",
    "    return vpd.to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b19de5-ea4e-4ece-8794-f1adec1d0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_narr_handler(files, sub_open):\n",
    "    \n",
    "    if 'time1' in list(sub_open.coords):\n",
    "\n",
    "        sub_open = sub_open.rename({'time1': 'time'})\n",
    "\n",
    "    elif 'time2' in list(sub_open.coords):\n",
    "\n",
    "        sub_open = sub_open.rename({'time2': 'time'})\n",
    "\n",
    "    else: \n",
    "\n",
    "        pass\n",
    "\n",
    "    if 'reftime' in list(sub_open.coords):\n",
    "    \n",
    "        sub_open = sub_open.drop('reftime')\n",
    "\n",
    "        files.append(sub_open)\n",
    "\n",
    "    else:\n",
    "\n",
    "         files = files\n",
    "            \n",
    "    return files \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe861670-15b7-4eb9-b078-b645f11c7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_narr_handler2(sub_open):\n",
    "    \n",
    "    if 'time1' in list(sub_open.coords):\n",
    "\n",
    "        sub_open = sub_open.rename({'time1': 'time'})\n",
    "\n",
    "    elif 'time2' in list(sub_open.coords):\n",
    "\n",
    "        sub_open = sub_open.rename({'time2': 'time'})\n",
    "\n",
    "    else: \n",
    "\n",
    "        pass\n",
    "\n",
    "    if 'reftime' in list(sub_open.coords):\n",
    "    \n",
    "        sub_open = sub_open.drop('reftime')\n",
    "\n",
    "    else:\n",
    "\n",
    "         sub_open = sub_open\n",
    "            \n",
    "    return sub_open\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6483cfd2-f95d-47e6-8d95-44e652067eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2187749-f85c-4444-aa66-55db7d319993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_looper(v, model):\n",
    "    \n",
    "    files = []\n",
    "            \n",
    "    dir1 = f'{model}/{v}'\n",
    "    \n",
    "    for file in sorted(os.listdir(dir1)):\n",
    "\n",
    "        if file[-2:] == 'nc':\n",
    "\n",
    "            try:\n",
    "\n",
    "                sub_open = xr.open_dataset(f'{dir1}/{file}').chunk(get_chunk(model))\n",
    "\n",
    "                if model == 'NARR':\n",
    "\n",
    "                    files = special_narr_handler(files, sub_open)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    files.append(sub_open)\n",
    "\n",
    "            except:\n",
    "\n",
    "                print(f'{file} may be corrupt')  \n",
    "        \n",
    "    \n",
    "    print(f'concat step starting for {model}, {v}')\n",
    "    \n",
    "    if model == 'CONUS404':\n",
    "\n",
    "        concat_files = xr.concat(files, dim = 'Time')\n",
    "\n",
    "    else:\n",
    "\n",
    "        concat_files = xr.concat(files, dim = 'time')                \n",
    "\n",
    "    return concat_files\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbffd907-59ad-4220-896a-2a7a56adbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_concat(model, variable):\n",
    "            \n",
    "    var = get_model_var(model, variable)\n",
    "    \n",
    "    if var == False:\n",
    "        \n",
    "        print(f'{variable} is not available for {model}')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        if type(var) == list:\n",
    "\n",
    "            multi_vars = []\n",
    "\n",
    "            for v in var:\n",
    "\n",
    "                multi_vars.append(file_looper(v, model))\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                out_ds = xr.merge(multi_vars)\n",
    "                \n",
    "            except:\n",
    "\n",
    "                print(f'Multiple variables for {variable} were unable to be merged, returning a list')\n",
    "                out_ds = multi_vars\n",
    "        else:\n",
    "\n",
    "            out_ds = file_looper(var, model)                        \n",
    "                                                    \n",
    "        return out_ds             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b221ccd1-3717-4e93-87be-d3a52a7e9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_concat_years(file_list, model):\n",
    "\n",
    "    df1 = special_narr_handler2(xr.open_dataset(file_list[0]).chunk(get_chunk_database(model)))\n",
    "\n",
    "    if model == 'CONUS404':\n",
    "        dim_con = 'Time'\n",
    "    else:\n",
    "        dim_con = 'time'\n",
    "\n",
    "    for f in file_list[1:]:\n",
    "        \n",
    "        df2 = special_narr_handler2(xr.open_dataset(f).chunk(get_chunk_database(model)))\n",
    "        df1 = xr.concat([df1, df2], dim = dim_con)\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "035d56c1-8108-4413-bb39-62aa9f30a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_minmaxavgabs(output_dir, model, var):\n",
    "    \n",
    "    parent_dir = f'{output_dir}/{model}'\n",
    "    v = get_model_var_database(model, var)\n",
    "\n",
    "    if type(v) == list:\n",
    "        if var == 'PRECIP':\n",
    "            if model == 'ERA5':\n",
    "                v = v[0]\n",
    "            elif model == 'NARR':\n",
    "                v = v[1]\n",
    "            \n",
    "    min = sorted(glob.glob(os.path.join(parent_dir, f'{v}MIN*')))\n",
    "    max = sorted(glob.glob(os.path.join(parent_dir, f'{v}MAX*')))\n",
    "    avg = sorted(glob.glob(os.path.join(parent_dir, f'{v}AVG*')))\n",
    "    abs = sorted(glob.glob(os.path.join(parent_dir, f'{v}*Abs*')))\n",
    "    \n",
    "    print(f'concat min: {model}, {v}')\n",
    "    min_out = model_concat_years(min, model)\n",
    "    \n",
    "    print(f'concat max: {model}, {v}')\n",
    "    max_out = model_concat_years(max, model)\n",
    "    \n",
    "    print(f'concat avg: {model}, {v}')\n",
    "    avg_out = model_concat_years(avg, model)\n",
    "    \n",
    "    print(f'concat abs: {model}, {v}')\n",
    "    abs_out = model_concat_years(abs, model)\n",
    "    \n",
    "    return [min_out, max_out, avg_out, abs_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e2187-7bb1-492d-875d-af64026c6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def climo_n_anom(input_file, groupby_str, output_file_loc, var, gb):\n",
    "    \n",
    "#     print(f'processing climo and anomaly of {var}')\n",
    "    \n",
    "#     climo = input_file.groupby(groupby_str).mean('time')\n",
    "#     anom = input_file.groupby(groupby_str) - climo\n",
    "    \n",
    "#     print(f'saving climo of {var}')\n",
    "#     climo.to_netcdf(f'{output_file_loc}/processed_files/climos/climo_{var}_{gb}.nc')\n",
    "    \n",
    "#     print(f'saving anom of {var}')\n",
    "#     anom.to_netcdf(f'{output_file_loc}/processed_files/anoms/anom_{var}_{gb}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
