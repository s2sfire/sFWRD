{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8662b6-1653-4667-b8c3-6a34ec89d44e",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>Clean Data and Export to Absolute and 24H-Min, Max, and Avg netcdf files. .</h1>\n",
    "By: Kayla Besong, PhD\n",
    "    <br>\n",
    "Last Edited: 11/16/23\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "\n",
    "Downloaded files are cleaned and organized by model, variable, and year. The absolute (all time steps) 24H-Min, Max, and Avg are each exported by year, variable individually. If only min or absolute are desired, they will need commented out in the function suite notebook Example of each output file saved to local directory for one variable 'air' for NCEP:\n",
    "    \n",
    "air_NCEP_REANALYSIS_Abs_2020.nc  \n",
    "\n",
    "airMAX_NCEP_REANALYSIS_Daily_2020.nc \n",
    "\n",
    "airMIN_NCEP_REANALYSIS_Daily_2020.nc \n",
    "\n",
    "airAVG_NCEP_REANALYSIS_Daily_2020.nc \n",
    "\n",
    "NOTE: the NAM and HRRR may take some extra care/attention and using one year at a time may help runtime/memory.\n",
    "\n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c1627-38f4-4e1e-aab9-4cb234af55fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import needed libraries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dd037-c86b-4d25-a07e-c3a3bc023d77",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.array as da\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a18bb-3d23-4aef-b1fd-5c1f9accb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b1270-e278-4ec7-a92b-8b6d7870178e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Establish a dask client.\n",
    "\n",
    "Figure out the appropriate workers, threads, and memory limit for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c441ade-063a-4be5-82db-750c0da14994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Cluster = LocalCluster(n_workers = 8, threads_per_worker=4, memory_limit='30GB',  processes=True)\n",
    "client = Client(Cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df786ff4-2b0a-46c8-9308-78f62c372f90",
   "metadata": {},
   "source": [
    "# The integral notebook of functions to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb2dbb-ee27-440e-ad4a-703f1d7e8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Universal_Functions/File_concat_mod_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd375697-4ea6-4cf3-9f8d-f4df2120171f",
   "metadata": {},
   "source": [
    "## These are the available options the functions are designed for\n",
    "\n",
    "Not all variables will be available from all models but most are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc223d-d4a6-4745-9977-cf1f5fca2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_options = ['CONUS404', 'ERA5', 'HRRR', 'NAM', 'NARR', 'NCEP']\n",
    "variable_options = ['PBL', 'CAPE', 'SOILM', 'WIND', 'PRECIP', 'TEMP', 'RH']               # this references a dictionary in the function suite that will grab the correct variable name per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983c7f0-3646-447d-af05-6644cc96c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'database_files'           # choose wisely, this will be referenced significantly throughout the code repository "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbef68-2692-4f59-9710-acb242b7af31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a1a9d5-7929-46db-ac6b-ed692fd72050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for v in variable_options:                              # loop through variable options above\n",
    "    print(f'starting {v}')\n",
    "    save_min_max_vars('CONUS404', v, output_dir)        # call the function that concats, cleans, and generates the 4 output files per year per variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aed5c9-b00e-45c5-a3b1-843d149275f4",
   "metadata": {},
   "source": [
    "### plot checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824791b6-3ffb-4209-870c-823becf76082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "conus_outs = []\n",
    "for f in os.listdir(f'database_files/CONUS404/'):\n",
    "    if f[-2:] == 'nc':\n",
    "        conus_outs.append(xr.open_dataset(f'database_files/CONUS404/{f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef41379-8c1a-4fcd-b5a4-ff506c9a579d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in conus_outs:\n",
    "    cv = list(c.variables)\n",
    "    \n",
    "    if 'soil_layers_stag' in list(c[cv[0]].dims):\n",
    "        \n",
    "        c[cv[0]].isel(soil_layers_stag = 0).groupby('Time.season').mean('Time').plot.contourf(x = 'west_east', y = 'south_north', col = 'season', levels = 22)\n",
    "    \n",
    "    else:\n",
    "        c[cv[0]].groupby('Time.season').mean('Time').plot.contourf(x = 'west_east', y = 'south_north', col = 'season', levels = 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7a03c-66e8-4014-8b89-aa51df63eee0",
   "metadata": {},
   "source": [
    "# ERA5\n",
    "expected timesteps = 1460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b779a-a9e1-4f7b-a0ea-b7b4c094b984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for v in variable_options: \n",
    "    print(f'starting {v}')\n",
    "    save_min_max_vars('ERA5', v, output_dir)        # call the function that concats, cleans, and generates the 4 output files per year per variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc29df7-d0f2-4ed7-9e7a-1e52afae305f",
   "metadata": {},
   "source": [
    "# HRRR\n",
    "expected timesteps = 1460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee389cd-24cb-4315-98c7-8bfb9785142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'HRRR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f9c77-c2cb-4055-8671-ff6b6b271d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_hrrr = []                             \n",
    "\n",
    "for vvv in variable_options:              ### loop through and append variables, unnesting where there are multiple names for a variable (ex. wind: [u10, v10])\n",
    "    v = get_model_var(model, vvv)\n",
    "    if type(v) == list:\n",
    "        for vv in v:\n",
    "            vars_hrrr.append(vv)\n",
    "    else:\n",
    "        vars_hrrr.append(v)\n",
    "\n",
    "vars_hrrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b235d2b-1607-4e17-9746-45e1fe18d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(2014, 2018)\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae43fb-57fa-4701-8121-69eea252e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = xr.open_dataset('HRRR/t2m/hrrr_t2m_hrrr.20140801_00.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bde1aa-6b1d-4961-a608-9a2cc77082e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lon = temp.latitude[0,:].longitude.values\n",
    "y_lat = temp.longitude[:,0].latitude.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332120a-f2c8-4a0f-9d6b-b6e4ce3c7390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for v in vars_hrrr:                                                                          # Iterate over variables in the HRRR dataset\n",
    "    for y in years:                                                                          # Iterate over years in the dataset\n",
    "        \n",
    "             \n",
    "        year_model_list = sorted(glob.glob(os.path.join(f'{model}/{v}', f'*{y}*.nc')))       # Generate a sorted list of file paths for the current variable and year\n",
    "    \n",
    "        # Open the first dataset of the year for the variable, and chunk it\n",
    "        df1 = xr.open_dataset(year_model_list[0]).chunk(get_chunk(model))\n",
    "        to_keep = ['time', 'latitude', 'longitude']                                          # Define coordinates to keep\n",
    "        ds_coords = list(df1.coords)                                                         # List all coordinates in the dataset\n",
    "        delete = [i for i in ds_coords if i not in to_keep]                                  # Identify coordinates to delete\n",
    "        df1 = df1.drop(delete)                                                               # Drop unwanted coordinates\n",
    "            \n",
    "        for f in year_model_list[1:]:                                                        # Iterate over the rest of the files for the year\n",
    "                    \n",
    "            try:               \n",
    "                            \n",
    "                df2 = xr.open_dataset(f).chunk(get_chunk(model))                             # Open and chunk the next dataset \n",
    "                to_keep = ['time', 'latitude', 'longitude']                                  # Re-define coordinates to keep\n",
    "                ds_coords = list(df2.coords)                                                 # List all coordinates in the dataset\n",
    "                delete = [i for i in ds_coords if i not in to_keep]                          # Identify coordinates to delete\n",
    "                df2 = df2.drop(delete)                                                       # Drop unwanted coordinates\n",
    "                \n",
    "                df1 = xr.concat([df1, df2], dim = 'time').chunk(get_chunk_database(model))   # Concatenate the current dataset with the aggregated dataset along the time dimension\n",
    "            except:\n",
    "                print(f'{f} may be corrupt')                                                 # Print a message if the file may be corrupt\n",
    "               \n",
    "                       \n",
    "        lat = df1.latitude.isel(time=0).values                                               # Extract latitude and longitude values from the first time step\n",
    "        lon = df1.longitude.isel(time=0).values               \n",
    "        \n",
    "        df1 = df1.assign_coords(latitude=(('y', 'x'), lat), longitude=(('y', 'x'), lon))     # Assign these latitude and longitude values as coordinates        \n",
    "        df1 = df1.assign_coords({'x': x_lon, 'y': y_lat})                                    # Re-assign x and y coordinates\n",
    "        \n",
    "        \n",
    "        resampler_regular_vars(v, df1.chunk(get_chunk_database(model)), output_dir, model)   # Call the function that concats, cleans, and generates the 4 output files per year per variable \n",
    "\n",
    "    del df1                                                                                  # Delete df1 dataset to free memory\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8d772-9c90-4621-b945-86098b7a4d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "add15c37-e917-417e-a5ca-e8dd871f24ec",
   "metadata": {},
   "source": [
    "# NAM\n",
    "the nam is a special problem that cannot be handled like the rest of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32caa72b-52d5-4a36-b891-b9f807b5f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(2013, 2019, 1)\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22fa901-5215-46b2-b9ee-323c05e115bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'NAM'\n",
    "output_dir = 'database_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80268ec6-c82c-4679-b3bf-833dc8785758",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_nm = []\n",
    "\n",
    "for vvv in variable_options:\n",
    "    vars_nm.append(get_model_var(model, vvv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be88821-c8a7-4400-aed2-cf382fda9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67234ff1-471d-4e48-ac28-8204e8a05808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "       \n",
    "for v in vars_nm:                                                                               # Iterate over variables in the HRRR dataset\n",
    "    for y in years:                                                                          # Iterate over years in the dataset\n",
    "                     \n",
    "        year_model_list = sorted(glob.glob(os.path.join(f'{model}/{v}', f'*{y}*.nc')))       # Generate a sorted list of file paths for the current variable and year\n",
    "    \n",
    "        # Open the first dataset of the year for the variable, and chunk it\n",
    "        df1 = xr.open_dataset(year_model_list[0]).chunk(get_chunk(model))\n",
    "        to_keep = ['time', 'latitude', 'longitude']                                          # Define coordinates to keep\n",
    "        ds_coords = list(df1.coords)                                                         # List all coordinates in the dataset\n",
    "        delete = [i for i in ds_coords if i not in to_keep]                                  # Identify coordinates to delete\n",
    "        df1 = df1.drop(delete)                                                               # Drop unwanted coordinates\n",
    "            \n",
    "        for f in year_model_list[1:]:                                                        # Iterate over the rest of the files for the year\n",
    "                    \n",
    "            try:               \n",
    "                            \n",
    "                df2 = xr.open_dataset(f).chunk(get_chunk(model))                             # Open and chunk the next dataset \n",
    "                to_keep = ['time', 'latitude', 'longitude']                                  # Re-define coordinates to keep\n",
    "                ds_coords = list(df2.coords)                                                 # List all coordinates in the dataset\n",
    "                delete = [i for i in ds_coords if i not in to_keep]                          # Identify coordinates to delete\n",
    "                df2 = df2.drop(delete)                                                       # Drop unwanted coordinates\n",
    "                \n",
    "                df1 = xr.concat([df1, df2], dim = 'time').chunk(get_chunk_database(model))   # Concatenate the current dataset with the aggregated dataset along the time dimension\n",
    "            except:\n",
    "                print(f'{f} may be corrupt')                                                 # Print a message if the file may be corrupt\n",
    "                       \n",
    "        resampler_regular_vars(v, df1.chunk(get_chunk_database(model)), output_dir, model)   # Call the function that concats, cleans, and generates the 4 output files per year per variable \n",
    "\n",
    "    del df1                                                                                  # Delete df1 dataset to free memory            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90c1b7-fb4c-44ff-afe4-385009dc22e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e941f651-77f2-4493-944e-885ae5fe8afa",
   "metadata": {},
   "source": [
    "### adjusting the 2017-2018 gap\n",
    "\n",
    "This is an example of one variable for one file type (AVG) to handle the 2017 gap between grib and grib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39a16d-bbc9-4cd6-857a-3bb25339d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = xr.open_dataset('database_files/NAM/smAVG_NAM_HISTORICAL_Daily_2017.nc').chunk(get_chunk_database('NAM'))\n",
    "n2 = xr.open_dataset('database_files/NAM/soilwAVG_NAM_HISTORICAL_Daily_2017.nc').chunk(get_chunk_database('NAM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea426b3-2bd4-45e9-970a-3113b5557fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n2 = n2.rename({'soilw': 'sm'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f3dde-6e19-40e4-8eeb-cb17eaa182eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86958ca7-f15e-43c0-8249-981056df95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n3 = xr.concat([n1, n2], dim = 'time').chunk(get_chunk_database('NAM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d291a1-5697-43c9-baac-1755c51faa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635ead6-0e1c-4cf3-b235-f57ca16413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n3.to_netcdf('database_files/NAM/smAVG_NAM_HISTORICAL_Daily_2017.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1395728-e8d4-4a06-b3c1-a9a5de9947ee",
   "metadata": {},
   "source": [
    "### plot checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29a5a5-44d0-450d-b299-5ed182cb925f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nam_outs = []\n",
    "for f in os.listdir(f'database_files/NAM/'):\n",
    "    if f[-2:] == 'nc':\n",
    "        nam_outs.append(xr.open_dataset(f'database_files/NAM/{f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d462ba-3582-459a-8593-18fec33bd63a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for n in nam_outs:\n",
    "    nv = list(n.variables)\n",
    "    nd = list(n.dims)\n",
    "\n",
    "    if len(nd) > 3:        \n",
    "        n[nv[0]].isel(depthBelowLandLayer = 0).groupby('time.season').mean('time').plot.contourf(x = 'x', y = 'y', col = 'season', levels = 22)            \n",
    "\n",
    "    else:\n",
    "        n[nv[0]].groupby('time.season').mean('time').plot.contourf(x = 'x', y = 'y', col = 'season', levels = 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134ba81-758d-41e9-ad94-14f7e347cf4e",
   "metadata": {},
   "source": [
    "# NARR\n",
    "expected timesteps = 1460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d71df-472f-4ac2-bd22-3580d3f992f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for v in variable_options: \n",
    "    print(f'starting {v}')\n",
    "    save_min_max_vars('NARR', v, output_dir)        # call the function that concats, cleans, and generates the 4 output files per year per variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d70d4-54de-4b16-a02f-c45e5bb41cfe",
   "metadata": {},
   "source": [
    "plot checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae207583-0d14-471a-b14e-f5034fb41f83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "narr_outs = []\n",
    "for f in os.listdir(f'database_files/NARR/'):\n",
    "    if f[-2:] == 'nc':\n",
    "        narr_outs.append(xr.open_dataset(f'database_files/NARR/{f}'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffebd9-0cfc-4c9e-a4cb-71cf42731fcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vars_list = []\n",
    "for j in narr_outs:\n",
    "    variable_sel = list(j.variables)\n",
    "    #print(variable_sel)\n",
    "    k = j[variable_sel[0]]\n",
    "    kdims = list(k.dims)\n",
    "    # print(kdims)\n",
    "    if len(kdims) > 3:\n",
    "        if 'layer_between_two_depths_below_surface_layer' in kdims:\n",
    "            k.isel(layer_between_two_depths_below_surface_layer = 0).groupby('time.season').mean('time').plot.contourf(x = 'x', y = 'y', col = 'season', levels = 22)\n",
    "            \n",
    "        elif 'height_above_ground2' in kdims:\n",
    "            k.isel(height_above_ground2 = 0).groupby('time.season').mean('time').plot.contourf(x = 'x', y = 'y', col = 'season', levels = 22)\n",
    "            \n",
    "        elif 'height_above_ground1' in kdims:\n",
    "            k.isel(height_above_ground1 = 0).groupby('time.season').mean('time').plot.contourf(x = 'x', y = 'y', col = 'season', levels = 22)\n",
    "        \n",
    "        elif 'height_above_ground' in kdims:\n",
    "            k.isel(height_above_ground = 0).groupby('time.season').mean('time').plot.contourf(x = 'x', y = 'y', col = 'season', levels = 22)\n",
    "            \n",
    "\n",
    "    else:\n",
    "        k.groupby('time.season').mean('time').plot.contourf(x = 'x', y = 'y', col = 'season', levels = 22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2aa9c-5c8a-428a-b565-ae72293b2a86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NCEP\n",
    "expected timesteps = 1460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c40e23-98ea-4bee-a797-1286cccce255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for v in variable_options: \n",
    "    print(f'starting {v}')\n",
    "    save_min_max_vars('NCEP', v, output_dir)        # call the function that concats, cleans, and generates the 4 output files per year per variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cebd86-6925-4f66-8da7-987b6b3b8489",
   "metadata": {},
   "source": [
    "plot checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f2c60-00c1-4ee7-9ff2-4268959d872b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ncep_outs = []\n",
    "for f in os.listdir(f'database_files/NCEP'):\n",
    "    if f[-2:] == 'nc':\n",
    "        ncep_outs.append(xr.open_dataset(f'database_files/NCEP/{f}'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1c79f-69f5-4d19-a6a5-bd40afb40ef7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vars_list = []\n",
    "for j in ncep_outs:\n",
    "    variable_sel = list(j.variables)\n",
    "    \n",
    "    k = j[variable_sel[0]]\n",
    "    \n",
    "    if 'level' in list(k.dims):\n",
    "          \n",
    "        k.isel(level = 0).groupby('time.season').mean('time').plot.contourf(x = 'lon', y = 'lat', col = 'season', levels = 22)\n",
    "    else:\n",
    "        k.groupby('time.season').mean('time').plot.contourf(x = 'lon', y = 'lat', col = 'season', levels = 22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad7cd2-5737-48e7-89f2-61a7114dd4b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UFS\n",
    "expected timesteps = 1460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30692ea5-c5b5-4411-b07c-102489791303",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'UFS_S2S'\n",
    "p = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bae95-964d-4577-9d12-385707a42d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for v in variable_options:\n",
    "    var = get_model_var(model, v)\n",
    "    grab_raw_files_and_resampler_UFS(var, output_dir, p)        # call the function that concats, cleans, and generates the 4 output files per year per variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626d4d7-b493-4985-829c-c05efebc9189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
