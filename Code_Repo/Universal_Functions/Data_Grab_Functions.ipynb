{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5e1c78-6627-4b3a-96e4-0aeb5aff50f1",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>Data Download Functions</h1>\n",
    "By: Kayla Besong, PhD\n",
    "    <br>\n",
    "Last Edited: 11/15/23\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "This is a suite of functions complimentary to Data_Grab.ipynb. Keep in the same directory or point to correct location in Data_Grab.ipynb as needed. More complex packages needed are left within functions to help eliminate hurdles.     \n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a735acd-7035-436e-82d5-6c928d981e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import metpy\n",
    "from metpy.units import units\n",
    "import cartopy.crs as ccrs\n",
    "import glob\n",
    "import shutil\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acedc5-df28-44a9-8f55-42409973a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_maker(path_name):  # Define a function named dir_maker that takes a path_name as an argument\n",
    "\n",
    "    '''This function generates a directory with the given path name if it does not exist already.\n",
    "\n",
    "\n",
    "    Inputs:\n",
    "    path_name: (str) path and name of directory wishing to be created\n",
    "\n",
    "    Returns:\n",
    "    Nothing, directory created in background\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(path_name)  # Use mkdir method from os module to create a directory\n",
    "\n",
    "    except FileExistsError:\n",
    "        pass  # Handle the exception without doing anything\n",
    "        # Uncomment below to print a message if the directory already exists\n",
    "        #print(f'{path_name} subdir exists')  # Optional: print message if directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdb01ec-8964-4b61-92df-3b5182489f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrrr_zarr_grabber(start_date, end_date, hour_range, variables, output_dir, forecast = None):\n",
    "\n",
    "    '''This function takes user input for the start, stop, and step for the desired HRRR forecast or analysis data from AWS S3 buckets and downloads them locally to the given directory. \n",
    "    \n",
    "    Inputs:\n",
    "    start_date: (str) date to start loop featuring day, month, and year in any order using either / or - \n",
    "    end_date: (str) date to end loop featuring day, month, and year in any order using either / or -     \n",
    "    hour_range: (list) list of numerical timesteps desired. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. \n",
    "    variables: (list) list of strings of varibles to download. \n",
    "    output_dir: (str) path where to download the files locally.\n",
    "    forecast: (str, int, float, boolean) OPTIONAL. If set to anything other than None, the forecast file will be downloaded, if left alone anaylsis will be downloaded.  \n",
    "    \n",
    "    Returns:\n",
    "    non_exist: (list) list of strings of filenames that were not downloaded due to lack of availability. \n",
    "    \n",
    "    '''\n",
    "        \n",
    "    import s3fs                                                                                          # Import s3fs for working with S3 filesystems\n",
    "    import warnings                                                                                      # Import warnings module\n",
    "                                         \n",
    "    non_exist = []                                                                                       # Initialize an empty list to keep track of non-existent files\n",
    "                                         \n",
    "    s3 = s3fs.S3FileSystem(anon=True)                                                                    # Create an S3FileSystem object for anonymous access\n",
    "    def lookup(path):                                                                                    # Define a function to lookup paths in S3\n",
    "        return s3fs.S3Map(path, s3=s3)                                                                   # Return an S3Map object for the given path\n",
    "                                                    \n",
    "    dir_maker(output_dir)                                                                                # Call dir_maker function to create the output directory if not already created\n",
    "                                                    \n",
    "    hrs = [str(i).zfill(2) for i in hour_range]                                                          # Generate a list of hours with leading zeros\n",
    "                                         \n",
    "    for date in pd.date_range(start_date, end_date):                                                     # Iterate over dates in the specified range\n",
    "    \n",
    "        str_date = f'{date.year}{str(date.month).zfill(2)}{str(date.day).zfill(2)}'                      # Format date as a string            \n",
    "     \n",
    "        for hr in hrs:                                                                                   # Iterate over hours\n",
    "                                                            \n",
    "            for key, item in variables.items():                                                          # Iterate over variables\n",
    "                                                        \n",
    "                for v in item:                                                                           # Iterate over items in variables\n",
    "                                                                \n",
    "                    dir_maker(f'{output_dir}/{v}')                                                       # Create a subdirectory for each variable\n",
    "                                                                    \n",
    "                    if f'hrrr_{v}_{str_date}_{hr}.nc' in os.listdir(f'{output_dir}/{v}'):                # Check if file already exists\n",
    "                                          \n",
    "                        print(f'hrrr_{v}_{str_date}_{hr}.nc has already been saved')                     # Print message if file exists\n",
    "                        \n",
    "                    else:                                                                                # If file does not exist, proceed with downloading \n",
    "                        try:                                    \n",
    "                            if forecast != None:                                                         # Check if forecast is not None\n",
    "                                path = f\"hrrrzarr/sfc/{str_date}/{str_date}_{hr}z_fcst.zarr\"             # Set path for forecast data\n",
    "                            else:           \n",
    "                                path = f\"hrrrzarr/sfc/{str_date}/{str_date}_{hr}z_anl.zarr\"              # Set path for analysis data\n",
    "                                    \n",
    "                            file = xr.open_mfdataset([lookup(f\"{path}/{key}/{v}\"), lookup(f\"{path}/{key}/{v}/{key}/\")], engine=\"zarr\")  # Open multi-file dataset\n",
    "                            \n",
    "                            # Define projection parameters\n",
    "                            projection = ccrs.LambertConformal(central_longitude=262.5, \n",
    "                                   central_latitude=38.5, \n",
    "                                   standard_parallels=(38.5, 38.5),\n",
    "                                    globe=ccrs.Globe(semimajor_axis=6371229,\n",
    "                                                     semiminor_axis=6371229))\n",
    "    \n",
    "                            # Rename coordinates and assign CRS\n",
    "                            file3 = file.rename(projection_x_coordinate=\"x\", projection_y_coordinate=\"y\")\n",
    "                            file3 = file3.metpy.assign_crs(projection.to_cf())\n",
    "                            file3 = file3.metpy.assign_latitude_longitude()\n",
    "                            file3 = file3.drop('metpy_crs')\n",
    "                            file4 = file3.astype('float32')                                              # Convert data type to float32\n",
    "                            file4['time'] = file3['time'].values                                         # Assign time values\n",
    "                            \n",
    "                            file4.to_netcdf(f'{output_dir}/{v}/hrrr_{v}_{str_date}_{hr}.nc')             # Save file as NetCDF\n",
    "                            \n",
    "                        except FileNotFoundError:                                                        # Handle FileNotFoundError\n",
    "                            print(f'part or all of hrrr_{v}_{str_date}_{hr}.nc does not exist')          # Print error message\n",
    "                            non_exist.append(f'hrrr_{v}_{str_date}_{hr}.nc')                             # Add non-existent file to list\n",
    "    \n",
    "        print(f'all files for {date} have been saved')                                                   # Print message after saving all files for a date\n",
    "            \n",
    "    return non_exist                                                                                     # Return the list of non-existent files\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd13828-5ffa-4729-95c3-6278733b0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "\n",
    "    '''Downloads a blob from a Google Cloud bucket.\n",
    "\n",
    "    Inputs:\n",
    "    bucket_name: (str) name of the bucket \n",
    "    source_blob_name: (str) path, name of file \n",
    "    destination_file_name: (str) desired path, name of output file \n",
    "\n",
    "    Outputs:\n",
    "    None\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    from google.cloud import storage\n",
    "\n",
    "    storage_client = storage.Client.create_anonymous_client()    # Create an anonymous client for Google Cloud Storage\n",
    "    bucket = storage_client.bucket(bucket_name)                  # Access the bucket with the specified name\n",
    "    blob = bucket.blob(source_blob_name)                         # Create a blob object for the specified source blob name in the bucket\n",
    "    blob.download_to_filename(destination_file_name)             # Download the blob to a local file with the specified destination file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e9ce025-8535-4e37-833b-f08099f64b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrrr_google_grabber(start_date, end_date, hour_range, variables, output_dir, fcst_hr_step = None):\n",
    "\n",
    "\n",
    "    '''This function takes user input for the start, stop, and step for the desired HRRR forecast or analysis data from Google Cloud and downloads them locally to the given directory. \n",
    "    \n",
    "    Inputs:\n",
    "    start_date: (str) date to start loop featuring day, month, and year in any order using either / or - \n",
    "    end_date: (str) date to end loop featuring day, month, and year in any order using either / or -     \n",
    "    hour_range: (list) list of numerical timesteps desired. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. \n",
    "    variables: (list) list of strings of varibles to download. \n",
    "    output_dir: (str) path where to download the files locally.\n",
    "    fcst_hr_step: (list) list of integers describing the lead times included for each init time described by hour_range. If left None, ['00'] will be input. \n",
    "    \n",
    "    Returns:\n",
    "    non_exist: (list) list of strings of filenames that were not downloaded due to lack of availability. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    from google.cloud.exceptions import NotFound                                                                  # Import NotFound exception for handling missing blobs\n",
    "                                    \n",
    "    dir_maker(output_dir)                                                                                         # Create the main output directory                     \n",
    "    dir_maker('temp_hrrr_grib_files')                                                                             # Create a temporary directory for HRRR grib files\n",
    "                                    \n",
    "    non_exist = []                                                                                                # Initialize a list to track files that don't exist or fail to download    \n",
    "                                        \n",
    "    hrs = [str(i).zfill(2) for i in hour_range]                                                                   # Generate a list of hours with leading zeros\n",
    "                                        \n",
    "    if fcst_hr_step == None:                                                                                      # Check if forecast hour step is not specified\n",
    "        fcst = [str(0).zfill(2)]                                                                                  # Default to a forecast step of 0 if none specified\n",
    "    else:                                \n",
    "        fcst = [str(i).zfill(2) for i in fcst_hr_step]                                                            # Generate forecast steps if specified\n",
    "                                    \n",
    "    bucket_name = \"high-resolution-rapid-refresh\"                                                                 # Specify the GCS bucket name\n",
    "                                    \n",
    "    for date in pd.date_range(start_date, end_date):                                                              # Iterate over each date in the specified range                                \n",
    "        for hr in hrs:                                                                                            # Iterate over each hour\n",
    "            for f in fcst:                                                                                        # Iterate over each forecast step\n",
    "                str_date = 'hrrr.{dt:%Y%m%d}'.format(dt = date)                                                   # Format the date\n",
    "                filename = f'hrrr.t{hr}z.wrfnatf{f}.grib2'                                                        # Construct the filename for the grib file\n",
    "                not_saved = {}                                                                                    # Initialize a dictionary to track variables not saved\n",
    "                for key, item in variables.items():                                                               # Iterate over each variable\n",
    "                    not_saved_ar = []                                                                             # Initialize a list to track not saved variables\n",
    "                    for v in item[0]:                                                                             # Iterate over variables in the item\n",
    "                        dir_maker(f'{output_dir}/{v}')                                                            # Create a directory for each variable\n",
    "                        if f'hrrr_{v}_{str_date}_{hr}.nc' in os.listdir(f'{output_dir}/{v}'):                     # Check if file already exists\n",
    "                            pass                                                                                  # Do nothing if file exists\n",
    "                        else:                                \n",
    "                            not_saved_ar.append(v)                                                                # Add variable to not saved list if file doesn't exist\n",
    "                    if len(not_saved_ar) > 0:                                \n",
    "                        not_saved[key] = [not_saved_ar, item[1]]                                                  # Update not_saved dictionary if there are variables not saved\n",
    "                if len(not_saved) > 0:                                                                            # Check if there are any variables not saved\n",
    "                    try:                                \n",
    "                        source_blob_name = f\"{str_date}/conus/{filename}\"                                         # Construct the source blob name\n",
    "                        dir_maker(f'temp_hrrr_grib_files/{str_date}')                                             # Create a temporary directory for the date                        \n",
    "                        destination_file_name = f\"temp_hrrr_grib_files/{str_date}/{filename}\"                     # Set the destination file name\n",
    "                        download_blob(bucket_name, source_blob_name, destination_file_name)                       # Download the blob                                                \n",
    "                        for key, item in not_saved.items():                                                       # Iterate over not saved items\n",
    "                            # Open the dataset with specified filters\n",
    "                            ds = xr.open_dataset(destination_file_name, decode_times = True, engine = 'cfgrib', \n",
    "                                                 filter_by_keys= item[1], backend_kwargs={'indexpath': ''})\n",
    "                            for v in item[0]:                                                                     # Iterate over variables in the item\n",
    "                                try:                                \n",
    "                                    ds_out = ds[v]                                                                # Extract the variable from the dataset\n",
    "                                    ds[v].to_netcdf(f'{output_dir}/{v}/hrrr_{v}_{str_date}_{hr}.nc')              # Save the variable to a NetCDF file\n",
    "                                except KeyError:                                                                  # Handle missing variables\n",
    "                                    print(f'part or all of hrrr_{v}_{str_date}_{hr}.nc does not exist')           # Print error message\n",
    "                                    non_exist.append(f'hrrr_{v}_{str_date}_{hr}.nc')                              # Add to non_exist list\n",
    "                            try:\n",
    "                                del_path = '/'.join(destination_file_name.split('/')[0:-1])                       # Construct the path to the temporary directory\n",
    "                                shutil.rmtree(del_path)                                                           # Delete the temporary directory\n",
    "                            except:                \n",
    "                                print('error in deleting temp file')                                              # Print error message if deletion fails\n",
    "                    except (FileNotFoundError, OSError, NotFound) as e:                                           # Handle exceptions\n",
    "                        print(f'part or all of {filename} does not exist')                                        # Print error message\n",
    "                        non_exist.append(f'{filename}')                                                           # Add filename to non_exist list                          \n",
    "        print(f'all of {date} has been saved')                                                                    # Print message after processing each date\n",
    "    return non_exist                                                                                              # Return the list of files that don't exist or failed to download                                                                       \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead1ef66-5d9a-4c54-a60e-177be9138300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ufs_s2s_grabber(start_date, end_date, to_lead_time, variables, output_dir, domain, prototype, cached_location):\n",
    "\n",
    "\n",
    "    '''This function takes user input for the start, stop, and step for the desired UFS S2S forecast or analysis data from AWS S3 Buckets and downloads them locally to the given directory. \n",
    "    \n",
    "    Inputs:\n",
    "    start_date: (str) date to start loop featuring day, month, and year in any order using either / or - \n",
    "    end_date: (str) date to end loop featuring day, month, and year in any order using either / or -     \n",
    "    to_lead_time: (int) number to the lead time you want from 0 to 840 by 6. All hours between 0 and the number you provide will be downloaded.\n",
    "    variables: (list) list of strings of varibles to download. \n",
    "    output_dir: (str) path where to download the files locally.\n",
    "    domain: (list) list of integers --> [N, S, E, W] to trim the domain by\n",
    "    prototype: (int) integer representing the current prototype, choose from: 5, 6, 7, or 8 \n",
    "    cached_location: (str) path to directory where temp cache files can be downloaded and deleted \n",
    "    \n",
    "    Returns:\n",
    "    non_exist: (list) list of strings of filenames that were not downloaded due to lack of availability. \n",
    "    \n",
    "    '''\n",
    "\n",
    "    import s3fs\n",
    "    import fsspec\n",
    "    from cfgrib.dataset import DatasetBuildError\n",
    "    \n",
    "    non_exist = []                                                                                          # Initialize a list to track files that don't exist or fail to download\n",
    "                              \n",
    "    date_range = []                          \n",
    "                              \n",
    "    for d in pd.date_range(start_date, end_date, freq = 'M'):                                               # Iterate over each month in the specified date range\n",
    "        init_date = d.strftime('%Y-%m-01')                                                                  # Format and append the first day of the month to date_range\n",
    "        init_date2 = d.strftime('%Y-%m-15')                                                                 # Format and append the fifteenth day of the month to date_range\n",
    "        date_range.append(init_date)                          \n",
    "        date_range.append(init_date2)                          \n",
    "                                  \n",
    "    dir_maker('temp_ufs_files')                                                                             # Create a temporary directory for UFS files\n",
    "    dir_maker(output_dir)                                                                                   # Create the main output directory\n",
    "                              \n",
    "    p = prototype                                                                                           # Assign the prototype number to p\n",
    "                              \n",
    "    # for p in np.arange(5,9): (This loop is commented out and can be used to iterate over multiple prototypes)\n",
    "                              \n",
    "    dir_maker(os.path.join(output_dir, str(p)))                                                             # Create a directory for the prototype within the output directory\n",
    "                              \n",
    "    for init in date_range:                                                                                 # Iterate over each initialization date in date_range\n",
    "                                  \n",
    "        str_init = '{dt:%Y%m%d}'.format(dt = pd.to_datetime(init))                                          # Format the initialization date\n",
    "                                  \n",
    "        path_ufs_date = os.path.join(output_dir, str(p), str_init)                                          # Construct the path for the UFS data for the given date\n",
    "                              \n",
    "        dir_maker(path_ufs_date)                                                                            # Create a directory for the UFS data for the given date\n",
    "                                                         \n",
    "        lead_time = -6                                                                                      # Initialize lead time\n",
    "                              \n",
    "        while lead_time < to_lead_time:                                                                     # Loop over lead times until the specified to_lead_time\n",
    "                                      \n",
    "            lead_time += 6                                                                                  # Increment lead time by 6 hours\n",
    "                                      \n",
    "            if p == 5:                                                                                      # Check if prototype is 5\n",
    "                # Construct the URL for prototype 5                          \n",
    "                url = 'simplecache::s3://noaa-ufs-prototypes-pds/Prototype{p}/{dt:%Y%m%d}/pgrb2/gfs.{dt:%Y%m%d}/00/gfs.t00z.pgrb2.0p25.f{hour}'.format(p = p, dt = pd.to_datetime(init), hour = str(lead_time).zfill(3))                \n",
    "            else:\n",
    "                # Construct the URL for other prototypes\n",
    "                url = 'simplecache::s3://noaa-ufs-prototypes-pds/Prototype{p}/{dt:%Y%m%d}/pgrb2/gfs.{dt:%Y%m%d}/00/atmos/gfs.t00z.pgrb2.0p25.f{hour}'.format(p = p, dt = pd.to_datetime(init), hour = str(lead_time).zfill(3))                \n",
    "            \n",
    "            ### check if files exist first\n",
    "            \n",
    "            not_saved = {}                                                                                   # Initialize a dictionary to track variables not saved\n",
    "                                       \n",
    "            for key,item in variables.items():                                                               # Iterate over each variable\n",
    "                                           \n",
    "                not_saved_ar = []                                                                            # Initialize a list to track not saved variables\n",
    "                                           \n",
    "                for v in item[0]:                                                                            # Iterate over variables in the item\n",
    "                               \n",
    "                    sub_vars = []                                                                            # Initialize a list for sub-variables\n",
    "                                                       \n",
    "                    dir_maker(f'{path_ufs_date}/{v}')                                                        # Create a directory for each variable\n",
    "    \n",
    "                    if f'ufs_{p}_{v}_{str_init}_{lead_time}.nc' in os.listdir(f'{path_ufs_date}/{v}'):\n",
    "                        pass                                                                                 # Do nothing if file exists\n",
    "                        #print(f'ufs_{p}_{v}_{str_init}_{lead_time}.nc has already been saved') \n",
    "    \n",
    "                    else:\n",
    "                        not_saved_ar.append(v)                                                               # Add variable to not saved list if file doesn't exist\n",
    "                                                    \n",
    "                if len(not_saved_ar) > 0:                           \n",
    "                    not_saved[key] = [not_saved_ar, item[1]]                                                 # Update not_saved dictionary if there are variables not saved\n",
    "                    \n",
    "            if len(not_saved) > 0:\n",
    "    \n",
    "                try: \n",
    "                    # Create a file system using fsspec with filecache and s3 protocols\n",
    "                    fs = fsspec.filesystem(\"filecache\", target_protocol='s3', target_options={'anon': True}, cache_storage= cached_location)\n",
    "    \n",
    "                    # Open the file locally from the cache\n",
    "                    file = fsspec.open_local(url, s3={'anon': True}, simplecache={'cache_storage': cached_location})\n",
    "                    \n",
    "                    for key,item in not_saved.items():                                                      # Iterate over not saved items\n",
    "          \n",
    "                        # Open the dataset with specified filters\n",
    "                        ds = xr.open_dataset(file, decode_times = True, engine=\"cfgrib\", filter_by_keys= item[1], backend_kwargs={'indexpath': ''})\n",
    "    \n",
    "                        for v in item[0]:                                                                   # Iterate over variables in the item\n",
    "    \n",
    "                            try:\n",
    "                                # Select the data within the specified domain and save to a NetCDF file\n",
    "                                ds_out = ds[v].sel(latitude = slice(domain[0], domain[1]), longitude = slice(domain[2], domain[3]))\n",
    "                                ds_out.to_netcdf(f'{path_ufs_date}/{v}/ufs_{p}_{v}_{str_init}_{lead_time}.nc')\n",
    "    \n",
    "                            except KeyError:\n",
    "                                # Handle missing variables and print an error message\n",
    "                                print(f'part or all of ufs_{p}_{v}_{str_init}_{lead_time}.nc does not exist')\n",
    "                                non_exist.append(f'ufs_{p}_{v}_{str_init}_{lead_time}.nc')                  # Add to non_exist list\n",
    "                    try:   \n",
    "                        os.remove(file)                                                                     # Attempt to delete the cached file\n",
    "                    except:                         \n",
    "                        print('error in deleting cached file')                                              # Print error message if deletion fails\n",
    "    \n",
    "                except (FileNotFoundError, OSError) as e:\n",
    "                    # Handle exceptions for missing files and print an error message\n",
    "                    print(f'part or all of {url} does not exist')\n",
    "                    non_exist.append(f'{url}')                                                              # Add URL to non_exist list          \n",
    "                                                  \n",
    "        print(f'all files for Protype {p} {str_init} have been saved')                                      # Print message after processing each date\n",
    "                                      \n",
    "    return non_exist                                                                                        # Return the list of files that don't exist or failed to download                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891ad85b-829a-4921-a59c-b7e614516723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nam_grabber(start_date, end_date, hour_range, output_dir, variables_grib = None, variables_grib2 = None, fcst_hr_step = None):\n",
    "\n",
    "\n",
    "    '''This function takes user input for the start, stop, and step for the desired NAM forecast or analysis data from NCEI and downloads them locally to the given directory. \n",
    "    \n",
    "    Inputs:\n",
    "    start_date: (str) date to start loop featuring day, month, and year in any order using either / or - \n",
    "    end_date: (str) date to end loop featuring day, month, and year in any order using either / or -     \n",
    "    hour_range: (list) list of numerical timesteps desired. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. \n",
    "    variables_grib: (list) list of strings of variables for NAM files prior to 04-09-2017 when files were still in grib format\n",
    "    variables_grib2: (list) list of strings of variables for NAM files after to 04-08-2017 when files switched to grib2 format\n",
    "    fcst_hr_step: (list) list of integers describing the lead times included for each init time described by hour_range. If left None, ['00'] will be input. All available include: [0, 1, 2, 3, 6], not all available for all time periods/variables. \n",
    "    \n",
    "    Returns:\n",
    "    non_exist: (list) list of strings of filenames that were not downloaded due to lack of availability. \n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    import s3fs\n",
    "    import fsspec\n",
    "    from cfgrib.dataset import DatasetBuildError\n",
    "\n",
    "        \n",
    "    dir_maker(output_dir)                                                                          # Create the main output directory                     \n",
    "    dir_maker('temp_nam_files')                                                                    # Create a temporary directory for NAM files\n",
    "                      \n",
    "    non_exist = []                                                                                 # Initialize a list to track files that don't exist    \n",
    "                          \n",
    "    base_url = 'simplecache::https://www.ncei.noaa.gov/data/north-american-mesoscale-model/access/historical/analysis/'  # Base URL for NAM data\n",
    "                      \n",
    "    hrs = [str(i).zfill(2) for i in hour_range]                                                    # Generate a list of hours with leading zeros\n",
    "                      \n",
    "    if fcst_hr_step == None:                                                                       # Check if forecast hour step is not specified\n",
    "        fcst = [str(0).zfill(3)]                                                                   # Default to a forecast step of 0 if none specified\n",
    "    else:                  \n",
    "        fcst = [str(i).zfill(3) for i in fcst_hr_step]                                             # Generate forecast steps if specified\n",
    "                      \n",
    "    for date in pd.date_range(start_date, end_date):                                               # Iterate over each date in the specified range                                \n",
    "                      \n",
    "        for hr in hrs:                                                                             # Iterate over each hour\n",
    "                      \n",
    "            for f in fcst:                                                                         # Iterate over each forecast step\n",
    "    \n",
    "                str_date = '{dt:%Y%m%d}_{hr}_{f}'.format(dt = date, hr = hr, f = f)                # Format the date, hour, and forecast step                          \n",
    "    \n",
    "                if date >= pd.to_datetime('2017-04-9'):                                            # Check if date is on or after April 9, 2017\n",
    "                    # Construct filename for GRIB2 files\n",
    "                    filename = '{}{dt:%Y%m}/{dt:%Y%m%d}/namanl_218_{dt:%Y%m%d}_{hr}00_{f}.grb2'.format(base_url, dt=date, hr=hr, f = f)\n",
    "                    variables = variables_grib2                                                    # Use GRIB2 variables\n",
    "                else: \n",
    "                    # Construct filename for GRIB files\n",
    "                    filename = '{}{dt:%Y%m}/{dt:%Y%m%d}/namanl_218_{dt:%Y%m%d}_{hr}00_{f}.grb'.format(base_url, dt=date, hr=hr, f = f)\n",
    "                    variables = variables_grib                                                     # Use GRIB variables\n",
    "                    \n",
    "                not_saved = {}                                                                     # Initialize a dictionary to track variables not saved\n",
    "                    \n",
    "                for key,item in variables.items():                                                 # Iterate over each variable\n",
    "                    \n",
    "                    not_saved_ar = []                                                              # Initialize a list to track not saved variables\n",
    "                    \n",
    "                    for v in item[0]:                                                              # Iterate over variables in the item\n",
    "                    \n",
    "                        sub_vars = []                                                              # Initialize a list for sub-variables\n",
    "                    \n",
    "                        dir_maker(f'{output_dir}/{v}')                                             # Create a directory for each variable\n",
    "    \n",
    "                        if f'nam_{v}_{str_date}.nc' in os.listdir(f'{output_dir}/{v}'):            # Check if file already exists\n",
    "                            pass                                                                   # Do nothing if file exists\n",
    "                        else:                 \n",
    "                            not_saved_ar.append(v)                                                 # Add variable to not saved list if file doesn't exist\n",
    "                     \n",
    "                    if len(not_saved_ar) > 0:                 \n",
    "                        not_saved[key] = [not_saved_ar, item[1]]                                   # Update not_saved dictionary if there are variables not saved\n",
    "    \n",
    "                if len(not_saved) > 0:\n",
    "                    try:\n",
    "                        # Open the file locally from the cache\n",
    "                        file = fsspec.open_local(filename, filecache={'cache_storage': f'/temp_nam_files/'})                        \n",
    "                        for key,item in not_saved.items():                                         # Iterate over not saved items\n",
    "                            # Open the dataset with specified filters\n",
    "                            ds = xr.open_dataset(file, decode_times = True, engine = 'cfgrib', \n",
    "                                                 filter_by_keys= item[1], backend_kwargs={'indexpath': ''})\n",
    "                            for v in item[0]:                                                      # Iterate over variables in the item\n",
    "                                try:\n",
    "                                    ds_out = ds[v]                                                 # Extract the variable from the dataset\n",
    "                                    ds[v].to_netcdf(f'{output_dir}/{v}/nam_{v}_{str_date}.nc')     # Save the variable to a NetCDF file\n",
    "                                except KeyError:                                                   # Handle missing variables\n",
    "                                    print(f'part or all of nam_{v}_{str_date}.nc does not exist')  \n",
    "                                    non_exist.append(f'nam_{v}_{str_date}.nc')                     # Add to non_exist list\n",
    "                            try:                \n",
    "                                del_path = '/'.join(file.split('/')[0:-1])                         # Construct the path to the temporary directory\n",
    "                                shutil.rmtree(del_path)                                            # Delete the temporary directory\n",
    "                            except:                \n",
    "                                print('error in deleting cached file')                             # Print error message if deletion fails\n",
    "                    except (FileNotFoundError, OSError) as e:                \n",
    "                        print(f'part or all of {filename} does not exist')                         # Print error message\n",
    "                        non_exist.append(f'{filename}')                                            # Add filename to non_exist list                          \n",
    "                        \n",
    "        print(f'all of {date} has been saved')                                                     # Print message after processing each date\n",
    "                    \n",
    "    return non_exist                                                                               # Return the list of files that don't exist or failed to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b006f5-53f0-47f9-b87d-63539c4dd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def narr_grabber(start_date, end_date, hour_range, variables, output_dir):\n",
    "\n",
    "\n",
    "    '''This function takes user input for the start, stop, and step for the desired NARR reanalysis data from THREDDS and downloads them locally to the given directory. \n",
    "    \n",
    "    Inputs:\n",
    "    start_date: (str) date to start loop featuring day, month, and year in any order using either / or - \n",
    "    end_date: (str) date to end loop featuring day, month, and year in any order using either / or -     \n",
    "    hour_range: (list) list of numerical timesteps desired. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. \n",
    "    variables: (list) list of strings of varibles to download. \n",
    "    output_dir: (str) path where to download the files locally.\n",
    "    \n",
    "    Returns:\n",
    "    non_exist: (list) list of strings of filenames that were not downloaded due to lack of availability. \n",
    "    \n",
    "    '''\n",
    "\n",
    "    non_exist = []                                                                     # Initialize a list to track files that don't exist    \n",
    "    base_url = 'https://www.ncei.noaa.gov/thredds/dodsC/model-narr-a-files/'           # Base URL for NARR data    \n",
    "    \n",
    "    dir_maker(output_dir)                                                              # Create the main output directory                    \n",
    "        \n",
    "    hrs = [str(i).zfill(2) for i in hour_range]                                        # Generate a list of hours with leading zeros \n",
    "    \n",
    "    for date in pd.date_range(start_date, end_date):                                   # Iterate over each date in the specified range        \n",
    "    \n",
    "        for hr in hrs:                                                                 # Iterate over each hour\n",
    "            \n",
    "            str_date = '{dt:%Y%m%d}_{hr}'.format(dt = date, hr = hr)                   # Format the date and hour                          \n",
    "    \n",
    "            filename = '{}{dt:%Y%m}/{dt:%Y%m%d}/narr-a_221_{dt:%Y%m%d}_{hr}00_000.grb'.format(base_url, dt=date, hr=hr)  # Construct the filename for the data file\n",
    "    \n",
    "            try:\n",
    "                file = xr.open_dataset(filename, decode_times=True)                    # Open the dataset\n",
    "         \n",
    "                variables_available = list(file.variables)                             # Get a list of available variables in the dataset\n",
    "                variables_sel = list(set(variables_available) & set(variables))        # Select variables that are both available and requested\n",
    "         \n",
    "                for v in variables_sel:                                                # Iterate over selected variables\n",
    "                         \n",
    "                    dir_maker(f'{output_dir}/{v}')                                     # Create a directory for each variable             \n",
    "    \n",
    "                    if f'narr_{v}_{str_date}.nc' in os.listdir(f'{output_dir}/{v}'):   # Check if file already exists\n",
    "                        print(f'narr_{v}_{str_date}.nc has already been saved')        \n",
    "                    else:\n",
    "                        file[v].to_netcdf(f'{output_dir}/{v}/narr_{v}_{str_date}.nc')  # Save the variable to a NetCDF file\n",
    "            except (FileNotFoundError, OSError) as e:\n",
    "                print(f'part or all of {filename} does not exist')                     # Print error message\n",
    "                non_exist.append(f'{filename}')                                        # Add filename to non_exist list\n",
    "          \n",
    "        print(f'all of {date} has been saved')                                         # Print message after processing each date\n",
    "          \n",
    "    return non_exist                                                                   # Return the list of files that don't exist or failed to download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e71de7-73ad-4069-8e1a-3a39255bc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conus404_grabber(start_date, end_date, hour_range, variables, output_dir):\n",
    "\n",
    "    '''This function takes user input for the start, stop, and step for the desired CONUS404 reanalysis data from RDA and downloads them locally to the given directory. \n",
    "    \n",
    "    Inputs:\n",
    "    start_date: (str) date to start loop featuring day, month, and year in any order using either / or - \n",
    "    end_date: (str) date to end loop featuring day, month, and year in any order using either / or -     \n",
    "    hour_range: (list) list of numerical timesteps desired. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. \n",
    "    variables: (list) list of strings of varibles to download. \n",
    "    output_dir: (str) path where to download the files locally.\n",
    "    \n",
    "    Returns:\n",
    "    non_exist: (list) list of strings of filenames that were not downloaded due to lack of availability. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    non_exist = []                                                                        # Initialize a list to track files that don't exist\n",
    "    var_not_exist= []                                                                     # Initialize a list to track variables that don't exist\n",
    "            \n",
    "    base_url = 'https://thredds.rda.ucar.edu/thredds/dodsC/files/g/ds559.0'               # Base URL for the dataset    \n",
    "            \n",
    "    dir_maker(output_dir)                                                                 # Create the main output directory                          \n",
    "            \n",
    "    hrs = [str(i).zfill(2) for i in hour_range]                                           # Generate a list of hours with leading zeros \n",
    "            \n",
    "    for date in pd.date_range(start_date, end_date):                                      # Iterate over each date in the specified range\n",
    "                \n",
    "        for hr in hrs:                                                                    # Iterate over each hour\n",
    "                    \n",
    "            str_date = '{dt:%Y%m%d}_{hr}'.format(dt = date, hr = hr)                      # Format the date and hour\n",
    "                                                    \n",
    "            if date.month == 10 or date.month == 11 or date.month == 12:                  # Check if the month is Oct, Nov, or Dec\n",
    "                wy_str = f\"wy{date.year + 1}\"                                             # Set water year string for next year\n",
    "            else:        \n",
    "                wy_str = 'wy{dt:%Y}'.format(dt=date)                                      # Set water year string for current year   \n",
    "    \n",
    "            filename = '{}/{wy_str}/{dt:%Y%m}/wrf2d_d01_{dt:%Y-%m-%d}_{hr}:00:00.nc'.format(base_url, wy_str = wy_str, dt=date, hr=hr)  # Construct the filename for the data file\n",
    "    \n",
    "            try:\n",
    "                file = xr.open_dataset(filename)                                          # Open the dataset\n",
    "\n",
    "                variables_available = list(file.variables)                                # Get a list of available variables in the dataset\n",
    "                variables_sel = list(set(variables_available) & set(variables))           # Select variables that are both available and requested\n",
    "            \n",
    "                for v in variables_sel:                                                   # Iterate over selected variables\n",
    "                    dir_maker(f'{output_dir}/{v}')                                        # Create a directory for each variable             \n",
    "                    \n",
    "                    if f'conus404_{v}_{str_date}.nc' in os.listdir(f'{output_dir}/{v}'):  # Check if file already exists\n",
    "                        print(f'conus404_{v}_{str_date}.nc has already been saved')       \n",
    "                    \n",
    "                    else:                                                                 # Save if it is does not exist\n",
    "                        file[v].to_netcdf(f'{output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d735e-0de0-4560-ace2-6a46ec76c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncep_secret_attribute_remover(files):\n",
    "\n",
    "    '''This function takes an NCEP Renalysis file and removes an attribute that can become problematic in preprocessing\n",
    "    \n",
    "    Inputs:\n",
    "    files: (xarray dataset) opened NCEP file \n",
    "\n",
    "    Returns:\n",
    "    files: (xarray dataset) same as the input files with problematic attribute removed\n",
    "    \n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        del files.attrs['_NCProperties']              # Attempt to delete the '_NCProperties' attribute from the files' attributes\n",
    "\n",
    "    except KeyError:\n",
    "        pass                                          # If the key does not exist, do nothing\n",
    "    \n",
    "    return files                                      # Return the xarray dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e05d77-5e54-46bd-9eb3-b075b679f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncep_grabber(start_date, end_date, variables, output_dir, domain, levels = None):\n",
    "\n",
    "\n",
    "    '''This function takes user input for the start, stop, and step for the desired NCEP Renalysis II data from THREDDS and downloads them locally to the given directory. \n",
    "    \n",
    "    Inputs:\n",
    "    start_date: (str) date to start loop featuring day, month, and year in any order using either / or - \n",
    "    end_date: (str) date to end loop featuring day, month, and year in any order using either / or -     \n",
    "    variables: (list) list of strings of varibles to download. \n",
    "    output_dir: (str) path where to download the files locally.\n",
    "    domain: (list) list of integers --> [N, S, E, W] to trim the domain by\n",
    "    levels: (list) list of floats describing pressure levels. This only applies to variables on pressure surfaces. The default = None in the function which will return only the [1000] level for each year. \n",
    "\n",
    "    Returns:\n",
    "    non_exist: (list) list of strings of filenames that were not downloaded due to lack of availability. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    non_exist = []                                                                        # Initialize a list to track files that don't exist\n",
    "             \n",
    "    base_url = 'http://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis2'              # Base URL for the NCEP Reanalysis 2 dataset\n",
    "             \n",
    "    dir_maker(output_dir)                                                                 # Create the main output directory                                  \n",
    "             \n",
    "    if levels == None:         \n",
    "        levels = [1000]                                                                   # Default to 1000mb level if no levels are specified\n",
    "             \n",
    "    else:         \n",
    "        pass                                                                              # If levels are specified, do nothing\n",
    "    \n",
    "    years = np.unique([date.year for date in pd.date_range(start_date, end_date)])        # Generate a list of unique years within the date range\n",
    "    \n",
    "    for y in years:                                                                       # Iterate over each year                                                                \n",
    "        for key, item in variables.items():                                               # Iterate over each variable in the variables dictionary\n",
    "            for v in item:                                                                # Iterate over each item in the variable list\n",
    "                        \n",
    "                try:         \n",
    "                    os.mkdir(f'{output_dir}/{v}')                                         # Attempt to create a directory for the variable\n",
    "                        \n",
    "                except FileExistsError:         \n",
    "                    pass                                                                  # If the directory already exists, do nothing\n",
    "                    \n",
    "                # Generate file names to check based on the variable and year, including specific levels if applicable\n",
    "                file_names_to_check = [f'ncep_{v}_{y}.nc'] + [f'ncep_{v}_{y}_{int(l)}mb.nc' for l in levels]\n",
    "                \n",
    "                found = False                                                             # Initialize a flag to track if the file is found\n",
    "            \n",
    "                for file_name in file_names_to_check:                                     # Iterate over each file name to check\n",
    "                    if file_name in os.listdir(f'{output_dir}/{v}'):                      # Check if the file exists in the directory\n",
    "                        found = True                                                      # Set the flag to True if the file is found\n",
    "                        break                                                             # Break the loop if the file is found\n",
    "                            \n",
    "                if found:        \n",
    "                    print(f'{file_name} has already been saved')                          # Print message if the file has already been saved\n",
    "                        \n",
    "                else:        \n",
    "                    filename = f'{base_url}/{key}/{v}.{y}.nc'                             # Construct the filename for the dataset\n",
    "            \n",
    "                    try:        \n",
    "                        if key == 'pressure':                                             # Check if the key is 'pressure'\n",
    "                            for l in levels:                                              # Iterate over each level\n",
    "                                # Open the dataset, select the domain and level, remove secret attributes, and save to NetCDF\n",
    "                                files = ncep_secret_attribute_remover(xr.open_dataset(filename).sel(lat = slice(domain[0], domain[1]), lon = slice(domain[2], domain[3]), level = l))                                \n",
    "                                files.to_netcdf(f'{output_dir}/{v}/ncep_{v}_{y}_{int(l)}mb.nc')\n",
    "                        \n",
    "                        elif key == 'gaussian_grid':                                      # Check if the key is 'gaussian_grid'\n",
    "                            # Open the dataset, select the domain, remove secret attributes, and save to NetCDF\n",
    "                            files = ncep_secret_attribute_remover(xr.open_dataset(filename, chunks={'time': '100MB'}).sel(lat = slice(domain[0], domain[1]), lon = slice(domain[2], domain[3])))                       \n",
    "                            files.to_netcdf(f'{output_dir}/{v}/ncep_{v}_{y}.nc')\n",
    "                        \n",
    "                        else:\n",
    "                            try:\n",
    "                                # Attempt to open the dataset, select the domain, remove secret attributes, and save to NetCDF\n",
    "                                files = ncep_secret_attribute_remover(xr.open_dataset(filename, chunks={'time': '100MB'}).sel(lat = slice(domain[0], domain[1]), lon = slice(domain[2], domain[3])))                                                                                                                     \n",
    "                                files.to_netcdf(f'{output_dir}/{v}/ncep_{v}_{y}.nc')\n",
    "                            \n",
    "                            except RuntimeError:\n",
    "                                # Handle RuntimeError, likely due to the data being too large to request over DAP\n",
    "                                print(f'The data may be too large to request over DAP. You need subset the data more due to RuntimeError: NetCDF: DAP failure')\n",
    "                    except FileNotFoundError:\n",
    "                        print(f'part or all of {filename} does not exist')               # Print error message if the file does not exist\n",
    "                        non_exist.append(f'{filename}')                                  # Add the filename to the non_exist list\n",
    "          \n",
    "        print(f'ncep vars in {y} have been saved')                                       # Print message after saving variables for the year\n",
    "    \n",
    "    return non_exist                                                                     # Return the list of files that don't exist or failed to download\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713cf34-ed03-4061-8ddc-296294d886c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def era_grabber_client(year, variables, hours, output_dir, domain):\n",
    "\n",
    "     '''This function takes user input from the era_grabber function and establishes a connection with the COPERNICUS API to download from.\n",
    "     It is an altered version that is featured on their webpage while selecting and exporting data from the on site interface. \n",
    "     \n",
    "     Correct installation of cdsapi and your personal token is required prior to this step. \n",
    "    \n",
    "    '''  \n",
    "    \n",
    "    import cdsapi\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'format': 'netcdf',\n",
    "        'variable': variables,\n",
    "            \n",
    "            'month': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "            ],\n",
    "            'year': [str(year)],\n",
    "            'day': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "                '13', '14', '15',\n",
    "                '16', '17', '18',\n",
    "                '19', '20', '21',\n",
    "                '22', '23', '24',\n",
    "                '25', '26', '27',\n",
    "                '28', '29', '30',\n",
    "                '31',\n",
    "            ],\n",
    "            'time': hours,\n",
    "            \n",
    "            'area': [domain[0], domain[2], domain[1], domain[3]],\n",
    "\n",
    "        },\n",
    "        \n",
    "        f'{output_dir}/era5_{year}.nc')\n",
    "\n",
    "    print(f'era5_{year}.nc has finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf69c40-40b4-44ca-ad5a-9dc765e1cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def era_grabber(start_date, end_date, hour_range, variables, output_dir, domain):\n",
    "\n",
    "    '''This function takes user input for the start, stop, and step for the desired ERA5 Renalysis from COPERNICUS and \n",
    "    downloads them locally using multiprocess to the given directory. \n",
    "    \n",
    "    Inputs:\n",
    "    start_date: (str) date to start loop featuring day, month, and year in any order using either / or - \n",
    "    end_date: (str) date to end loop featuring day, month, and year in any order using either / or -     \n",
    "    hour_range: (list) list of numerical timesteps desired. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. \n",
    "    variables: (list) list of strings of varibles to download. \n",
    "    output_dir: (str) path where to download the files locally.\n",
    "    domain: (list) list of integers --> [N, S, E, W] to trim the domain by\n",
    "\n",
    "    Returns:\n",
    "    non_exist: (list) list of strings of filenames that were not downloaded due to lack of availability. \n",
    "    \n",
    "    '''   \n",
    "    \n",
    "    import multiprocess\n",
    "    from multiprocess.pool import Pool\n",
    "    from contextlib import closing\n",
    "                \n",
    "    dir_maker(output_dir)                                                                          # Create the main output directory                          \n",
    "                      \n",
    "    hours = [f'{str(i).zfill(2)}:00' for i in hour_range]                                          # Generate a list of hours in 'HH:00' format\n",
    "                      \n",
    "    end_year = pd.to_datetime(end_date).year                                                       # Extract the end year from the end_date\n",
    "                      \n",
    "    years = np.arange(pd.to_datetime(start_date).year, end_year+1)                                 # Generate an array of years from start_date to end_date\n",
    "    with multiprocess.get_context(\"spawn\").Pool() as pool:                                         # Create a multiprocessing pool using the \"spawn\" context\n",
    "        for year in years[::4]:                                                                    # Iterate over years in groups of 4\n",
    "            subyears = [y for y in np.arange(year, year+4) if y <= end_year]                       # Create a list of subyears within the 4-year range, ensuring it doesn't exceed end_year\n",
    "            print(subyears)                                                                        \n",
    "            input_data = [(year, variables, hours, output_dir, domain) for year in subyears]       # Prepare input data for each subyear\n",
    "            pool.starmap(era_grabber_client, input_data)                                           # Use starmap to apply era_grabber_client function to the input data in parallel, by year in groups of 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011e134-682a-4e86-9057-d3c26f29be75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
