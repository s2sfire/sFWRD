{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b140edb1-8393-46a8-b28d-2eded826ddac",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>Calculate the Multi-Prototype Mean, Stack Along Lead Time for the UFS-S2S Database Files</h1>\n",
    "By: Kayla Besong, PhD\n",
    "    <br>\n",
    "Last Edited: 01/09/24\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "Calculates the mean along forecast time of prototypes 5-8 of the UFS-S2S forecast model. There is also methodology to group forecasts by lead day and lead week, saving out files accordingly by variable. This notebook leverages already generated database files, structured in sub folders by init dates. \n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef7317-6a3b-47f9-8906-f08cc51cce29",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import needed libraries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b84ccc4-8f05-4994-91d8-58d5ff467e4c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.array as da\n",
    "import os\n",
    "import glob\n",
    "from metpy.units import units\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9e7312-268a-423c-a7b8-3bf2af5d7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d003c-d9a2-440d-bc73-d3883c79c4f6",
   "metadata": {},
   "source": [
    "### The integral notebook of functions to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846321ae-ee36-4e59-a2f5-4b792dd66ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run File_concat_mod_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb9154e2-d38e-41f4-93ad-0a271d4f80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'database_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db34ab46-d499-4df4-9b9e-3e1076c9ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars = os.listdir('UFS_S2S/5/20110401/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3ddba2-286e-451e-a07d-a9f3e18760ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t2m', 'r2', 'u10', 'v10', 'gust', 'cape', 'prate', 'soilw', 'wdir', 'wspeed']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vars = [i for i in input_vars if i != 'hindex']\n",
    "input_vars = [i for i in input_vars if i != 'hpbl']\n",
    "input_vars = [i for i in input_vars if i != 'lsm']\n",
    "\n",
    "input_vars.append('wdir')\n",
    "input_vars.append('wspeed')\n",
    "input_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd98f201-35c8-4a4a-92e3-54eb16a15858",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'UFS_S2S'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3315ac9-0a89-4baf-a95e-d979d9609e81",
   "metadata": {},
   "source": [
    "# Grab files and take mean of all prototypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95665d6-e9b2-44c0-8404-5a23be73218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dirs_5 = sorted(glob.glob(os.path.join(f'{model}/5/', '*')))\n",
    "parent_dirs_6 = sorted(glob.glob(os.path.join(f'{model}/6/', '*')))\n",
    "parent_dirs_7 = sorted(glob.glob(os.path.join(f'{model}/7/', '*')))\n",
    "parent_dirs_8 = sorted(glob.glob(os.path.join(f'{model}/8/', '*')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6bb07-58b3-4f88-bcd4-c7557f6ff8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(parent_dirs_5)):                                                                                                                       # Iterate over the indices of parent directories for prototype 5\n",
    "                                                                                                                                                          # Check if the last 8 characters of the directory names match across all prototypes\n",
    "    if parent_dirs_5[i][-8:] == parent_dirs_6[i][-8:] == parent_dirs_7[i][-8:] == parent_dirs_8[i][-8:]:\n",
    "                                                                                                                       \n",
    "        dir_maker(os.path.join(f'{output_dir}/UFS_S2S/MPM', f'{parent_dirs_5[i][-8:]}'))                                                                  # Create a directory for the matched subfolder within the MPM directory\n",
    "        for v in range(len(input_vars)):                                                                                                                  # Iterate over the indices of input variables\n",
    "            # Open the dataset for each prototype, select the variable, and chunk the data\n",
    "            p5_var = xr.open_dataset(glob.glob(os.path.join(f'{output_dir}/{parent_dirs_5[i]}/', f'*{input_vars[v]}*Abs*'))[0]).chunk(get_chunk(model))\n",
    "            p6_var = xr.open_dataset(glob.glob(os.path.join(f'{output_dir}/{parent_dirs_6[i]}/', f'*{input_vars[v]}*Abs*'))[0]).chunk(get_chunk(model))\n",
    "            p7_var = xr.open_dataset(glob.glob(os.path.join(f'{output_dir}/{parent_dirs_7[i]}/', f'*{input_vars[v]}*Abs*'))[0]).chunk(get_chunk(model))\n",
    "            p8_var = xr.open_dataset(glob.glob(os.path.join(f'{output_dir}/{parent_dirs_8[i]}/', f'*{input_vars[v]}*Abs*'))[0]).chunk(get_chunk(model))\n",
    "            \n",
    "            merged = xr.concat([p5_var, p6_var, p7_var, p8_var], dim = 'prototype')                                                                       # Concatenate the datasets along a new 'prototype' dimension\n",
    "            mpm = merged.mean('prototype')                                                                                                                # Calculate the mean across the 'prototype' dimension\n",
    "            \n",
    "            resampler_UFS(input_vars[v], mpm.chunk(get_chunk(model)), f'{output_dir}/UFS_S2S/MPM', parent_dirs_5[i][-8:], 'MPM')                          # Resample the mean dataset and save it\n",
    "    else:\n",
    "        print('the subfolders for each prototype are not aligned')                                                                                        # Print a message if the subfolder names do not match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932ea90-0d15-4d83-8b57-96a100189bfd",
   "metadata": {},
   "source": [
    "# Stack along lead time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b2ffa3-f1da-443b-8126-131cce9eb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_type = ['Abs', 'MAX', 'MIN', 'AVG']\n",
    "#ps = [5, 6, 7, 8, 'MPM']\n",
    "ps = [7, 8, 'MPM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a49f66-a162-417c-93d2-46dbd5b4ae4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t2m', 'r2', 'u10', 'v10', 'gust', 'cape', 'prate', 'soilw', 'wdir', 'wspeed']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fe700-1808-4494-9e5e-ffe777471df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for p in ps:                                                                                                                                # Iterate over each prototype\n",
    "    for dft in data_file_type:                                                                                                              # Iterate over each data file type\n",
    "        for v in range(len(input_vars)):                                                                                                    # Iterate over each variable by index\n",
    "\n",
    "            print(f'Prototype: {p} - File Type: {dft} - Variable: {input_vars[v]}')\n",
    "                   \n",
    "            sub_fl = sorted(glob.glob(f\"{output_dir}/{model}/{p}/*/*{input_vars[v]}*{dft}*\"))                                               # Generate a sorted list of file paths matching the current prototype, variable, and data file type\n",
    "\n",
    "            day_dict = {}                                                                                                                   # Initialize a dictionary to store daily data\n",
    "            week_dict = {}                                                                                                                  # Initialize a dictionary to store weekly data\n",
    "                                                              \n",
    "            ct = 0                                                                                                                          # Initialize a counter\n",
    "                                                              \n",
    "            for f in sub_fl:                                                                                                                # Iterate over each file in the sorted list\n",
    "                                                              \n",
    "                df1 = xr.open_dataset(f).chunk(get_chunk(model))                                                                            # Open the dataset and chunk it according to the model\n",
    "                \n",
    "                df1['lead_days'] = xr.DataArray(pd.to_timedelta((df1['valid_time'] - df1['time']).values).days)                             # Calculate lead days and weeks from valid_time and time, then add as data arrays\n",
    "                df1['lead_week'] = df1['lead_days'] // 7\n",
    "                df1 = df1.rename({'dim_0': 'lead_dim'})                                                                                     # Rename 'dim_0' to 'lead_dim'\n",
    "                                                                                              \n",
    "                for wd in np.unique(df1.lead_dim):                                                                                          # Iterate over unique lead dimensions\n",
    "                                                                                  \n",
    "                    # Select data for the current lead day\n",
    "                    selected_group_day = df1.groupby('lead_days').groups[wd]    \n",
    "                    selected_valid_times_day = df1.isel(valid_time=selected_group_day).drop(['lead_days', 'lead_week'])\n",
    "                    selected_valid_times_day = selected_valid_times_day.rename({'lead_dim': 'lead_days'}).sel(lead_days = wd)\n",
    "\n",
    "                    if ct == 0:                                                                                                             # If first iteration, initialize the dictionary with the data\n",
    "                        day_dict[wd] = selected_valid_times_day                                                              \n",
    "                    else:                                                                                                                   # Otherwise, concatenate the new data with existing data\n",
    "                        day_dict[wd] = xr.concat([day_dict[wd], selected_valid_times_day], dim = 'valid_time')\n",
    "                    \n",
    "                    if wd in np.arange(0,6):                                                                                                # If within the first 6 lead dimensions (weeks)\n",
    "                        # Select data for the current lead week\n",
    "                        selected_group_wk = df1.groupby('lead_week').groups[wd]    \n",
    "                        selected_valid_times_wk = df1.isel(valid_time=selected_group_wk).drop(['lead_days', 'lead_week'])                      \n",
    "                        selected_valid_times_wk['lead_dim'] = selected_valid_times_wk['lead_dim'] // 7\n",
    "                        selected_valid_times_wk = selected_valid_times_wk.rename({'lead_dim': 'lead_week'}).sel(lead_week = wd)\n",
    "\n",
    "                        if ct == 0:                                                                                                         # If first iteration, initialize the dictionary with the data\n",
    "                            week_dict[wd] = selected_valid_times_wk                                                              \n",
    "                        else:                                                                                                               # Otherwise, concatenate the new data with existing data\n",
    "                            week_dict[wd] = xr.concat([week_dict[wd], selected_valid_times_wk], dim = 'valid_time')\n",
    "\n",
    "                ct += 1                                                                                                                     # Increment the counter\n",
    "                                                              \n",
    "            for wd in np.unique(df1.lead_dim):                                                                                              # Iterate over unique lead dimensions again\n",
    "                                                              \n",
    "                str_wd = str(wd).zfill(2)                                                                                                   # Format the lead dimension with leading zeros\n",
    "                \n",
    "                day_dict[wd].to_netcdf(f\"{output_dir}/{model}/LEAD/{p}/day/{input_vars[v]}_UFS_S2S_FORECAST_{dft}_lead{str_wd}.nc\")         # Save the daily data to NetCDF\n",
    "\n",
    "                if wd in np.arange(0,6):                                                                                                    # If within the first 6 lead dimensions (weeks)\n",
    "                    \n",
    "                    week_dict[wd].to_netcdf(f\"{output_dir}/{model}/LEAD/{p}/week/{input_vars[v]}_UFS_S2S_FORECAST_{dft}_lead{str_wd}.nc\")   # Save the weekly data to NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e184acb-cb2d-4d59-9998-5bfd2e2ae46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
