{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a548ed0-bf70-4a36-a1ad-5640bdecc9f7",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>Calculate Surface-Based Hot Dry Windy for Each Model and Timestep</h1>\n",
    "By: Kayla Besong, PhD\n",
    "    <br>\n",
    "Last Edited: 11/29/23\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "Takes models/variables downloaded and calculates surface based hot-dry-windy. The hot-dry-windy calculation uses vapor pressure deficit and multiplies by windspeed, hence leveraging previously calculated variables. The function that computes the 24HR AVG, MIN, MAX outputs is in File_concat_mod_functions.ipynb. \n",
    "<br>\n",
    "<br>\n",
    "NOTE: The operational and 'true' hot-dry-windy index (HDWI) is not computed at the surface, rather it involves analyzing vpd and windspeed in the lowest 500m of the atmosphere and is  more computationally intensive. The 'true' HDWI also takes the max value of the day. Here, by just multiplying vpd by windspeed at the surface, the resulting product is a 'surface-based-hot-dry-windy'. The difference between surface based HDW and the HDWI can be stark depending on the region you are analyzing. Please see: (Kramer et al., 2024; Watts et al., 2020).\n",
    "<br>\n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f85ca-8388-4c33-bb77-ded0c5e1552e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import needed libraries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e3115-63fa-4d33-be39-3d71d8b32060",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.array as da\n",
    "import os\n",
    "import glob\n",
    "from metpy.units import units\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e41eb-7e5b-47e8-b872-5752dc9ce96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef8e74e-c6b4-4418-beca-47bb69848ba5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Establish a dask client. This is a lot of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60312341-f84a-49a8-87e0-b4e9a87afb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Cluster = LocalCluster(n_workers = 8, threads_per_worker=4, memory_limit='30GB',  processes=True)\n",
    "#Cluster = LocalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe703ac9-e1d9-4390-9413-beec4c29f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(Cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef59df-1744-49f6-930f-bce1780f93f6",
   "metadata": {},
   "source": [
    "### The integral notebook of functions to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20149960-4696-447d-8447-480b8d815272",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run File_concat_mod_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f133142-6a8f-4cf3-81de-075094f58b73",
   "metadata": {},
   "source": [
    "## The HDWI function, variables, models, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98a4a4-ec9a-487a-9cb5-e3507824bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdwi(model, main_dir):\n",
    "\n",
    "    '''\n",
    "\n",
    "    This function generates the surface-based hot-dry-windy by multiplying preexisting vpd and windspeed. Path naming convention may need altered. \n",
    "\n",
    "    Inputs:\n",
    "    \n",
    "    model: (str) model name, used as file path \n",
    "    main_dir: (str) the directory that contains the model data \n",
    "\n",
    "    Outputs:\n",
    "    \n",
    "    Nothing, files are saved to pointed directory.  \n",
    "\n",
    "    '''   \n",
    "                               \n",
    "    model_list = []                                                                                                     # Initialize an empty list to store file lists for each variable\n",
    "    parent_dir = f'{main_dir}/{model}'                                                                                  # Define the parent directory path for the model\n",
    "    variable_options = ['vpd', 'wspeed']                                                                                # List of variable options to process\n",
    "                               \n",
    "    for v in variable_options:                                                                                          # Iterate over each variable option\n",
    "        model_list.append(sorted(glob.glob(os.path.join(parent_dir, f'{v}_{get_filename(model)}_Abs_*.nc'))))           # Append sorted list of file paths for each variable\n",
    "                                       \n",
    "    if len(model_list[0]) != len(model_list[1]):                                                                        # Check if the number of files for each variable is the same\n",
    "        print('the number of years for each variable are not the same')                                                 # Print a message if the number of files is not the same\n",
    "                               \n",
    "    else:                                                                                                               # If the number of files is the same\n",
    "        for v, ws in zip(model_list[0], model_list[1]):                                                                 # Iterate over pairs of files for each variable\n",
    "            if int(v[-7:-3]) != int(ws[-7:-3]):                                                                         # Check if the years in the file names are aligned\n",
    "                print('the years for each variable are not aligned')                                                    # Print a message if the years are not aligned\n",
    "            else:                                                                                                       # If the years are aligned\n",
    "                print(v, ws)                                                                                            # Print the file names\n",
    "                           \n",
    "                if model == 'NAM':                                                                                      # Special handling for NAM model\n",
    "                    vt = xr.open_dataset(v)                                                                             # Open the dataset for vpd\n",
    "                    wt = xr.open_dataset(ws)                                                                            # Open the dataset for wspeed\n",
    "                           \n",
    "                    v_times = vt.time.values                                                                            # Get the time values from the vpd dataset\n",
    "                    matching_indices_1 = [i for i, t in enumerate(wt.time.values) if t in v_times]                      # Find matching time indices in wspeed dataset\n",
    "                           \n",
    "                    wt = wt.isel(time=matching_indices_1)                                                               # Select matching time indices in wspeed dataset\n",
    "                           \n",
    "                else:                                                                                                   # For other models\n",
    "                    vt = xr.open_dataset(v).chunk(get_chunk_database(model))                                            # Open and chunk the dataset for vpd\n",
    "                    wt = xr.open_dataset(ws).chunk(get_chunk_database(model))                                           # Open and chunk the dataset for wspeed\n",
    "                                               \n",
    "                hdwi = (vt['vpd'] * wt['wspeed']).to_dataset(name='hdwi')                                               # Calculate HDWI and create a new dataset\n",
    "                                           \n",
    "                resampler_regular_vars('hdwi', hdwi, main_dir, model)                                                   # Resample and save the HDWI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610e9fe-af8e-4dd0-a2df-b1733a266692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_options = ['CONUS404', 'ERA5', 'HRRR', 'NAM', 'NARR', 'NCEP', 'UFS_S2S']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9e1b4-a72e-4387-bf08-80178a1ff3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = 'database_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7924893-6d0c-45f3-8b6e-f579b145ad32",
   "metadata": {},
   "source": [
    "### breaking it down into easy to process, not easy to process so I can restart the kernel in between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c08b84-382e-4217-97f3-aaeb5ee28a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list1 = ['NARR', 'NCEP']\n",
    "## era5 already done in development of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0d4e8-d93a-4484-88c1-916368fb18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for m in model_list1:\n",
    "    hdwi(m, main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de4110-0fbc-4af7-b661-b753e734d073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hdwi('HRRR', main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc25397-9f5f-428e-ae29-711c68afb8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hdwi('CONUS404', main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b472e-292b-4fd4-9481-2a61e5b9b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hdwi('NAM', main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb8c16-23a7-49cf-8f58-4c57dda2a5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
