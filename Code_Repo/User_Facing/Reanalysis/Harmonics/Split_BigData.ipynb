{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91a3bb7-65a2-44a8-8a90-4c0c30f6a5ca",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>Compute Harmonics on the sFWRD Database</h1>\n",
    "By: Tyler M. Fenske\n",
    "    <br>\n",
    "Last Edited: 2024-02-01\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "This notebook splits each yearly variable file into 4 distinct quadrant files. The purpose is because the original files are too large to concatenate together and apply the harmonics on.<br>    \n",
    "<br>\n",
    "Note: This notebook should be used in conjunction with Harmonics_Applications.ipynb. This notebook is needed for that one to run properly without memory errors.\n",
    "<br>    \n",
    "<br>\n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691e9fd-2789-44ff-9cb8-11a5090f6108",
   "metadata": {},
   "source": [
    "### Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b7e249-4752-433d-b198-1763c74febdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c0d42a-7946-43bb-b393-a4c2c43da46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run File_concat_mod_functions.ipynb\n",
    "#include many of the existing functions to handle the NOAA S2S database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005eb376-eadb-42c8-8e93-216f2b026e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_database_abs_file_xr(model, var, year):\n",
    "    '''This function opens and returns a dataframe for a given:\n",
    "       model : one of the six available model outputs in the database (does not work for UFS)\n",
    "       var   : any var present in the database (must match an available var or will throw an error)\n",
    "       year  : same as var, but for year instead'''\n",
    "\n",
    "    path = f'/raid60B/s2sfire/NOAA_S2S/database_files/{model}/'\n",
    "    base = get_filename(model)\n",
    "    name = f'{path}{var}_{base}_Abs_{str(year)}.nc'\n",
    "\n",
    "    df = xr.open_dataset(name, decode_times=True)[var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706e72c4-03c4-4ccc-b83c-5a0262308be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_database_abs_split_file_xr(model, var, year, q):\n",
    "    '''This function opens and returns a dataframe for a given:\n",
    "       model : one of the six available model outputs in the database (does not work for UFS)\n",
    "       var   : any var present in the database (must match an available var or will throw an error)\n",
    "       year  : same as var, but for year instead'''\n",
    "\n",
    "    path = f'/raid60B/s2sfire/NOAA_S2S/database_files/{model}/SplitFiles/'\n",
    "    base = get_filename(model)\n",
    "    name = f'{path}{var}_{base}_Abs_{str(year)}_quad{q}.nc'\n",
    "\n",
    "    df = xr.open_dataset(name, decode_times=True)[var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f62b52a-ae3e-449e-9eef-57c04b30fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_database_file_xr(model, var, year, stat):\n",
    "    '''This function opens and returns a dataframe for a given:\n",
    "       model : one of the six available model outputs in the database (does not work for UFS)\n",
    "       var   : any var present in the database (must match an available var or will throw an error)\n",
    "       year  : same as var, but for year instead'''\n",
    "\n",
    "    path = f'/raid60B/s2sfire/NOAA_S2S/database_files/{model}/'\n",
    "    base = get_filename(model)\n",
    "    name = f'{path}{var}{stat}_{base}_Daily_{str(year)}.nc'\n",
    "\n",
    "    df = xr.open_dataset(name, decode_times=True)[var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfc4cef-c799-4751-bef0-1909e5e9951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_conus404_twicesplit_file(var, type, quad):\n",
    "    ''''''\n",
    "\n",
    "    path = f'../database_files/CONUS404/SplitFiles/TwiceSplit/{type}/'\n",
    "    file = f'{path}{var}_CONUS404_ANALYSIS_Abs_{type.lower()}_full_period_quad{quad}.nc'\n",
    "\n",
    "    df = xr.open_dataset(file, decode_times=True).astype('float32')\n",
    "    return df\n",
    "\n",
    "def open_conus404_split_stat_file(var, type, stat, quad):\n",
    "    ''''''\n",
    "\n",
    "    path = f'../database_files/CONUS404/SplitStatFiles/{type}/'\n",
    "    file = f'{path}{var}_{stat}_CONUS404_ANALYSIS_Daily_{type.lower()}_full_period_quad{quad}.nc'\n",
    "\n",
    "    df = xr.open_dataset(file, decode_times=True).astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03c40cd-22c3-4e78-a182-1a86313a5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_hrrr_twicesplit_file(var, type, quad):\n",
    "    ''''''\n",
    "\n",
    "    path = f'../database_files/HRRR/SplitFiles/TwiceSplit/{type}/'\n",
    "    file = f'{path}{var}_HRRR_HISTORICAL_Abs_{type.lower()}_full_period_quad{quad}.nc'\n",
    "\n",
    "    df = xr.open_dataset(file, decode_times=True).astype('float32')\n",
    "    df['latitude']  = df.latitude[0,:,:]\n",
    "    df['longitude'] = df.longitude[0,:,:]\n",
    "    return df.astype('float32')\n",
    "\n",
    "def open_hrrr_split_stat_file(var, type, stat, quad):\n",
    "    ''''''\n",
    "\n",
    "    path = f'../database_files/HRRR/SplitStatFiles/{type}/'\n",
    "    file = f'{path}{var}_{stat}_HRRR_HISTORICAL_Daily_{type.lower()}_full_period_quad{quad}.nc'\n",
    "\n",
    "    df = xr.open_dataset(file, decode_times=True).astype('float32')\n",
    "    df['latitude']  = df.latitude[0,:,:]\n",
    "    df['longitude'] = df.longitude[0,:,:]\n",
    "    return df.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb80d3-3f11-4bae-a7b7-cd3fd1896ea0",
   "metadata": {},
   "source": [
    "### CONUS404 File Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce2c4eee-0d40-4253-b91c-086c2b394b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conus404_years     = np.linspace(2011, 2018, 8).astype(int).astype(str)\n",
    "conus404_var_names = [\n",
    "    'ffwi', 'hdwi',   'MLCAPE', 'PBLH', 'PREC_ACC_NC',\n",
    "    'rh',   'SBCAPE', 'SMOIS',  'T2',   'TD2',\n",
    "    'U10',  'V10',    'vpd',    'wdir', 'wspeed']\n",
    "\n",
    "outpath  = f'/raid60B/s2sfire/NOAA_S2S/database_files/CONUS404/SplitFiles/'\n",
    "filebase = get_filename('CONUS404')\n",
    "\n",
    "#initialize the years, vars, and paths needed for CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b82548-8885-4048-9b19-1b4d7c6b44b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for var in conus404_var_names:\n",
    "    for year in conus404_years:\n",
    "        buffer = open_database_abs_file_xr('CONUS404', var, year)\n",
    "        if (var == 'ffwi'):\n",
    "            buffer = buffer.astype('float32')\n",
    "        if (var == 'SMOIS'):\n",
    "            buffer = buffer.sel(soil_layers_stag=0)\n",
    "\n",
    "        x_split = int(buffer.shape[1]/2)\n",
    "        y_split = int(buffer.shape[2]/2)\n",
    "        \n",
    "        quad1 = buffer[:,:x_split,:y_split]\n",
    "        quad2 = buffer[:,:x_split,y_split:]\n",
    "        quad3 = buffer[:,x_split:,:y_split]\n",
    "        quad4 = buffer[:,x_split:,y_split:]\n",
    "\n",
    "        quad1.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad1.nc')\n",
    "        quad2.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad2.nc')\n",
    "        quad3.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad3.nc')\n",
    "        quad4.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad4.nc')\n",
    "\n",
    "#iterate through each CONUS404 year and var file so they can be split into 4 sub files each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744c0ff-032d-4334-b6d5-fb0912f8f6a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "outpath  = f'/raid60B/s2sfire/NOAA_S2S/database_files/CONUS404/SplitFiles/TwiceSplit/'\n",
    "\n",
    "for var in conus404_var_names:\n",
    "    for year in conus404_years:\n",
    "        for q in (np.arange(4)+1).astype('str'):\n",
    "            buffer = open_database_abs_split_file_xr('CONUS404', var, year, q)\n",
    "        \n",
    "            x_split = int(buffer.shape[1]/2)\n",
    "            y_split = int(buffer.shape[2]/2)\n",
    "            \n",
    "            quad1 = buffer[:,:x_split,:y_split]\n",
    "            quad2 = buffer[:,:x_split,y_split:]\n",
    "            quad3 = buffer[:,x_split:,:y_split]\n",
    "            quad4 = buffer[:,x_split:,y_split:]\n",
    "    \n",
    "            quad1.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad{q}1.nc')\n",
    "            quad2.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad{q}2.nc')\n",
    "            quad3.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad{q}3.nc')\n",
    "            quad4.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad{q}4.nc')\n",
    "\n",
    "#iterate through each CONUS404 year and var file so they can be split into 4 sub files each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1455334b-caf3-4626-9e33-db0aaa3cca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath  = f'../database_files/CONUS404/SplitStatFiles/'\n",
    "\n",
    "for stat in ['AVG','MAX','MIN']:\n",
    "    #for var in conus404_var_names:\n",
    "    for var in ['SMOIS']:\n",
    "        for year in conus404_years:\n",
    "            buffer = open_database_file_xr('CONUS404', var, year, stat).astype('float32')\n",
    "            if var == 'SMOIS':\n",
    "                buffer = buffer.sel(soil_layers_stag=0).squeeze()\n",
    "\n",
    "            x_split = int(buffer.shape[1]/2)\n",
    "            y_split = int(buffer.shape[2]/2)\n",
    "            \n",
    "            quad1 = buffer[:,:x_split,:y_split]\n",
    "            quad2 = buffer[:,:x_split,y_split:]\n",
    "            quad3 = buffer[:,x_split:,:y_split]\n",
    "            quad4 = buffer[:,x_split:,y_split:]\n",
    "    \n",
    "            quad1.to_netcdf(f'{outpath}{var}_{stat}_{filebase}_Daily_{str(year)}_quad1.nc', mode='w')\n",
    "            quad2.to_netcdf(f'{outpath}{var}_{stat}_{filebase}_Daily_{str(year)}_quad2.nc', mode='w')\n",
    "            quad3.to_netcdf(f'{outpath}{var}_{stat}_{filebase}_Daily_{str(year)}_quad3.nc', mode='w')\n",
    "            quad4.to_netcdf(f'{outpath}{var}_{stat}_{filebase}_Daily_{str(year)}_quad4.nc', mode='w')\n",
    "\n",
    "            del buffer, quad1, quad2, quad3, quad4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30100031-b9ad-4e20-9ead-e6eedd1dcb11",
   "metadata": {},
   "source": [
    "### HRRR File Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b1e63-8ea8-4cd8-a647-97c0df1b0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrrr_years     = np.linspace(2014, 2018, 5).astype(int).astype(str)\n",
    "hrrr_var_names = [\n",
    "    'blh',  'cape',  'd2m',   'ffwi', 'gust', \n",
    "    'hdwi', 'mstav', 'prate', 'rh',   't2m', \n",
    "    'tp',   'u10',   'v10',   'vpd',  'wdir', \n",
    "    'wspeed']\n",
    "\n",
    "outpath  = f'/raid60B/s2sfire/NOAA_S2S/database_files/HRRR/SplitFiles/'\n",
    "filebase = get_filename('HRRR')\n",
    "\n",
    "#initialize the years, vars, and paths needed for HRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762ac54-50c8-494c-b074-e6273ff25e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in hrrr_var_names:\n",
    "    for year in hrrr_years:\n",
    "        buffer = open_database_abs_file_xr('HRRR', var, year)\n",
    "        if (var == 'ffwi'):\n",
    "            buffer = buffer.astype('float32')\n",
    "\n",
    "        x_split = int(buffer.shape[1]/2)\n",
    "        y_split = int(buffer.shape[2]/2)\n",
    "        \n",
    "        quad1 = buffer[:,:x_split,:y_split]\n",
    "        quad2 = buffer[:,:x_split,y_split:]\n",
    "        quad3 = buffer[:,x_split:,:y_split]\n",
    "        quad4 = buffer[:,x_split:,y_split:]\n",
    "\n",
    "        quad1.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad1.nc')\n",
    "        quad2.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad2.nc')\n",
    "        quad3.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad3.nc')\n",
    "        quad4.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad4.nc')\n",
    "\n",
    "#iterate through each HRRR year and var file so they can be split into 4 sub files each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6592e3-5bbb-4654-954f-084ed408d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath  = f'/raid60B/s2sfire/NOAA_S2S/database_files/HRRR/SplitFiles/TwiceSplit/'\n",
    "\n",
    "for var in hrrr_var_names:\n",
    "    for year in hrrr_years:\n",
    "        for q in (np.arange(4)+1).astype('str'):\n",
    "            buffer = open_database_abs_split_file_xr('HRRR', var, year, q)\n",
    "        \n",
    "            x_split = int(buffer.shape[1]/2)\n",
    "            y_split = int(buffer.shape[2]/2)\n",
    "            \n",
    "            quad1 = buffer[:,:x_split,:y_split]\n",
    "            quad2 = buffer[:,:x_split,y_split:]\n",
    "            quad3 = buffer[:,x_split:,:y_split]\n",
    "            quad4 = buffer[:,x_split:,y_split:]\n",
    "    \n",
    "            quad1.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad{q}1.nc')\n",
    "            quad2.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad{q}2.nc')\n",
    "            quad3.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad{q}3.nc')\n",
    "            quad4.to_netcdf(f'{outpath}{var}_{filebase}_Abs_{str(year)}_quad{q}4.nc')\n",
    "\n",
    "#iterate through each HRRR year and var file so they can be split into 4 sub files each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f41df0-b764-475d-9b86-24e9837cf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath  = f'/raid60B/s2sfire/NOAA_S2S/database_files/HRRR/SplitStatFiles/'\n",
    "\n",
    "for stat in ['AVG','MAX','MIN']:\n",
    "    for var in hrrr_var_names:\n",
    "        for year in hrrr_years:\n",
    "            buffer = open_database_file_xr('HRRR', var, year, stat)\n",
    "\n",
    "            x_split = int(buffer.shape[1]/2)\n",
    "            y_split = int(buffer.shape[2]/2)\n",
    "            \n",
    "            quad1 = buffer[:,:x_split,:y_split]\n",
    "            quad2 = buffer[:,:x_split,y_split:]\n",
    "            quad3 = buffer[:,x_split:,:y_split]\n",
    "            quad4 = buffer[:,x_split:,y_split:]\n",
    "    \n",
    "            quad1.to_netcdf(f'{outpath}{var}_{stat}_{filebase}_Daily_{str(year)}_quad1.nc')\n",
    "            quad2.to_netcdf(f'{outpath}{var}_{stat}_{filebase}_Daily_{str(year)}_quad2.nc')\n",
    "            quad3.to_netcdf(f'{outpath}{var}_{stat}_{filebase}_Daily_{str(year)}_quad3.nc')\n",
    "            quad4.to_netcdf(f'{outpath}{var}_{stat}_{filebase}_Daily_{str(year)}_quad4.nc')\n",
    "\n",
    "            del buffer, quad1, quad2, quad3, quad4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a2d95c-1807-4a7d-937c-3e64be8a0cff",
   "metadata": {},
   "source": [
    "### CONUS404 Re-Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb79697-2d01-497a-b8b4-0c3131bbf3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "conus404_var_names = [\n",
    "    'ffwi', 'hdwi',   'MLCAPE', 'PBLH', 'PREC_ACC_NC',\n",
    "    'rh',   'SBCAPE', 'SMOIS',  'T2',   'TD2',\n",
    "    'U10',  'V10',    'vpd',    'wdir', 'wspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff6fed77-ffd9-4c21-bc12-16862fe3c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in ['Anoms','Climos']:\n",
    "    for var in conus404_var_names:\n",
    "\n",
    "        q11 = open_conus404_twicesplit_file(var, type, '11')\n",
    "        q12 = open_conus404_twicesplit_file(var, type, '12')\n",
    "        q13 = open_conus404_twicesplit_file(var, type, '13')\n",
    "        q14 = open_conus404_twicesplit_file(var, type, '14')\n",
    "        q1112 = xr.concat([q11,q12],dim='west_east')\n",
    "        q1314 = xr.concat([q13,q14],dim='west_east')\n",
    "        q1 = xr.concat([q1112,q1314], dim='south_north')\n",
    "        del q11,q12,q13,q14,q1112,q1314\n",
    "\n",
    "        q21 = open_conus404_twicesplit_file(var, type, '21')\n",
    "        q22 = open_conus404_twicesplit_file(var, type, '22')\n",
    "        q23 = open_conus404_twicesplit_file(var, type, '23')\n",
    "        q24 = open_conus404_twicesplit_file(var, type, '24')\n",
    "        q2122 = xr.concat([q21,q22],dim='west_east')\n",
    "        q2324 = xr.concat([q23,q24],dim='west_east')\n",
    "        q2 = xr.concat([q2122,q2324], dim='south_north')\n",
    "        del q21,q22,q23,q24,q2122,q2324\n",
    "\n",
    "        q31 = open_conus404_twicesplit_file(var, type, '31')\n",
    "        q32 = open_conus404_twicesplit_file(var, type, '32')\n",
    "        q33 = open_conus404_twicesplit_file(var, type, '33')\n",
    "        q34 = open_conus404_twicesplit_file(var, type, '34')\n",
    "        q3132 = xr.concat([q31,q32],dim='west_east')\n",
    "        q3334 = xr.concat([q33,q34],dim='west_east')\n",
    "        q3 = xr.concat([q3132,q3334], dim='south_north')\n",
    "        del q31,q32,q33,q34,q3132,q3334\n",
    "\n",
    "        q41 = open_conus404_twicesplit_file(var, type, '41')\n",
    "        q42 = open_conus404_twicesplit_file(var, type, '42')\n",
    "        q43 = open_conus404_twicesplit_file(var, type, '43')\n",
    "        q44 = open_conus404_twicesplit_file(var, type, '44')\n",
    "        q4142 = xr.concat([q41,q42],dim='west_east')\n",
    "        q4344 = xr.concat([q43,q44],dim='west_east')\n",
    "        q4 = xr.concat([q4142,q4344], dim='south_north')\n",
    "        del q41,q42,q43,q44,q4142,q4344\n",
    "\n",
    "        final_q12 = xr.concat([q1,q2], dim='west_east').astype('float32')\n",
    "        final_q34 = xr.concat([q3,q4], dim='west_east').astype('float32')\n",
    "\n",
    "        final_q1234 = xr.concat([final_q12,final_q34], dim='south_north')\n",
    "        final_q1234.to_netcdf(f'../database_files/CONUS404/{type}/{var}_CONUS404_ANALYSIS_Abs_{type.lower()}_full_period.nc')\n",
    "\n",
    "        del q1,q2,q3,q4,final_q12,final_q34,final_q1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaa23010-29d1-4cbb-a3ce-311b49bc4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in ['Climos']:\n",
    "    for stat in ['AVG','MIN','MAX']:\n",
    "        for var in ['ffwi','hdwi','MLCAPE','PBLH','PREC_ACC_NC','rh','SBCAPE','T2','TD2']:\n",
    "            \n",
    "            q1 = open_conus404_split_stat_file(var, type, stat, '1')\n",
    "            q2 = open_conus404_split_stat_file(var, type, stat, '2')\n",
    "            q3 = open_conus404_split_stat_file(var, type, stat, '3')\n",
    "            q4 = open_conus404_split_stat_file(var, type, stat, '4')\n",
    "\n",
    "            q12 = xr.concat([q1,q2], dim='west_east')\n",
    "            q34 = xr.concat([q3,q4], dim='west_east')\n",
    "            \n",
    "            q1234 = xr.concat([q12,q34], dim='south_north')\n",
    "            \n",
    "            q1234.to_netcdf(f'../database_files/CONUS404/{type}/{var}_{stat}_CONUS404_ANALYSIS_Daily_{type.lower()}_full_period.nc')\n",
    "\n",
    "            del q1,q2,q3,q4,q12,q34,q1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf2688-4f93-4182-ba40-139a419b125a",
   "metadata": {},
   "source": [
    "### HRRR Re-Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd525bd2-c2f5-4315-b91e-705e3353635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrrr_var_names = [\n",
    "    'blh',  'cape',  'd2m',   'ffwi', 'gust', \n",
    "    'hdwi', 'mstav', 'prate', 'rh',   't2m', \n",
    "    'tp',   'u10',   'v10',   'vpd',  'wdir', \n",
    "    'wspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b956c09d-be7b-4498-889c-9c91c40f921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in ['Anoms','Climos']:\n",
    "    for var in hrrr_var_names:\n",
    "\n",
    "        q11 = open_hrrr_twicesplit_file(var, type, '11')\n",
    "        q12 = open_hrrr_twicesplit_file(var, type, '12')\n",
    "        q13 = open_hrrr_twicesplit_file(var, type, '13')\n",
    "        q14 = open_hrrr_twicesplit_file(var, type, '14')\n",
    "        q1112 = xr.concat([q11,q12],dim='x').astype('float32')\n",
    "        q1314 = xr.concat([q13,q14],dim='x').astype('float32')\n",
    "        q1 = xr.concat([q1112,q1314], dim='y').astype('float32')\n",
    "        del q11,q12,q13,q14,q1112,q1314\n",
    "\n",
    "        q21 = open_hrrr_twicesplit_file(var, type, '21')\n",
    "        q22 = open_hrrr_twicesplit_file(var, type, '22')\n",
    "        q23 = open_hrrr_twicesplit_file(var, type, '23')\n",
    "        q24 = open_hrrr_twicesplit_file(var, type, '24')\n",
    "        q2122 = xr.concat([q21,q22],dim='x').astype('float32')\n",
    "        q2324 = xr.concat([q23,q24],dim='x').astype('float32')\n",
    "        q2 = xr.concat([q2122,q2324], dim='y').astype('float32')\n",
    "        del q21,q22,q23,q24,q2122,q2324\n",
    "\n",
    "        q31 = open_hrrr_twicesplit_file(var, type, '31')\n",
    "        q32 = open_hrrr_twicesplit_file(var, type, '32')\n",
    "        q33 = open_hrrr_twicesplit_file(var, type, '33')\n",
    "        q34 = open_hrrr_twicesplit_file(var, type, '34')\n",
    "        q3132 = xr.concat([q31,q32],dim='x').astype('float32')\n",
    "        q3334 = xr.concat([q33,q34],dim='x').astype('float32')\n",
    "        q3 = xr.concat([q3132,q3334], dim='y').astype('float32')\n",
    "        del q31,q32,q33,q34,q3132,q3334\n",
    "\n",
    "        q41 = open_hrrr_twicesplit_file(var, type, '41')\n",
    "        q42 = open_hrrr_twicesplit_file(var, type, '42')\n",
    "        q43 = open_hrrr_twicesplit_file(var, type, '43')\n",
    "        q44 = open_hrrr_twicesplit_file(var, type, '44')\n",
    "        q4142 = xr.concat([q41,q42],dim='x').astype('float32')\n",
    "        q4344 = xr.concat([q43,q44],dim='x').astype('float32')\n",
    "        q4 = xr.concat([q4142,q4344], dim='y').astype('float32')\n",
    "        del q41,q42,q43,q44,q4142,q4344\n",
    "\n",
    "        final_q12 = xr.concat([q1,q2], dim='x').astype('float32')\n",
    "        final_q34 = xr.concat([q3,q4], dim='x').astype('float32')\n",
    "\n",
    "        final_q12.to_netcdf(f'../database_files/HRRR/{type}/{var}_HRRR_HISTORICAL_Abs_{type.lower()}_full_period_western_US.nc')\n",
    "        final_q34.to_netcdf(f'../database_files/HRRR/{type}/{var}_HRRR_HISTORICAL_Abs_{type.lower()}_full_period_eastern_US.nc')\n",
    "        del q1,q2,q3,q4,final_q12,final_q34\n",
    "\n",
    "        #final_q1234 = xr.concat([final_q12,final_q34], dim='y').astype('float32')\n",
    "        #final_q1234.to_netcdf(f'../database_files/HRRR/{type}/{var}_HRRR_HISTORICAL_Abs_{type.lower()}_full_period.nc')\n",
    "\n",
    "        #del q1,q2,q3,q4,final_q12,final_q34,final_q1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "714b23a7-684f-4fdf-8dac-56c5849c2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in ['Anoms','Climos']:\n",
    "    for var in hrrr_var_names:\n",
    "        \n",
    "        west = xr.open_dataset(f'../database_files/HRRR/{type}/{var}_HRRR_HISTORICAL_Abs_{type.lower()}_full_period_western_US.nc', decode_times=True).astype('float32')\n",
    "        east = xr.open_dataset(f'../database_files/HRRR/{type}/{var}_HRRR_HISTORICAL_Abs_{type.lower()}_full_period_eastern_US.nc', decode_times=True).astype('float32')\n",
    "        \n",
    "        final = xr.concat([west,east], dim='y').astype('float32')\n",
    "        final.to_netcdf(f'../database_files/HRRR/{type}/{var}_HRRR_HISTORICAL_Abs_{type.lower()}_full_period.nc')\n",
    "\n",
    "    del west, east, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2473b322-5eb0-4196-883d-cd59a41e8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in ['Anoms','Climos']:\n",
    "    for stat in ['AVG','MIN','MAX']:\n",
    "        for var in hrrr_var_names:\n",
    "\n",
    "            q1 = open_hrrr_split_stat_file(var, type, stat, '1')\n",
    "            q2 = open_hrrr_split_stat_file(var, type, stat, '2')\n",
    "            q3 = open_hrrr_split_stat_file(var, type, stat, '3')\n",
    "            q4 = open_hrrr_split_stat_file(var, type, stat, '4')\n",
    "\n",
    "            q12 = xr.concat([q1,q2], dim='x')\n",
    "            q34 = xr.concat([q3,q4], dim='x')\n",
    "            \n",
    "            q1234 = xr.concat([q12,q34], dim='y')\n",
    "            \n",
    "            q1234.to_netcdf(f'../database_files/HRRR/{type}/{var}_{stat}_HRRR_HISTORICAL_Daily_{type.lower()}_full_period.nc')\n",
    "\n",
    "            del q1,q2,q3,q4,q12,q34,q1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bf740-33a5-4ea6-a34b-296c3c9736b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
