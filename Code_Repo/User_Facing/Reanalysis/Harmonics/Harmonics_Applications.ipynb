{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f8a727-7970-4676-829c-14127f8cdb6e",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>Compute Harmonics on the sFWRD Database</h1>\n",
    "By: Tyler M. Fenske\n",
    "    <br>\n",
    "Last Edited: 2024-02-01\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "This notebook applies harmonics analysis to various forecast and renalysis data as part of the data pre-processing for future forecasting work. \n",
    "<br>    \n",
    "<br>\n",
    "Note: some of the datasets were too large to be processed all at once. The work around is to use Split_BigData.ipynb that takes a given model and splits each yearly variable file into 4 distinct quadrant files spatially and rejoining them after harmonic processing. \n",
    "<br>    \n",
    "<br>\n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50716cc3-a474-40bf-90ac-47de0cc43515",
   "metadata": {},
   "source": [
    "### Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1478728-4925-423a-b5f1-38b5f91fa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6509aaa9-3d87-4574-9e78-6ad206134e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run File_concat_mod_functions.ipynb\n",
    "#include many of the existing functions to handle the NOAA S2S database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3398de-4ea8-4f1d-a0a2-0caccd410b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_concat(model_list, model):\n",
    "\n",
    "    '''\n",
    "    Takes a list of files and concatenates them along the time dimension.\n",
    "\n",
    "    Inputs:\n",
    "    \n",
    "    model_list: (list of str) list of filenames to be opened and concatenates them along the time dimension \n",
    "    model: (str) one of the six available model outputs in the database (does not work for UFS)\n",
    "    \n",
    "    Outputs:\n",
    "\n",
    "    df1: (xarray dataset) combined dataset of all files along the time dimension\n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    if model == 'CONUS404':\n",
    "        cc_dim = 'Time'\n",
    "        df1 = xr.open_dataset(model_list[0]).astype('float32') #.chunk(get_chunk_database(model))\n",
    "    \n",
    "        for f in model_list[1:]:\n",
    "            df2 = xr.open_dataset(f).astype('float32') #.chunk(get_chunk_database(model))\n",
    "            df1 = xr.concat([df1, df2], dim = cc_dim)\n",
    "    else:\n",
    "        cc_dim = 'time'\n",
    "    \n",
    "        df1 = xr.open_dataset(model_list[0]).astype('float32') #.chunk(get_chunk_database(model))\n",
    "    \n",
    "        for f in model_list[1:]:\n",
    "            df2 = xr.open_dataset(f).astype('float32')#.chunk(get_chunk_database(model))\n",
    "            print(f)\n",
    "            df1 = xr.concat([df1, df2], dim = cc_dim)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ac7c2b-c72a-4af7-9d23-b5fce06539a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_database_abs_file_xr(model, var, year):\n",
    "    \n",
    "    '''This function opens and returns an xarray dataset for a given:\n",
    "       model : one of the six available model outputs in the database (does not work for UFS)\n",
    "       var   : any var present in the database (must match an available var or will throw an error)\n",
    "       year  : same as var, but for year instead'''\n",
    "\n",
    "    path = f'/raid60B/s2sfire/NOAA_S2S/database_files/{model}/'\n",
    "    base = get_filename(model)\n",
    "    name = f'{path}{var}_{base}_Abs_{str(year)}.nc'\n",
    "\n",
    "    df = xr.open_dataset(name, decode_times=True)[var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7943b39-56ce-4763-9ad8-1f79c937fc7e",
   "metadata": {},
   "source": [
    "### Harmonics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6bd95a9-a6d7-41f7-a2e1-33923c223876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_harmonics(yt, k=5, offbyone=False):\n",
    "    \n",
    "    '''This function is based off of Section 8.4 (Frequency Domain - 1. Harmonic Analysis)\n",
    "        from the textbook Statistical Methods in the Atmospheric Sciences by Wilks (2006).\n",
    "        This function calculates a specified number of harmonics, or fitted cosine curves,\n",
    "        for a given time series. It functions very similiarly to fourier analysis.\n",
    "        \n",
    "        Inputs: \n",
    "        yt : a 1-d time series to perform harmonics analysis on\n",
    "        k  : the number of harmonics to compute\n",
    "        \n",
    "        Outputs: \n",
    "        yt0  : the sum of the k-harmonics computed\n",
    "        ybar : the mean of yt\n",
    "        C    : the coefficient for each harmonic\n",
    "        w    : the frequency adjustment for each harmonic\n",
    "        \n",
    "        KEYWORDS\n",
    "        offbyone : use if your iterator should start at 1 (Python iterators start at 0)\n",
    "                   (do not use if unsure; for long time series (100+), it won't matter)'''\n",
    "\n",
    "    n = yt.shape[-1]                                                                                                          # Make time the last dimension \n",
    "                       \n",
    "    if (k > int(n/2)):                                                                                                        # Ensure the number of harmonics is within the valid range\n",
    "        k = int(n/2)                                                                                                          # Set k to the maximum valid number of harmonics\n",
    "                       \n",
    "    x    = np.linspace(1, n, n) - 1                                                                                           # Create an array of time indices\n",
    "    ybar = np.nanmean(yt, axis=-1, keepdims=True)                                                                             # Compute the mean of yt along the time dimension\n",
    "    freq = 2 * np.pi * x / n                                                                                                  # Compute the frequency array\n",
    "                   \n",
    "    harmonics_range = np.arange(1, k + 1)                                                                                     # Create an array of harmonic indices\n",
    "                   \n",
    "    cos_terms = np.cos(freq * harmonics_range[:, np.newaxis])[np.newaxis, np.newaxis, ...]                                    # Compute cosine terms for harmonics\n",
    "    sin_terms = np.sin(freq * harmonics_range[:, np.newaxis])[np.newaxis, np.newaxis, ...]                                    # Compute sine terms for harmonics\n",
    "                       \n",
    "    A = (2 / n) * np.nansum(yt[:, :, np.newaxis, :] * cos_terms, axis=-1)                                                     # Compute cosine coefficients\n",
    "    B = (2 / n) * np.nansum(yt[:, :, np.newaxis, :] * sin_terms, axis=-1)                                                     # Compute sine coefficients\n",
    "                   \n",
    "    C = np.sqrt(A**2 + B**2)                                                                                                  # Compute the amplitude of the harmonics\n",
    "                   \n",
    "    w = np.where(A == 0, np.pi / 2, np.arctan(B / A))                                                                         # Compute the phase angle\n",
    "    w = np.where(A < 0, w + np.pi, w)                                                                                         # Adjust phase angle for negative cosine coefficients\n",
    "    w = np.where(w >= 2 * np.pi, w - 2 * np.pi, w)                                                                            # Ensure phase angle is within the range [0, 2Ï€]\n",
    "                   \n",
    "    if (offbyone):                                                                                                            # Check for off-by-one error\n",
    "        w += np.deg2rad(1 / n)                                                                                                # Adjust phase angle to correct off-by-one error\n",
    "                   \n",
    "    yt0 = np.repeat(ybar, n, axis=-1)                                                                                         # Initialize the reconstructed time series with the mean\n",
    "                   \n",
    "    freq_adjusted = freq[np.newaxis, np.newaxis, :, np.newaxis]                                                               # Adjust frequency array for broadcasting\n",
    "                   \n",
    "    harmonics_range = harmonics_range[np.newaxis, np.newaxis, np.newaxis, :]                                                  # Adjust harmonic range array for broadcasting\n",
    "                   \n",
    "    yt0 = yt0 + np.sum(C[:, :, np.newaxis, :] * np.cos(harmonics_range * freq_adjusted - w[:, :, np.newaxis, :]), axis=-1)    # Reconstruct the time series using harmonics\n",
    "\n",
    "    return (yt0, ybar.squeeze(axis=-1), C, w)                                                                                 # Return the reconstructed time series, mean, amplitude, and phase angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3d424-798f-485c-a05d-2fdece497dc4",
   "metadata": {},
   "source": [
    "## Application of the Harmonics Function\n",
    "\n",
    "Organized by model as each needs handled differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b23b15-360d-4ab2-93a6-aca0fb37d803",
   "metadata": {},
   "source": [
    "### ERA5 (Initial testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96716541-42ec-4d1d-bd50-2b82cb4eb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these cells applies the harmonics to the full period available for each variable for ERA5 data\n",
    "#use this one! this is the more useful application of harmonics\n",
    "\n",
    "k = 5\n",
    "\n",
    "filepath = '../database_files/ERA5/'\n",
    "filebase = '_ERA5_REANALYSIS_Abs_'\n",
    "suffix   = '.nc'\n",
    "\n",
    "var_names = [\n",
    "   'blh' ,   'cape',    'cp',     'd2m',   'ffwi',\n",
    "   'hdwi',   'i10fg',   'lsrr',   'rh',    'swvl1',\n",
    "   't2m',    'tp',      'u10',    'v10',   'vpd',\n",
    "   'wdir',   'wspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f364bd-d855-4fec-938e-03b9a02b268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in var_names:                                                                                             # Loop through each variable name in var_names\n",
    "    raw_df = xr.open_mfdataset(f'{filepath}{var_name}{filebase}*{suffix}', decode_times=True)[var_name].load()         # Open and load the dataset for the current variable\n",
    "      \n",
    "    print(f'Applying Harmonics to: {var_name}')                                                                        # Print a message indicating the current variable being processed\n",
    "                \n",
    "    ufunc_output = xr.apply_ufunc(                                                                                     # Apply the compute_harmonics function using xarray's apply_ufunc\n",
    "        compute_harmonics,                                                                                             # Function to apply\n",
    "        raw_df,                                                                                                        # Input dataset\n",
    "        k,                                                                                                             # Number of harmonics\n",
    "        False,                                                                                                         # Off-by-one correction flag\n",
    "        input_core_dims=[['time'], [], []],                                                                            # Input core dimensions\n",
    "        output_core_dims=[['time'], [], ['k'], ['k']],                                                                 # Output core dimensions\n",
    "        output_dtypes=[float, float, float, float],                                                                    # Output data types\n",
    "        vectorize=False)                                                                                               # Do not vectorize the function\n",
    "                \n",
    "    climo = ufunc_output[0].transpose('time', 'latitude', 'longitude').rename(var_name + '_climo')                     # Extract and rename the climatology component\n",
    "    anoms = raw_df - climo                                                                                             # Compute anomalies by subtracting climatology from raw data\n",
    "    anoms = anoms.rename(var_name + '_anoms')                                                                          # Rename the anomalies component\n",
    "                \n",
    "    means  = ufunc_output[1].rename(var_name + '_means')                                                               # Extract and rename the means component\n",
    "    consts = ufunc_output[2].rename(var_name + '_consts')                                                              # Extract and rename the constants component\n",
    "    phis   = ufunc_output[3].rename(var_name + '_phis')                                                                # Extract and rename the phase angles component\n",
    "                \n",
    "    climo_data = xr.merge([climo, means, consts, phis])                                                                # Merge climatology components into a single dataset\n",
    "                \n",
    "    print(f'Complete. Writing anomaly and climatology files over full period for {var_name}...')                       # Print a message indicating completion of processing\n",
    "      \n",
    "    anoms.to_netcdf(os.path.join(filepath + '/Anoms/', var_name + filebase + 'anoms_full_period' + suffix))            # Save anomalies to a NetCDF file\n",
    "    climo_data.to_netcdf(os.path.join(filepath + '/Climos/', var_name + filebase + 'climos_full_period' + suffix))     # Save climatology data to a NetCDF file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca17818-a667-4cad-bf8f-86b6c3be11c0",
   "metadata": {},
   "source": [
    "### NCEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec409a2-b77a-437e-888a-82230816b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these cells applies the harmonics to the full period available for each variable for NCEP data\n",
    "#use this one! this is the more useful application of harmonics\n",
    "\n",
    "k = 5\n",
    "\n",
    "filepath = '../database_files/NCEP/'\n",
    "filebase = '_NCEP_RENALYSIS_II_Abs_'\n",
    "suffix   = '.nc'\n",
    "\n",
    "var_names = [\n",
    "    'air',   'ffwi', 'hdwi', 'prate', 'rhum', \n",
    "    'soilw', 'uwnd', 'vpd',  'vwnd',  'wdir', \n",
    "    'wspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4917c49-3751-43fb-a73e-43225af36fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in var_names:                                                                                                  # Loop through each variable name in var_names\n",
    "    raw_df = xr.open_mfdataset(f'{filepath}{var_name}{filebase}*{suffix}', decode_times=True)[var_name].load()              # Open and load the dataset for the current variable\n",
    "    raw_df = raw_df.sortby('time').squeeze()                                                                                # Sort the dataset by time and remove any singleton dimensions\n",
    "    \n",
    "    print(f'Applying Harmonics to: {var_name}')                                                                             # Print a message indicating the current variable being processed\n",
    "                     \n",
    "    ufunc_output = xr.apply_ufunc(                                                                                          # Apply the compute_harmonics function using xarray's apply_ufunc\n",
    "        compute_harmonics,                                                                                                  # Function to apply\n",
    "        raw_df,                                                                                                             # Input dataset\n",
    "        k,                                                                                                                  # Number of harmonics\n",
    "        False,                                                                                                              # Off-by-one correction flag\n",
    "        input_core_dims=[['time'], [], []],                                                                                 # Input core dimensions\n",
    "        output_core_dims=[['time'], [], ['k'], ['k']],                                                                      # Output core dimensions\n",
    "        output_dtypes=[float, float, float, float],                                                                         # Output data types\n",
    "        vectorize=False)                                                                                                    # Do not vectorize the function\n",
    "                     \n",
    "    climo = ufunc_output[0].transpose('time', 'lat', 'lon').rename(var_name + '_climo')                                     # Extract and rename the climatology component\n",
    "    anoms = raw_df - climo                                                                                                  # Compute anomalies by subtracting climatology from raw data\n",
    "    anoms = anoms.rename(var_name + '_anoms')                                                                               # Rename the anomalies component\n",
    "                     \n",
    "    means  = ufunc_output[1].rename(var_name + '_means')                                                                    # Extract and rename the means component\n",
    "    consts = ufunc_output[2].rename(var_name + '_consts')                                                                   # Extract and rename the constants component\n",
    "    phis   = ufunc_output[3].rename(var_name + '_phis')                                                                     # Extract and rename the phase angles component\n",
    "                     \n",
    "    climo_data = xr.merge([climo, means, consts, phis])                                                                     # Merge climatology components into a single dataset\n",
    "                     \n",
    "    print(f'Complete. Writing anomaly and climatology files over full period for {var_name}...')                            # Print a message indicating completion of processing\n",
    "    \n",
    "    anoms.to_netcdf(os.path.join(filepath + '/Anoms/', var_name + filebase + 'anoms_full_period' + suffix))                 # Save anomalies to a NetCDF file\n",
    "    climo_data.to_netcdf(os.path.join(filepath + '/Climos/', var_name + filebase + 'climos_full_period' + suffix))          # Save climatology data to a NetCDF file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965b81a-2cc5-4471-868a-dcbbb94b8074",
   "metadata": {},
   "source": [
    "### NARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2334b1-fc30-4c72-a0fb-68c30f0bff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "narr_years     = np.linspace(2011, 2014, 4).astype(int).astype(str)\n",
    "narr_filebase  = '_NARR_REANALYSIS_Abs_'\n",
    "narr_filepath = '../database_files/NARR/'\n",
    "narr_var_names = [\n",
    "    'ffwi', 'hdwi', \n",
    "    'Planetary_boundary_layer_height_surface', \n",
    "    'Precipitation_rate_surface', \n",
    "    'Relative_humidity_height_above_ground', \n",
    "    'Soil_moisture_content_layer_between_two_depths_below_surface_layer', \n",
    "    'Temperature_height_above_ground', \n",
    "    'Total_precipitation_surface_3_Hour_Accumulation', \n",
    "    'u-component_of_wind_height_above_ground', \n",
    "    'v-component_of_wind_height_above_ground', \n",
    "    'vpd', 'wdir', 'wspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd48ccb-d121-41da-9729-489042a8cd89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 3                                                                                                                  # Number of harmonics to compute\n",
    "            \n",
    "for var in narr_var_names:                                                                                             # Loop through each variable name in narr_var_names\n",
    "    for year, i in zip(narr_years, np.arange(len(narr_years))):                                                        # Loop through each year and its index\n",
    "        narr_buffer = open_database_abs_file_xr('NARR', var, year)                                                     # Open the dataset for the current variable and year\n",
    "        if ('height_above_ground2' in narr_buffer.dims):                                                               # Check if 'height_above_ground2' dimension exists\n",
    "            narr_buffer = narr_buffer.sel(height_above_ground2=10.0)                                                   # Select data at 10 meters above ground\n",
    "        if ('height_above_ground' in narr_buffer.dims):                                                                # Check if 'height_above_ground' dimension exists\n",
    "            narr_buffer = narr_buffer.sel(height_above_ground=2.0)                                                     # Select data at 2 meters above ground\n",
    "        if ('reftime' in narr_buffer.coords):                                                                          # Check if 'reftime' coordinate exists\n",
    "            narr_buffer = narr_buffer.drop('reftime')                                                                  # Drop the 'reftime' coordinate\n",
    "        narr_buffer = narr_buffer.squeeze().astype('float32')                                                          # Squeeze and convert data to float32\n",
    "                    \n",
    "        if (i == 0):                                                                                                   # If it's the first year\n",
    "            narr_df = narr_buffer.copy()                                                                               # Initialize narr_df with the first year's data\n",
    "        else:                                                                                                          # For subsequent years\n",
    "            narr_df = xr.concat([narr_df, narr_buffer], dim='time')                                                    # Concatenate data along the time dimension\n",
    "                \n",
    "    print(f'Reading files complete. Final dataframe shape: {narr_df.shape}')                                           # Print the shape of the final dataframe\n",
    "            \n",
    "    print(f'Applying Harmonics to: {var}')                                                                             # Print a message indicating the current variable being processed\n",
    "                \n",
    "    ufunc_output = xr.apply_ufunc(                                                                                     # Apply the compute_harmonics function using xarray's apply_ufunc\n",
    "        compute_harmonics,                                                                                             # Function to apply\n",
    "        narr_df,                                                                                                       # Input dataset\n",
    "        k,                                                                                                             # Number of harmonics\n",
    "        False,                                                                                                         # Off-by-one correction flag\n",
    "        input_core_dims=[['time'], [], []],                                                                            # Input core dimensions\n",
    "        output_core_dims=[['time'], [], ['k'], ['k']],                                                                 # Output core dimensions\n",
    "        output_dtypes=[float, float, float, float],                                                                    # Output data types\n",
    "        vectorize=False)                                                                                               # Do not vectorize the function\n",
    "                \n",
    "    climo = ufunc_output[0].transpose('time', 'y', 'x').rename(f'{var}_climo')                                         # Extract and rename the climatology component\n",
    "    anoms = narr_df - climo                                                                                            # Compute anomalies by subtracting climatology from raw data\n",
    "    anoms = anoms.rename(f'{var}_anoms')                                                                               # Rename the anomalies component\n",
    "                \n",
    "    means  = ufunc_output[1].rename(f'{var}_means')                                                                    # Extract and rename the means component\n",
    "    consts = ufunc_output[2].rename(f'{var}_consts')                                                                   # Extract and rename the constants component\n",
    "    phis   = ufunc_output[3].rename(f'{var}_phis')                                                                     # Extract and rename the phase angles component\n",
    "                \n",
    "    climo_data = xr.merge([climo, means, consts, phis])                                                                # Merge climatology components into a single dataset\n",
    "                \n",
    "    print(f'Complete. Writing anomaly and climatology files over full period for {var}...')                            # Print a message indicating completion of processing\n",
    "                \n",
    "    anoms.to_netcdf(os.path.join(narr_filepath + '/Anoms/', f'{var}{narr_filebase}anoms_full_period.nc'))              # Save anomalies to a NetCDF file\n",
    "    climo_data.to_netcdf(os.path.join(narr_filepath + '/Climos/', f'{var}{narr_filebase}climos_full_period.nc'))       # Save climatology data to a NetCDF file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4097c9-3cb2-4a5f-89fb-fecd9a07aa99",
   "metadata": {},
   "source": [
    "### CONUS404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41db2a-be51-4dbe-8695-0f7d1f847ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these cells applies the harmonics to the full period available for each variable for CONUS404 data\n",
    "k = 5\n",
    "\n",
    "filepath = '../database_files/CONUS404/SplitFiles/TwiceSplit/'\n",
    "filebase = '_CONUS404_ANALYSIS_Abs_'\n",
    "suffix   = '.nc'\n",
    "\n",
    "var_names = [\n",
    "    'ffwi', 'hdwi',   'MLCAPE', 'PBLH', 'PREC_ACC_NC',\n",
    "    'rh',   'SBCAPE', 'SMOIS',  'T2',   'TD2',\n",
    "    'U10',  'V10',    'vpd',    'wdir', 'wspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885beb05-3980-4d05-b97a-8245c9152ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in var_names:                                                                                                                                    # Loop through each variable name in var_names\n",
    "    for q1 in (np.arange(4) + 1).astype('str'):                                                                                                               # Loop through the first quadrant index (1 to 4)\n",
    "        for q2 in (np.arange(4) + 1).astype('str'):                                                                                                           # Loop through the second quadrant index (1 to 4)\n",
    "            raw_df = xr.open_mfdataset(f'{filepath}{var_name}{filebase}*_quad{q1}{q2}{suffix}', decode_times=True)[var_name].load().astype('float32')         # Open and load the dataset for the current variable and quadrant\n",
    "                    \n",
    "            print(f'Applying Harmonics to: CONUS404 {var_name} quad{q1}{q2}')                                                                                 # Print a message indicating the current variable and quadrant being processed\n",
    "                                                                \n",
    "            ufunc_output = xr.apply_ufunc(                                                                                                                    # Apply the compute_harmonics function using xarray's apply_ufunc\n",
    "                compute_harmonics,                                                                                                                            # Function to apply\n",
    "                raw_df,                                                                                                                                       # Input dataset\n",
    "                k,                                                                                                                                            # Number of harmonics\n",
    "                False,                                                                                                                                        # Off-by-one correction flag\n",
    "                input_core_dims=[['Time'], [], []],                                                                                                           # Input core dimensions\n",
    "                output_core_dims=[['Time'], [], ['k'], ['k']],                                                                                                # Output core dimensions\n",
    "                output_dtypes=[float, float, float, float],                                                                                                   # Output data types\n",
    "                vectorize=False)                                                                                                                              # Do not vectorize the function\n",
    "                    \n",
    "            climo = ufunc_output[0].transpose('Time', 'south_north', 'west_east').rename(var_name + '_climo')                                                 # Extract and rename the climatology component\n",
    "            anoms = raw_df - climo                                                                                                                            # Compute anomalies by subtracting climatology from raw data\n",
    "            anoms = anoms.rename(var_name + '_anoms')                                                                                                         # Rename the anomalies component\n",
    "                                                               \n",
    "            means  = ufunc_output[1].rename(var_name + '_means')                                                                                              # Extract and rename the means component\n",
    "            consts = ufunc_output[2].rename(var_name + '_consts')                                                                                             # Extract and rename the constants component\n",
    "            phis   = ufunc_output[3].rename(var_name + '_phis')                                                                                               # Extract and rename the phase angles component\n",
    "                                                               \n",
    "            climo_data = xr.merge([climo, means, consts, phis])                                                                                               # Merge climatology components into a single dataset\n",
    "                                                               \n",
    "            print(f'Complete. Writing anomaly and climatology files over full period for {var_name}...')                                                      # Print a message indicating completion of processing\n",
    "            \n",
    "            anoms.to_netcdf(f'{filepath}/Anoms/{var_name}{filebase}anoms_full_period_quad{q1}{q2}{suffix}')                                                   # Save anomalies to a NetCDF file\n",
    "            climo_data.to_netcdf(f'{filepath}/Climos/{var_name}{filebase}climos_full_period_quad{q1}{q2}{suffix}')                                            # Save climatology data to a NetCDF file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41c6ed-6980-46af-9f0d-e250a3e96a21",
   "metadata": {},
   "source": [
    "### HRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de184c51-1636-4d67-8bd6-484c05b47344",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "filepath = '../database_files/HRRR/SplitFiles/TwiceSplit/'\n",
    "\n",
    "hrrr_years     = np.linspace(2014, 2018, 5).astype(int).astype(str)\n",
    "hrrr_filebase  = '_HRRR_HISTORICAL_Abs_'\n",
    "suffix         = '.nc'\n",
    "hrrr_var_names = [\n",
    "    'blh', 'cape', 'd2m', 'ffwi', 'gust', \n",
    "    'hdwi', 'mstav', 'prate', 'rh', 't2m', \n",
    "    'tp', 'u10', 'v10', 'vpd', 'wdir', \n",
    "    'wspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f8c5a-5e45-4e03-9942-4d4a7bcb7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in hrrr_var_names:                                                                                                      # Loop through each variable name in hrrr_var_names\n",
    "    for q1 in (np.arange(4) + 1).astype('str'):                                                                                      # Loop through the first quadrant index (1 to 4)\n",
    "        for q2 in (np.arange(4) + 1).astype('str'):                                                                                  # Loop through the second quadrant index (1 to 4)\n",
    "            raw_df = xr.open_mfdataset(f'{filepath}{var_name}{hrrr_filebase}*_quad{q1}{q2}{suffix}',                                 # Open and load the dataset for the current variable and quadrant\n",
    "                                       decode_times=True)[var_name].load().astype('float32')                          \n",
    "                                      \n",
    "            print(f'Applying Harmonics to: HRRR {var_name} quad{q1}{q2}')                                                            # Print a message indicating the current variable and quadrant being processed\n",
    "                                      \n",
    "            ufunc_output = xr.apply_ufunc(                                                                                           # Apply the compute_harmonics function using xarray's apply_ufunc\n",
    "                compute_harmonics,                                                                                                   # Function to apply\n",
    "                raw_df,                                                                                                              # Input dataset\n",
    "                k,                                                                                                                   # Number of harmonics\n",
    "                False,                                                                                                               # Off-by-one correction flag\n",
    "                input_core_dims=[['time'], [], []],                                                                                  # Input core dimensions\n",
    "                output_core_dims=[['time'], [], ['k'], ['k']],                                                                       # Output core dimensions\n",
    "                output_dtypes=[float, float, float, float],                                                                          # Output data types\n",
    "                vectorize=False)                                                                                                     # Do not vectorize the function\n",
    "                                      \n",
    "            climo = ufunc_output[0].transpose('time', 'y', 'x').rename(var_name + '_climo')                                          # Extract and rename the climatology component\n",
    "            anoms = raw_df - climo                                                                                                   # Compute anomalies by subtracting climatology from raw data\n",
    "            anoms = anoms.rename(var_name + '_anoms')                                                                                # Rename the anomalies component\n",
    "                                      \n",
    "            means  = ufunc_output[1].rename(var_name + '_means')                                                                     # Extract and rename the means component\n",
    "            consts = ufunc_output[2].rename(var_name + '_consts')                                                                    # Extract and rename the constants component\n",
    "            phis   = ufunc_output[3].rename(var_name + '_phis')                                                                      # Extract and rename the phase angles component\n",
    "                                      \n",
    "            climo_data = xr.merge([climo, means, consts, phis])                                                                      # Merge climatology components into a single dataset\n",
    "                                      \n",
    "            print(f'Complete. Writing anomaly and climatology files over full period for {var_name}...')                             # Print a message indicating completion of processing\n",
    "    \n",
    "            anoms.to_netcdf(f'{filepath}/Anoms/{var_name}{hrrr_filebase}anoms_full_period_quad{q1}{q2}{suffix}')                     # Save anomalies to a NetCDF file\n",
    "            climo_data.to_netcdf(f'{filepath}/Climos/{var_name}{hrrr_filebase}climos_full_period_quad{q1}{q2}{suffix}')              # Save climatology data to a NetCDF file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749df10-871a-4d00-bda9-879e25013cb2",
   "metadata": {},
   "source": [
    "### NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e86a3-dbbb-406a-820d-ffbc4519bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_years     = np.linspace(2011, 2018, 8).astype(int).astype(str)\n",
    "nam_filebase  = '_NAM_HISTORICAL_Abs_'\n",
    "nam_filepath  = '../database_files/NAM/'\n",
    "nam_var_names = [\n",
    "    'cape', 'ffwi', 'gust', 'hdwi', 'r',\n",
    "    'sm',   't2m',  'tp',   'u10',  'v10',\n",
    "    'vpd',  'wdir', 'wspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64881c7-9c70-4298-9deb-ae5b807550ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5                                                                                                                    # Number of harmonics to compute\n",
    "for var in nam_var_names:                                                                                                # Loop through each variable name in nam_var_names\n",
    "    if var in ['cape', 'gust']:                                                                                          # Check if the variable is 'cape' or 'gust'\n",
    "        years = np.linspace(2017, 2018, 2).astype(int).astype(str)                                                       # Set years to 2017 and 2018\n",
    "        k = 1                                                                                                            # Set number of harmonics to 1\n",
    "    elif var in ['ffwi', 'tp']:                                                                                          # Check if the variable is 'ffwi' or 'tp'\n",
    "        years = np.linspace(2013, 2018, 6).astype(int).astype(str)                                                       # Set years from 2013 to 2018\n",
    "    else:                                                                                                                # For other variables\n",
    "        years = nam_years                                                                                                # Use the predefined nam_years\n",
    "                      \n",
    "    for year, i in zip(years, np.arange(len(years))):                                                                    # Loop through each year and its index\n",
    "        nam_buffer = open_database_abs_file_xr('NAM', var, year)                                                         # Open the dataset for the current variable and year\n",
    "        print(f'File read: NAM {var} for {year}')                                                                        # Print a message indicating the file read\n",
    "                      \n",
    "        if year == '2012':                                                                                               # Check if the year is 2012\n",
    "            if var == 'hdwi':                                                                                            # Check if the variable is 'hdwi'\n",
    "                drop_coords = ['step', 'valid_time']                                                                     # Set coordinates to drop\n",
    "            elif var == 'sm':                                                                                            # Check if the variable is 'sm'\n",
    "                drop_coords = ['step', 'depthBelowLandLayer', 'valid_time']                                              # Set coordinates to drop\n",
    "            else:                                                                                                        # For other variables\n",
    "                drop_coords = ['step', 'heightAboveGround', 'valid_time']                                                # Set coordinates to drop\n",
    "            nam_buffer = nam_buffer.drop(drop_coords)                                                                    # Drop the specified coordinates\n",
    "              \n",
    "        if (var == 'sm' and year in ['2017', '2018']):                                                                   # Check if the variable is 'sm' and the year is 2017 or 2018\n",
    "            nam_buffer = nam_buffer.sel(depthBelowLandLayer=0)                                                           # Select data at the surface layer\n",
    "        nam_buffer = nam_buffer.sortby('time')                                                                           # Sort the dataset by time\n",
    "        nam_buffer = nam_buffer.squeeze().astype('float32')                                                              # Squeeze and convert data to float32\n",
    "                  \n",
    "        if i == 0:                                                                                                       # If it's the first year\n",
    "            nam_df = nam_buffer.copy()                                                                                   # Initialize nam_df with the first year's data\n",
    "        else:                                                                                                            # For subsequent years\n",
    "            nam_df = xr.concat([nam_df, nam_buffer], dim='time')                                                         # Concatenate data along the time dimension\n",
    "                          \n",
    "    print(f'Reading files complete. Final dataframe shape: {nam_df.shape}')                                              # Print the shape of the final dataframe\n",
    "              \n",
    "    print(f'Applying Harmonics to: {var}')                                                                               # Print a message indicating the current variable being processed\n",
    "                  \n",
    "    ufunc_output = xr.apply_ufunc(                                                                                       # Apply the compute_harmonics function using xarray's apply_ufunc\n",
    "        compute_harmonics,                                                                                               # Function to apply\n",
    "        nam_df,                                                                                                          # Input dataset\n",
    "        k,                                                                                                               # Number of harmonics\n",
    "        False,                                                                                                           # Off-by-one correction flag\n",
    "        input_core_dims=[['time'], [], []],                                                                              # Input core dimensions\n",
    "        output_core_dims=[['time'], [], ['k'], ['k']],                                                                   # Output core dimensions\n",
    "        output_dtypes=[float, float, float, float],                                                                      # Output data types\n",
    "        vectorize=False)                                                                                                 # Do not vectorize the function\n",
    "                  \n",
    "    climo = ufunc_output[0].transpose('time', 'y', 'x').rename(f'{var}_climo')                                           # Extract and rename the climatology component\n",
    "    anoms = nam_df - climo                                                                                               # Compute anomalies by subtracting climatology from raw data\n",
    "    anoms = anoms.rename(f'{var}_anoms')                                                                                 # Rename the anomalies component\n",
    "                  \n",
    "    means  = ufunc_output[1].rename(f'{var}_means')                                                                      # Extract and rename the means component\n",
    "    consts = ufunc_output[2].rename(f'{var}_consts')                                                                     # Extract and rename the constants component\n",
    "    phis   = ufunc_output[3].rename(f'{var}_phis')                                                                       # Extract and rename the phase angles component\n",
    "                  \n",
    "    climo_data = xr.merge([climo, means, consts, phis])                                                                  # Merge climatology components into a single dataset\n",
    "                  \n",
    "    print(f'Complete. Writing anomaly and climatology files over full period for {var}...')                              # Print a message indicating completion of processing\n",
    "                  \n",
    "    anoms.to_netcdf(os.path.join(nam_filepath + '/Anoms/', f'{var}{nam_filebase}anoms_full_period.nc'))                  # Save anomalies to a NetCDF file\n",
    "    climo_data.to_netcdf(os.path.join(nam_filepath + '/Climos/', f'{var}{nam_filebase}climos_full_period.nc'))           # Save climatology data to a NetCDF file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385bd22-4629-435e-b4dc-5b49e04a23d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b724c-90b1-4958-8e9a-19dff6041066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
