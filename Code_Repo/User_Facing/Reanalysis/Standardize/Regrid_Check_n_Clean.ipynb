{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50bc75df-7b6e-483b-9f35-25775c1dec36",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>Data Regridding with CDO Step 2: Check Files and Clean for Resubmit</h1>\n",
    "By: Kayla Besong, PhD\n",
    "    <br>\n",
    "Last Edited: 05/10/24\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "This is the post-CDO regridding process that that identifies which files were not regridded or failed due to various reasons and provides some means for helping to fix that. This is the most chaotic of notebooks the sFWRD database as it really is a back of the napkin, fix what needs to be fixed, and send it back through CDO process. The processes used here might not solve problems that arose personally during the gridding process but may help identify a solution path.\n",
    "<br>    \n",
    "<br>\n",
    "In some of cleaning steps there are multple code blocks, one for the original database and another for the processed harmonics analysis.\n",
    "\n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a959e012-cfee-4268-b1b7-ef91360ca89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c68c6-6ef0-4644-99b6-6e421dff57e3",
   "metadata": {},
   "source": [
    "# Find out if any files had issues in the CDO regridding process\n",
    "\n",
    "and then fix them with various brut-force, bandaid, just-get-it-done code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6786c43-e5de-4ec8-b1ca-d5ab7e004e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_not_regridded(input_dir, output_dir):\n",
    "\n",
    "    ''' Find out what files from the input directory did not make it to the output, regridded directory\n",
    "    \n",
    "    Inputs:\n",
    "\n",
    "    input_dir: (str) path where input files are stored \n",
    "    output_fir: (str) path where output files are stored\n",
    "\n",
    "    Output:\n",
    "\n",
    "    bad: (list of strs) list of file names that were in the input directory but not the output directory    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    regridded_list = os.listdir(output_dir)                                        # List all files in the regridded database directory for model m\n",
    "    og_list = os.listdir(input_dir)                                                # List all files in the original database directory for model m\n",
    "    \n",
    "    bad = []                                                                       # Initialize a list to keep track of files that are not regridded\n",
    "    \n",
    "    for i in og_list:                                                              # Iterate over each file in the original list\n",
    "        if i not in regridded_list:                                                # Check if the file is not in the regridded list\n",
    "            if i[-2:] == 'nc':                                                     # Ensure the file is a NetCDF file\n",
    "                bad.append(i)                                                      # Add the file to the bad list if it's not regridded\n",
    "    \n",
    "    if len(bad) > 0:                                                               # Check if there are any bad files\n",
    "        \n",
    "        return bad                                                                  # Return the list of bad files\n",
    "    \n",
    "    else:\n",
    "        print(f'No bad files for {m}')                                             # Print a message if there are no bad files for model m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d056b-8622-4d02-9043-67dbed2f9763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e035e7c-2659-4076-a45e-bb4e0c13462a",
   "metadata": {},
   "source": [
    "## NARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b53c839-ceac-4284-9929-b6cc4264bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for NARR\n"
     ]
    }
   ],
   "source": [
    "parent_out = f'../database_files_regridded'\n",
    "parent_in = f'../database_files'\n",
    "\n",
    "m = 'NARR'\n",
    "narr_no = find_not_regridded(f'{parent_in}/{m}', f'{parent_out}/{m}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28739d-6162-4260-8d79-f059824998a3",
   "metadata": {},
   "source": [
    "#### Check Climo and Anom files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e030dda-b276-47bb-a54e-de966fd7307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for NARR\n"
     ]
    }
   ],
   "source": [
    "m = 'NARR'\n",
    "\n",
    "parent_out = f'../database_files_regridded/anoms/{m}'\n",
    "parent_in = f'../database_files/{m}/Anoms/'\n",
    "\n",
    "narr_no_anom = find_not_regridded(f'{parent_in}', f'{parent_out}')\n",
    "narr_no_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c64c258-0b7d-4c6a-907a-7ce674d82c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for NARR\n"
     ]
    }
   ],
   "source": [
    "m = 'NARR'\n",
    "\n",
    "parent_out = f'../database_files_regridded/climos/{m}'\n",
    "parent_in = f'../database_files/{m}/Climos/'\n",
    "\n",
    "narr_no_climo = find_not_regridded(f'{parent_in}', f'{parent_out}')\n",
    "narr_no_climo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7b94f-68f8-46ee-9591-752d65cab5ae",
   "metadata": {},
   "source": [
    "## NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b6c2219-abc9-4736-8cc8-db87ab2c09f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for NAM\n"
     ]
    }
   ],
   "source": [
    "parent_out = f'../database_files_regridded'\n",
    "parent_in = f'../database_files'\n",
    "\n",
    "m = 'NAM'\n",
    "\n",
    "nam_no = find_not_regridded(f'{parent_in}/{m}', f'{parent_out}/{m}')\n",
    "nam_no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ac944-1d0a-4e71-8a10-f8fd4ebfb692",
   "metadata": {},
   "source": [
    "#### Check Climo and Anom files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d62d8e8d-9dd5-41d8-a30b-8d7dd6e6bbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for NAM\n"
     ]
    }
   ],
   "source": [
    "m = 'NAM'\n",
    "\n",
    "parent_out = f'../database_files_regridded/anoms/{m}'\n",
    "parent_in = f'../database_files/{m}/Anoms/'\n",
    "\n",
    "nam_no_anom = find_not_regridded(f'{parent_in}', f'{parent_out}')\n",
    "nam_no_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "058e5591-7d0a-4fa2-9d98-65bc1ed201a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for NAM\n"
     ]
    }
   ],
   "source": [
    "m = 'NAM'\n",
    "\n",
    "parent_out = f'../database_files_regridded/climos/{m}'\n",
    "parent_in = f'../database_files/{m}/Climos/'\n",
    "\n",
    "nam_no_climo = find_not_regridded(f'{parent_in}', f'{parent_out}')\n",
    "nam_no_climo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21038aaf-8399-486a-9064-64b7bd3767b4",
   "metadata": {},
   "source": [
    "## CONSU404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee844b40-993d-40f7-bd6a-3169739f0e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for CONUS404\n"
     ]
    }
   ],
   "source": [
    "parent_out = f'../database_files_regridded'\n",
    "parent_in = f'../database_files'\n",
    "\n",
    "m = 'CONUS404'\n",
    "\n",
    "c44_no = find_not_regridded(f'{parent_in}/{m}', f'{parent_out}/{m}')\n",
    "c44_no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fdfa57-5ec9-4929-820d-345978136162",
   "metadata": {},
   "source": [
    "#### Check Climo and Anom files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bfc338c-9a8c-445a-a925-5c66b79a9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for CONUS404\n"
     ]
    }
   ],
   "source": [
    "m = 'CONUS404'\n",
    "\n",
    "parent_out = f'../database_files_regridded/anoms/{m}'\n",
    "parent_in = f'../database_files/{m}/Anoms/'\n",
    "\n",
    "c404_no_anom = find_not_regridded(f'{parent_in}', f'{parent_out}')\n",
    "c404_no_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f272426-d0fe-43a3-a0d2-f4b3cee45624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for CONUS404\n"
     ]
    }
   ],
   "source": [
    "m = 'CONUS404'\n",
    "\n",
    "parent_out = f'../database_files_regridded/climos/{m}'\n",
    "parent_in = f'../database_files/{m}/Climos/'\n",
    "\n",
    "c404_no_climo = find_not_regridded(f'{parent_in}', f'{parent_out}')\n",
    "c404_no_climo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a72ca-8188-4291-8a8e-12e4bc1a4247",
   "metadata": {},
   "source": [
    "## HRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45eb45-9b5a-487e-9538-4a75254ea661",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_out = f'../database_files_regridded'\n",
    "parent_in = f'../database_files'\n",
    "\n",
    "m = 'HRRR'\n",
    "\n",
    "hrrr_no = find_not_regridded(f'{parent_in}/{m}', f'{parent_out}/{m}')\n",
    "hrrr_no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e0669-2d8a-4a36-9485-36ed4f1f7f65",
   "metadata": {},
   "source": [
    "#### Check Climo and Anom files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f6660e3-1318-468d-9cdb-f466f409cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for HRRR\n"
     ]
    }
   ],
   "source": [
    "m = 'HRRR'\n",
    "\n",
    "parent_out = f'../database_files_regridded/anoms/{m}'\n",
    "parent_in = f'../database_files/{m}/Anoms/'\n",
    "\n",
    "hrrr_no_anom = find_not_regridded(f'{parent_in}', f'{parent_out}')\n",
    "hrrr_no_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0292b81-1967-45b0-bbea-ea8fe0e10a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bad files for HRRR\n"
     ]
    }
   ],
   "source": [
    "m = 'HRRR'\n",
    "\n",
    "parent_out = f'../database_files_regridded/climos/{m}'\n",
    "parent_in = f'../database_files/{m}/Climos/'\n",
    "\n",
    "hrrr_no_climo = find_not_regridded(f'{parent_in}', f'{parent_out}')\n",
    "hrrr_no_climo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941cd33-5a2e-4c46-8808-40ea08349850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d14f3bb9-39c4-491a-88b3-03109f234357",
   "metadata": {},
   "source": [
    "# Cleaning steps for failed files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72351e3-6a3a-45b0-a25c-3c5b14b905a0",
   "metadata": {},
   "source": [
    "## NARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c06fcd-2155-4e0c-9b51-77bfb7430a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "narr_grid = xr.open_dataset('air.sfc.1992.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6231bd-2137-422e-b79e-0042ed155f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "narr_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de69f41-e692-4837-ac65-90ddbe2b02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['ffwi', 'hdwi', 'Planetary_boundary_layer_height_surface',\n",
    "        'Precipitation_rate_surface', 'Relative_humidity_height_above_ground',\n",
    "        'Soil_moisture_content_layer_between_two_depths_below_surface_layer',\n",
    "        'Temperature_height_above_ground', \n",
    "        'Total_precipitation_surface_3_Hour_Accumulation',\n",
    "        'u-component_of_wind_height_above_ground',\n",
    "        'v-component_of_wind_height_above_ground',\n",
    "        'vpd', 'wdir', 'wspeed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1768a28-1065-4610-81fd-1678e49e0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dict = {}\n",
    "\n",
    "for v in vars:\n",
    "\n",
    "    vars_dict[v.split('_')[0]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ab6c9-41d0-4186-a2a0-bcc347f4c35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "narr_dir = f'../database_files/NARR_ungrid/Climos/'\n",
    "\n",
    "dsarr = []\n",
    "ct =0\n",
    "for f in os.listdir(narr_dir):\n",
    "\n",
    "    if f[-2:] == 'nc':\n",
    "\n",
    "        ds = xr.open_dataset(f'{narr_dir}/{f}')\n",
    "\n",
    "        try:      \n",
    "            v = vars_dict[f.split('/')[-1].split('_')[0]]                        # Attempt to extract the variable name from the file name\n",
    "        \n",
    "        except KeyError:      \n",
    "            v = vars_dict[f.split('/')[-1].split('_')[0][0:-3]]                  # Adjust the variable extraction for special cases\n",
    "\n",
    "        coords_to_keep = ['x', 'y', 'time']\n",
    "        all_coords = list(ds.coords)\n",
    "        \n",
    "        if len(coords_to_keep) != len(all_coords):                        # Check if there are extra coordinates\n",
    "            delete = [i for i in all_coords if i not in coords_to_keep]   # Identify coordinates to delete\n",
    "            ds = ds.drop(delete)                                          # Drop unwanted coordinates\n",
    "            \n",
    "        narr_grid = xr.open_dataset('air.sfc.1992.nc')\n",
    "        narr_grid = narr_grid.drop('air')        \n",
    "        narr_grid = narr_grid.drop('time')\n",
    "        \n",
    "        narr_grid['time'] = ds['time']\n",
    "        \n",
    "        str_var = f'{v}_climo'\n",
    "        \n",
    "        vals_da = xr.DataArray(ds[str_var].values, dims=['time', 'y', 'x'], coords={'lat': narr_grid['lat'], 'lon': narr_grid['lon'], 'y': narr_grid['y'], 'x': narr_grid['x'], 'time': narr_grid['time']})\n",
    "        narr_grid[str_var] = vals_da \n",
    "        \n",
    "        narr_grid.to_netcdf(f'../database_files/NARR/Climos/cleaned/{f}')\n",
    "\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d58cf5-156e-4087-9041-9bcda874f0e3",
   "metadata": {},
   "source": [
    "## NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec8b57-5dde-4dba-91a2-b7c8b99bf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_nam_handler_cdo(file, v):\n",
    "\n",
    "    ''' This function takes problematic NAM files from the database \n",
    "        and adds a missing time dimension or grid that got lost or selects one level.\n",
    "    \n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    file: (xarray dataset) a NAM file opened with xarray\n",
    "    v: (str) variable name\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    nam_grid: (xarray dataset) the new, cleaned version of the input \n",
    "    \n",
    "    '''\n",
    "    nam_grid = xr.open_dataset('../database_files/NAM/t2m_NAM_HISTORICAL_Abs_2012.nc')              # Open a NAM dataset to use its grid information and drop the 't2m' variable, always use a leap year!! \n",
    "    \n",
    "    if 'Daily' in file.split('/')[-1].split('_'):                                                   # Check if the file represents daily data\n",
    "        nam_grid = nam_grid.resample(time = '24H').mean('time')                                     # Resample the grid data to daily frequency and compute the mean\n",
    "    \n",
    "    df = xr.open_dataset(file)                                                                      # Open the dataset file\n",
    "    \n",
    "    nam_grid = nam_grid.isel(time = np.arange(0, len(df.time)))\n",
    "    \n",
    "    if v == 'sm':                                                                                   # Check if the variable is soil moisture ('sm')\n",
    "        df = df.isel(depthBelowLandLayer = 0)                                                       # Select the first layer of soil moisture data\n",
    "    \n",
    "    nam_grid['time'] = df['time']                                                                   # Update the 'time' coordinate in the grid dataset to match the data file\n",
    "    \n",
    "    # Create a DataArray with the data values, dimensions, and coordinates aligned with the NAM grid\n",
    "    df_vals_da = xr.DataArray(df[v].values, dims=['time', 'y', 'x'], coords={'latitude': nam_grid['latitude'], 'longitude': nam_grid['longitude'], 'y': nam_grid['y'], 'x': nam_grid['x'], 'time': nam_grid['time']})\n",
    "\n",
    "    nam_grid = nam_grid.drop('t2m')\n",
    "    nam_grid[v] = df_vals_da                                                                        # Assign the newly created DataArray to the NAM grid dataset under the variable v\n",
    "    \n",
    "    return nam_grid                                                                                 # Return the updated NAM grid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037cbfc2-febe-459d-a004-d80fd1b3a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check which files need a level selection \n",
    "\n",
    "ct = 0\n",
    "for i in nam_no:\n",
    "    x = xr.open_dataset(f'../database_files/NAM/{i}')\n",
    "    print(list(x.dims), ct)\n",
    "    ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5a64c-6319-4ac7-92bc-d306ca755e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = os.listdir('../NAM')\n",
    "vars.append('vpd')\n",
    "vars.append('wspeed')\n",
    "vars.append('wdir')\n",
    "vars.append('hdwi')\n",
    "vars.append('ffwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c2be6-0c58-406e-a742-9481bf96ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dict = {}\n",
    "\n",
    "for v in vars:\n",
    "\n",
    "    vars_dict[v.split('_')[0]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcb9cd-0ecc-4c08-ab4a-25cdc8c77d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../database_files/NAM/'                                          # Define the input directory for NAM files\n",
    "\n",
    "for f in nam_no:                                                              # Iterate over the list of NAM files\n",
    "    if f[-2:] == 'nc':                                                        # Check if the file is a NetCDF file\n",
    "        \n",
    "        file = f'{input_dir}/{f}'                                             # Construct the full file path\n",
    "\n",
    "        try:\n",
    "            v = vars_dict[file.split('/')[-1].split('_')[0]]                  # Attempt to extract the variable name from the file name\n",
    "        \n",
    "        except KeyError:\n",
    "            v = vars_dict[file.split('/')[-1].split('_')[0][0:-3]]            # Adjust the variable extraction for special cases\n",
    "\n",
    "        out_nam = special_nam_handler_cdo(file, v)                            # Call a special handler function for NAM data processing\n",
    "\n",
    "        coords_to_keep = ['time', 'latitude', 'longitude', 'y', 'x']          # Define the coordinates to keep\n",
    "        out_nam_coords = list(out_nam.coords)                                 # List all coordinates in the output dataset\n",
    "        \n",
    "        if len(coords_to_keep) != len(out_nam_coords):                        # Check if there are extra coordinates\n",
    "            delete = [i for i in out_nam_coords if i not in coords_to_keep]   # Identify coordinates to delete\n",
    "            out_nam = out_nam.drop(delete)                                    # Drop unwanted coordinates\n",
    "\n",
    "        out_nam.to_netcdf(f'../database_files/NAM_added_grid/{f}')            # Save the processed dataset to a new NetCDF file\n",
    "        print(f)                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb923f3-3c26-4e90-8bcc-e6c48507da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_file = xr.open_dataset(f'../database_files/NAM/Climos/cape_AVG_NAM_HISTORICAL_Daily_climos_full_period.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4e793-103f-451b-8d96-fda1cc4b9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nam_dir = f'../database_files/NAM/Climos'\n",
    "\n",
    "for f in os.listdir(nam_dir):\n",
    "\n",
    "    ds = xr.open_dataset(f'{nam_dir}/{f}')\n",
    "\n",
    "    ds['latitude'] = g_file['latitude']\n",
    "    ds['longitude'] = g_file['longitude']\n",
    "\n",
    "    ds.to_netcdf(f'../database_files/NAM_added_grid/{f}')\n",
    "    print(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec85873-1b00-40d5-a0e9-f3a354dbb338",
   "metadata": {},
   "source": [
    "## CONSU404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0b2d0-dfde-483f-9f85-f73aec53c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_c404_handler_cdo(file, v):\n",
    "\n",
    "    ''' This function takes problematic CONUS404 files from the database \n",
    "        and adds a non-generic grid.\n",
    "    \n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    file: (xarray dataset) a CONUS404 file opened with xarray\n",
    "    v: (str) variable name\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    C404_grid: (xarray dataset) the new, cleaned version of the input \n",
    "    \n",
    "    '''\n",
    "    C404_grid = xr.open_dataset('../database_files/CONUS404/TD2_CONUS404_ANALYSIS_Abs_2012.nc')              # Open a C404 dataset to use its grid information and drop the 't2m' variable, always use a leap year!!! \n",
    "    \n",
    "    if 'Daily' in file.split('/')[-1].split('_'):                                                            # Check if the file represents daily data\n",
    "        C404_grid = C404_grid.resample(Time = '24H').mean('Time')                                            # Resample the grid data to daily frequency and compute the mean\n",
    "            \n",
    "    df = xr.open_dataset(file)                                                                               # Open the dataset file\n",
    "    \n",
    "    C404_grid = C404_grid.isel(Time = np.arange(0, len(df.Time)))\n",
    "            \n",
    "    C404_grid['Time'] = df['Time']                                                                           # Update the 'time' coordinate in the grid dataset to match the data file\n",
    "            \n",
    "    # Create a DataArray with the data values, dimensions, and coordinates aligned with the C404 grid\n",
    "    df_vals_da = xr.DataArray(\n",
    "    data=df[v].values,\n",
    "    dims=['Time', 'south_north', 'west_east'],  \n",
    "    coords={\n",
    "        'Time': C404_grid.coords['Time'],  \n",
    "        'XLAT': (('south_north', 'west_east'), C404_grid.coords['XLAT'].data),  \n",
    "        'XLONG': (('south_north', 'west_east'), C404_grid.coords['XLONG'].data) \n",
    "    }\n",
    ")\n",
    "    C404_grid = C404_grid.drop('TD2')   \n",
    "    C404_grid[v] = df_vals_da                                                                                # Assign the newly created DataArray to the C404 grid dataset under the variable v\n",
    "            \n",
    "    return C404_grid                                                                                         # Return the updated C404 grid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c931c-828a-4aec-b0ac-4dcfc1f99f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = os.listdir('../CONUS404')\n",
    "vars.append('vpd')\n",
    "vars.append('wspeed')\n",
    "vars.append('wdir')\n",
    "vars.append('hdwi')\n",
    "vars.append('ffwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206349f-3f49-4eab-a34d-d096feea3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dict = {}\n",
    "\n",
    "for v in vars:\n",
    "\n",
    "    vars_dict[v] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc2519e-e8ca-4636-8404-a2bdc9affbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../database_files/CONUS404/'                                     # Define the input directory for C404 files\n",
    "\n",
    "for f in c44_no:                                                              # Iterate over the list of C404 files\n",
    "    if f[-2:] == 'nc':                                                        # Check if the file is a NetCDF file\n",
    "        \n",
    "        file = f'{input_dir}/{f}'                                             # Construct the full file path\n",
    "\n",
    "        try:\n",
    "            v = vars_dict[file.split('/')[-1].split('_')[0]]                  # Attempt to extract the variable name from the file name\n",
    "        \n",
    "        except KeyError:\n",
    "            v = vars_dict[file.split('/')[-1].split('_')[0][0:-3]]            # Adjust the variable extraction for special cases\n",
    "\n",
    "        out_C404 = special_c404_handler_cdo(file, v)                          # Call a special handler function for C404 data processing\n",
    "\n",
    "        out_C404.to_netcdf(f'../database_files/CONUS404_added_grid/{f}')      # Save the processed dataset to a new NetCDF file\n",
    "        print(f)                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa29bc3-69a3-408b-bf55-dd046603490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in c404_no_climo[1:]:\n",
    "    \n",
    "    ex_file = xr.open_dataset(f'{parent_in}/T2_MAX_CONUS404_ANALYSIS_Daily_climos_full_period.nc')\n",
    "    ex_file = ex_file.drop('T2_climo')\n",
    "\n",
    "    \n",
    "    ds = xr.open_dataset(f'{parent_in}/{f}')\n",
    "    \n",
    "    ex_file['Time'] = ds['Time']\n",
    "    # ex_file['XTIME'] = ds['XTIME']\n",
    "\n",
    "    ex_file['ffwi_climo'] = ds['ffwi_climo']\n",
    "    \n",
    "    ex_file.to_netcdf(f'../database_files/CONUS404_2/{f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e71eb7-79b5-4c1a-a2c1-d460f26b3d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26f65d1f-05be-498e-a0a4-aa7cff47cb3c",
   "metadata": {},
   "source": [
    "## HRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a224c-f3e7-4d91-8efe-037cb8a0655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_HRRR_handler_cdo(file, v):\n",
    "\n",
    "    ''' This function takes problematic HRRR files from the database \n",
    "        and add a non-generic grid.\n",
    "    \n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    file: (xarray dataset) a HRRR file opened with xarray\n",
    "    v: (str) variable name\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    HRRR_grid: (xarray dataset) the new, cleaned version of the input \n",
    "    \n",
    "    '''\n",
    "    HRRR_grid = xr.open_dataset('../database_files/HRRR/t2m_HRRR_HISTORICAL_Abs_2017.nc')                    # Open a HRRR dataset to use its grid information and drop the 't2m' variable\n",
    "    \n",
    "    if 'Daily' in file.split('/')[-1].split('_'):                                                            # Check if the file represents daily data\n",
    "        HRRR_grid = HRRR_grid.resample(time = '24H').mean('time')                                            # Resample the grid data to daily frequency and compute the mean\n",
    "            \n",
    "    df = xr.open_dataset(file)                                                                               # Open the dataset file\n",
    "\n",
    "    HRRR_grid = HRRR_grid.drop('t2m')\n",
    "    HRRR_grid['time'] = df['time']                                                                           # Update the 'time' coordinate in the grid dataset to match the data file\n",
    "\n",
    "    df_vals_da = xr.DataArray(\n",
    "    data=df[v].values,\n",
    "    dims=['time', 'y', 'x'],  \n",
    "    coords={\n",
    "        'time': HRRR_grid.coords['time'],  \n",
    "        'latitude': (('y', 'x'), HRRR_grid.coords['latitude'].data),  \n",
    "        'longitude': (('y', 'x'), HRRR_grid.coords['longitude'].data),\n",
    "        'x': HRRR_grid.coords['x'],\n",
    "        'y': HRRR_grid.coords['y']\n",
    "    }\n",
    ")\n",
    "       \n",
    "    HRRR_grid[v] = df_vals_da                                                                                # Assign the newly created DataArray to the HRRR grid dataset under the variable v\n",
    "            \n",
    "    return HRRR_grid                                                                                         # Return the updated HRRR grid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441a6ae-4475-4905-a3e6-42e1c2c63b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcb64e-e1dc-4561-bdc1-8368d1071922",
   "metadata": {},
   "outputs": [],
   "source": [
    "added = os.listdir('../database_files/HRRR_added_grid/')\n",
    "added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5148b-4de9-4800-b590-e37d66020d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['blh',\n",
    " 'cape',\n",
    " 'mstav',\n",
    " 'u10',\n",
    " 'v10',\n",
    " 'gust',\n",
    " 'prate',\n",
    " 'tp',\n",
    " 't2m',\n",
    " 'd2m',\n",
    " 'wspeed',\n",
    " 'wdir',\n",
    " 'hdwi',\n",
    " 'vpd',\n",
    " 'ffwi', 'rh']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09896ad-d23c-4a4d-9466-8b62d5bd1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_dict = {}\n",
    "\n",
    "for v in vars:\n",
    "\n",
    "    vars_dict[v] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde140f5-f842-424d-91a1-83e2230519cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../database_files/HRRR/'                                         # Define the input directory for HRRR files\n",
    "test_files = []\n",
    "for f in hrrr_no:                                                             # Iterate over the list of HRRR files\n",
    "    if f[-2:] == 'nc':                                                        # Check if the file is a NetCDF file\n",
    "        \n",
    "        file = f'{input_dir}/{f}'                                             # Construct the full file path\n",
    "\n",
    "        try:\n",
    "            v = vars_dict[file.split('/')[-1].split('_')[0]]                  # Attempt to extract the variable name from the file name\n",
    "        \n",
    "        except KeyError:\n",
    "            v = vars_dict[file.split('/')[-1].split('_')[0][0:-3]]            # Adjust the variable extraction for special cases\n",
    "\n",
    "        t = xr.open_dataset(file)\n",
    "        v = list(t.variables)[0]\n",
    "\n",
    "        out_HRRR = special_HRRR_handler_cdo(file, v)                          # Call a special handler function for HRRR data processing\n",
    "\n",
    "        out_HRRR.to_netcdf(f'../database_files/HRRR_added_grid/{f}')          # Save the processed dataset to a new NetCDF file\n",
    "        print(f)                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fafdfd-9aa4-46dd-8650-a086854c7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in hrrr_no_climo:\n",
    "    \n",
    "    ex_file = xr.open_dataset(f'{parent_in}/t2m_HRRR_HISTORICAL_Abs_climos_full_period.nc')\n",
    "    ex_file = ex_file.drop('t2m_climo')\n",
    "\n",
    "    \n",
    "    ds = xr.open_dataset(f'{parent_in}/{f}')\n",
    "    \n",
    "    ex_file['time'] = ds['time']\n",
    "\n",
    "    ex_file['ffwi_climo'] = ds['ffwi_climo']\n",
    "    \n",
    "    ex_file.to_netcdf(f'../database_files/HRRR2/{f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649678b-6888-4923-bbdb-1e6478224c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
