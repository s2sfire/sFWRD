{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99429ee8-7187-492e-8eac-9f7e9975eebe",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://cdn.miami.edu/_assets-common/images/system/um-logo-gray-bg.png\" alt=\"Miami Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"float:right; width:98 px; height:98px;\">\n",
    "<img src=\"https://media.licdn.com/dms/image/C4E0BAQFlOZSAJABP4w/company-logo_200_200/0/1548285168598?e=2147483647&v=beta&t=g4jl8rEhB7HLJuNZhU6OkJWHW4cul_y9Kj_aoD7p0_Y\" alt=\"STI Logo\" style=\"height: 98px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "<h1>Data Download</h1>\n",
    "By: Kayla Besong, PhD\n",
    "    <br>\n",
    "Last Edited: 01/09/24\n",
    "<br>\n",
    "<br>    \n",
    "<br>\n",
    "This code is designed to download fire weather variables from multiple renalaysis and forecast products including the HRRR, NAM, NARR, CONUS404, NCEP Reanalysis II, and ERA5. A complementary notebook 'Data_Grab_Functions.ipynb' hosts the suite of functions tailored to each product's download process that this notebook leverages. Each product has various input parameters specific to itself such as the dates the product is available for, timesteps, variables, etc. This notebook was designed with intentention for those inputs to be changed depending on user need. Below it is tailored to fire weather metrics including: temperature, u-,v-wind components, relative humidty, soil moisture, planetary boundary layer height (mixing height), CAPE, and either precipitation accumulation or precipitation rate or both. If additional or changed variables are desired it is often necessary to find the level or surface your variable is on and align it properly with the examples below. Each function will generate a file tree and save the files to that file tree structure, with the only object returned being a list of unavailable files for your specified dates/hours/variables. Lastly, be patient sometimes the servers from which you are requesting fies from can be slow. Approximate run times for example cases are provided in each section.\n",
    "    \n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52113d7-40db-4d58-a67d-f99dc93f3d0a",
   "metadata": {},
   "source": [
    "### The integral notebook of functions to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2aee1e5-a93b-4541-a6d6-af8f67a65d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../Universal_Functions/Data_Grab_Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea8f0db-abc5-4d60-9788-c5bbb9376c69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HRRR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee94d74-c63c-45c0-a48b-1dd992e0c648",
   "metadata": {},
   "source": [
    "source: AWS or Google\n",
    "<br>\n",
    "file type: zarr or grib2\n",
    "<br>\n",
    "zarr dates available from: 2016-08-23 to Present; grib2 dates = 2014-Present\n",
    "<br>\n",
    "analysis time steps: 0 to 23hr by 1 (not all always available) \n",
    "<br>\n",
    "domain: all of the HRRR domain is downloaded covering most of North America\n",
    "<br>\n",
    "<br>\n",
    "Resources on HRRR AWS Zarr:\n",
    "<br>\n",
    "https://registry.opendata.aws/noaa-hrrr-pds/\n",
    "<br>\n",
    "https://hrrrzarr.s3.amazonaws.com/index.html\n",
    "<br>\n",
    "https://mesowest.utah.edu/html/hrrr/\n",
    "<br>\n",
    "<br>\n",
    "Resources on HRRR Google Cloud:\n",
    "<br>\n",
    "https://console.cloud.google.com/marketplace/product/noaa-public/hrrr?project=python-232920&pli=1\n",
    "<br>\n",
    "<br>\n",
    "Estimated time to run zarr: 1-year, 4x daily, 9 vars = ~18h hours; ~0.5T\n",
    "<br>\n",
    "<br>\n",
    "output:\n",
    "<br>\n",
    "1. A list of all missing or incomplete files\n",
    "2. A file tree structured HRRR/variable/hrrr_variable_YEARMONDAY_HR.nc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a5a438-b486-4368-a728-db08ffaa97c7",
   "metadata": {},
   "source": [
    "On AWS Zarr, the files are stored as analysis (F00) and forecast (F01-FXX). Analysis is the only thing available until mid 2018 for the 2016-08-23 to Present stored. There is another bucket that stores the grib2 files, but that would require simple-caching as in the codes for UFS (see function script if curious). The reason a second, grib2-Google script was develop was due to lack of forecast data stored in zarr format. So to get anything, such as precipitation for F01 (blank for F00) prior to 2018, you need to use the google method which treats F00 as analysis and downloads F00 only by default unless specifying the \"fcst_hr_step\" below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7104d06-d778-4162-bdb6-91f73b7ea2bb",
   "metadata": {},
   "source": [
    "## AWS ZARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd21934-27f9-49db-b4ae-8ecd5197b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables are set up in a dictionary format as not all variables are stored with the same coordinates.\n",
    "### The keys represent the 'level' or coordinate each variable is found on and is used in determining the path \n",
    "### to obtain the data. Each variable of interest is stored in a list as the 'values' of the dictionary,\n",
    "### pairing to the level 'keys' of the dictionary. \n",
    "\n",
    "\n",
    "variables = {'2m_above_ground': ['TMP', 'RH'],\n",
    "             '10m_above_ground': ['UGRD', 'VGRD'],\n",
    "             'surface': ['HPBL', 'CAPE', 'GUST'],\n",
    "             '0m_underground': ['MSTAV']}\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f080b8-217b-43f2-b032-94b5493423dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### inputs to the hrrr_grabber function are established here. \n",
    "### start_date, end_date, and output_dir = strings\n",
    "### hour_range = array of timesteps you would like to grab. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. Leave in numerics.\n",
    "\n",
    "\n",
    "start_date = '2016-08-23'\n",
    "end_date = '2018-12-31'\n",
    "hour_range = np.arange(0, 24, 6)\n",
    "output_dir = 'HRRR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e6a84-fb62-43f9-baff-263aa6be7874",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Calling the HRRR grabber function to download all data of interest. The only item that is returned is a \n",
    "### list of files that does not exists (is missing or otherwise) for one to be aware of and double check on. \n",
    "\n",
    "\n",
    "### OPTIONAL ####\n",
    "### choose if you want the analysis (i.e. F00) or the forecast (F01-FXX). For some variables such as PRATE, only forecast is available. \n",
    "### The input time/hour will be the forecast init time. Function returns all forecasted hours. \n",
    "\n",
    "non_exist_hrr = hrrr_zarr_grabber(start_date, end_date, hour_range, variables, output_dir, forecast = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b60bc0-5357-4eca-a137-79228fb5e71e",
   "metadata": {},
   "source": [
    "## Google Grib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb97929-30b8-400c-ac2c-e7d8f019ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables are set up in a dictionary format as not all variables are stored with the same coordinates.\n",
    "### The keys represent basically whatever you want. Each variable of interest is stored in a list of lists as the 'values' of the dictionary.\n",
    "### The additional list is the 'filter_by_keys' input for each of the variables required by xarray-cfgrib to obtain the variables of interest.\n",
    "### Each variable at a minimum requires 'typeOfLevel' while others require more such as below. If you add/change variables, some investigation\n",
    "### as to which filters are needed may be required. Presently, you cannot provide multiple choices per key-value pair. For example, for the first\n",
    "### two they both are on the typeOfLevel: 'heightAboveGround' but require seperate 'level' values...level: [2, 10] will return an empty xr.dataset\n",
    "### due to xarray-cfgrib not \"liking\" the different 'coordinates'. Best of luck! \n",
    "\n",
    "\n",
    "variables = {'heightAboveGround2': [['t2m', 'd2m'], {'typeOfLevel': 'heightAboveGround', 'level': 2}],\n",
    "             'heightAboveGround10': [['u10', 'v10'], {'typeOfLevel': 'heightAboveGround', 'level': 10}],\n",
    "             'surface1': [['gust', 'blh', 'cape', 'prate', 'lsm'], {'typeOfLevel': 'surface', 'stepType': 'instant'}],\n",
    "             'surface2': [['tp'], {'typeOfLevel': 'surface', 'stepType': 'accum'}],\n",
    "             'depthBelowLandLayer': [['mstav'], {'typeOfLevel': 'depthBelowLand'}]}\n",
    "\n",
    "\n",
    "precip_vars = {'surface1': [['prate'], {'typeOfLevel': 'surface', 'stepType': 'instant'}],\n",
    "              'surface2': [['tp'], {'typeOfLevel': 'surface', 'stepType': 'accum'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248af55-8ffa-444e-ac9b-746c9c6f4dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### inputs to the hrrr_grabber function are established here. \n",
    "### start_date, end_date, and output_dir = strings\n",
    "### hour_range = array of timesteps you would like to grab. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. Leave in numerics.\n",
    "\n",
    "\n",
    "start_date = '2014-07-30'\n",
    "end_date = '2018-12-31'\n",
    "hour_range = np.arange(0, 24, 6)\n",
    "output_dir = 'HRRR2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d60e4-4718-43ee-a532-93a27b9475f1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Calling the HRRR grabber function to download all data of interest. The only item that is returned is a \n",
    "### list of files that does not exists (is missing or otherwise) for one to be aware of and double check on. \n",
    "\n",
    "\n",
    "### OPTIONAL ####\n",
    "### choose if you want the analysis (i.e. F00) or the forecast (F01-FXX). For some variables such as PRATE, only forecast is available. \n",
    "### The input time/hour will be the forecast init time. Function returns all forecasted hours. \n",
    "\n",
    "\n",
    "non_exist_hrr = hrrr_google_grabber(start_date, end_date, hour_range, variables, output_dir, fcst_hr_step = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184a4be-b351-467d-9112-d960ad8bb287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "non_exist_hrr = hrrr_google_grabber(start_date, end_date, hour_range, precip_vars, output_dir, fcst_hr_step = [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9868fe2-9238-4b55-9398-e0d4c1a4c5ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# NAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0f947-663c-4f46-9474-7758f932ac3c",
   "metadata": {},
   "source": [
    "source: NCEI Data https://www.ncei.noaa.gov/data/north-american-mesoscale-model/\n",
    "<br>\n",
    "file type: grib, grib2 \n",
    "<br>\n",
    "dates available from: 2004-03-03 - 2020-05-15\n",
    "<br>\n",
    "hour time steps: 00, 06, 12, 18\n",
    "<br>\n",
    "lead times per time step: 00, 03, 06 (until 2012-12-31, on 20200515) --> 00, 01, 02, 03, 06 2013-01-01 through 2020-05-14\n",
    "<br>\n",
    "domain: all of the NAM domain is downloaded covering most of North America\n",
    "<br>\n",
    "<br>\n",
    "Resources on NAM:\n",
    "<br>\n",
    "https://www.ncei.noaa.gov/products/weather-climate-models/north-american-mesoscale\n",
    "<br>\n",
    "<br>\n",
    "Estimated time to run: 1-year, 4x daily, 6-11 vars = ~5.5 hours; ~72GB\n",
    "<br>\n",
    "<br>\n",
    "output:\n",
    "<br>\n",
    "1. A list of all missing or incomplete files\n",
    "2. A file tree structured NAM/variable/nam_variable_YEARMONDAY_HR_FCSTHR.nc\n",
    "\n",
    "NOTE: to read grib/grib2 off of this server, this code will store temporary files that can manually be deleted later. Controlling where the files are cached has proven difficult. This method may work on your machine, it may not depending on what is set up. Within the nam_grabber function in the Data_Grab_Functions.ipynb there are 5 lines commented out that delete the cached grib files as you go. If you find that all the cached files are causing a problem, you can uncomment these lines to automatically delete them. It is advised to print 'file' before doing so to become aware of where the cached files are being stored or generated first. This method uses shutil.rmtree() and could delete things you do not want deleted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca2679-2755-49d9-947e-54135f04217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables are set up in a dictionary format as not all variables are stored with the same coordinates.\n",
    "### The keys represent basically whatever you want. Each variable of interest is stored in a list of lists as the 'values' of the dictionary.\n",
    "### The additional list is the 'filter_by_keys' input for each of the variables required by xarray-cfgrib to obtain the variables of interest.\n",
    "### Each variable at a minimum requires 'typeOfLevel' while others require more such as below. If you add/change variables, some investigation\n",
    "### as to which filters are needed may be required. Presently, you cannot provide multiple choices per key-value pair. For example, for the first\n",
    "### two they both are on the typeOfLevel: 'heightAboveGround' but require seperate 'level' values...level: [2, 10] will return an empty xr.dataset\n",
    "### due to xarray-cfgrib not \"liking\" the different 'coordinates'. Best of luck! \n",
    "\n",
    "### For NAM specifically, the variables change after an update that occurs in spring 2017. Thus if you need prior to and/or after different dictionaries \n",
    "### of variables and their levels will need to be passed. You only have to input the ones created, each is optional but at least one is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783bf023-6f13-44c3-ad06-50d653a212ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_b4_04092017 = {'heightAboveGround2': [['t2m', 'r'], {'typeOfLevel': 'heightAboveGround', 'level': 2}],\n",
    "                         'heightAboveGround10': [['u10', 'v10'], {'typeOfLevel': 'heightAboveGround', 'level': 10}],\n",
    "                         'surface': [['tp'], {'typeOfLevel': 'surface', 'stepType': 'accum'}],\n",
    "                         'depthBelowLandLayer': [['sm'], {'typeOfLevel': 'depthBelowLandLayer', 'shortName': 'sm'}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669237b4-511c-4434-905b-46c4f60b9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_grib2 = {'heightAboveGround2': [['t2m', 'r2'], {'typeOfLevel': 'heightAboveGround', 'level': 2}],\n",
    "             'heightAboveGround10': [['u10', 'v10'], {'typeOfLevel': 'heightAboveGround', 'level': 10}],\n",
    "             'surface': [['gust', 'hpbl', 'cape', 'lsm', 'hindex'], {'typeOfLevel': 'surface', 'stepType': 'instant'}],\n",
    "             'surface_accum': [['tp'], {'typeOfLevel': 'surface', 'stepType': 'accum'}],\n",
    "             'depthBelowLandLayer': [['soilw'], {'typeOfLevel': 'depthBelowLandLayer', 'shortName': 'soilw'}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a0a52-9400-4056-945b-52771632c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "### inputs to the nam_grabber function are established here. \n",
    "### start_date, end_date, and output_dir = strings\n",
    "### hour_range = array of timesteps you would like to grab. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. Leave in numerics.\n",
    "\n",
    "start_date = '2017-11-27'\n",
    "end_date = '2018-12-31'\n",
    "hour_range = np.arange(0, 24, 6)\n",
    "output_dir = 'NAM'\n",
    "\n",
    "\n",
    "### OPTIONAL ####\n",
    "### forecast hour step of list of integers. The default = None in the function which will return only the ['00'] forecast for each init hour and date. \n",
    "### edit and uncomment the list of integers below to your desired lead times if ['00'] is not sufficient. The below is all that is available in thredds. Info above. \n",
    "\n",
    "## note that if you are doing total precipitation the 0 hr fcst will be blank, 1hr forecast needed to get a signal\n",
    "\n",
    "### fcst_hr_step = [0, 1, 2, 3, 6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22c639-4e29-43e4-8c01-2908f8dd4ca0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#non_exist_nam = nam_grabber(start_date, end_date, hour_range, output_dir, variables_grib = None, variables_grib2 =None, fcst_hr_step = None)\n",
    "\n",
    "non_exist_nam = nam_grabber(start_date, end_date, hour_range, output_dir, variables_grib = variables_b4_04092017, variables_grib2 =variables_grib2, fcst_hr_step = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3bfb1-2683-4908-a949-8094a686cf5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NARR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9c6ba-5205-41ea-ab63-1df3943fbb71",
   "metadata": {},
   "source": [
    "source: THREDDS\n",
    "<br>\n",
    "file type: grib\n",
    "<br>\n",
    "dates available from: 1971-01-01 to 2014-10-02\n",
    "<br>\n",
    "hour time steps: 0 to 21 by 3 ([ 0,  3,  6,  9, 12, 15, 18, 21])\n",
    "<br>\n",
    "domain: all of the NARR domain is downloaded covering all of North America including Alaska and Hawai'i \n",
    "<br>\n",
    "<br>\n",
    "Resources on NARR:\n",
    "<br>\n",
    "https://www.ncei.noaa.gov/products/weather-climate-models/north-american-regional\n",
    "<br>\n",
    "<br>\n",
    "Estimated time to run: 1-year, 4x daily, 10 vars = ~6 hours; ~6.6GB\n",
    "<br>\n",
    "<br>\n",
    "output:\n",
    "<br>\n",
    "1. A list of all missing or incomplete files\n",
    "2. A file tree structured NARR/variable/narr_variable_YEARMONDAY_HR.nc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb200bda-ae51-4400-86ca-848ab4a84561",
   "metadata": {},
   "outputs": [],
   "source": [
    "### variables are just a list of variables but with their associated pressure level \n",
    "\n",
    "variables = ['Temperature_height_above_ground', 'u-component_of_wind_height_above_ground', 'v-component_of_wind_height_above_ground', 'Relative_humidity_height_above_ground',\n",
    "             'Soil_moisture_content_layer_between_two_depths_below_surface_layer', 'Planetary_boundary_layer_height_surface', 'Convective_Available_Potential_Energy_surface',\n",
    "             'Convective_Available_Potential_Energy_layer_between_two_pressure_difference_from_ground_layer',\n",
    "             'Total_precipitation_surface_3_Hour_Accumulation', 'Precipitation_rate_surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c4253a-69fc-4ea4-bc0a-81dda9103fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### inputs to the narr_grabber function are established here. \n",
    "### start_date, end_date, and output_dir = strings\n",
    "### hour_range = array of timesteps you would like to grab. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. Leave in numerics.\n",
    "\n",
    "start_date = '2011-01-01'\n",
    "end_date = '2011-12-31'\n",
    "hour_range = np.arange(0, 24, 6)\n",
    "output_dir = 'NARR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e34c4f-6aca-47ca-9654-f9a05f230efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Calling the NARR grabber function to download all data of interest. The only item that is returned is a \n",
    "### list of files that does not exists (is missing or otherwise) for one to be aware of and double check on. \n",
    "\n",
    "non_exist_narr = narr_grabber(start_date, end_date, hour_range, variables, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b8451-8915-4ae7-beb8-87595b59b806",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CONUS404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b12bc-2419-4409-aa26-34512ce8b111",
   "metadata": {},
   "source": [
    "source: THREDDS\n",
    "<br>\n",
    "file type: netCDF\n",
    "<br>\n",
    "dates available from: 1979-10-01 to 2022-09-30; by water year --> starts in October of year N, ends in September of year N+1\n",
    "<br>\n",
    "hour time steps: 0 to 23 by 1 (all hours of day)\n",
    "<br>\n",
    "domain: all of the CONUS404 domain is downloaded covering all of the U.S. \n",
    "<br>\n",
    "<br>\n",
    "Resources on CONUS404:\n",
    "<br>\n",
    "https://rda.ucar.edu/datasets/ds559.0/\n",
    "<br>\n",
    "https://journals.ametsoc.org/view/journals/bams/104/8/BAMS-D-21-0326.1.xml\n",
    "<br>\n",
    "<br>\n",
    "Estimated time to run: 1-year, 4x daily, 10 vars = ~4 hours; ~200GB\n",
    "<br>\n",
    "<br>\n",
    "output:\n",
    "<br>\n",
    "1. A list of all missing or incomplete files\n",
    "2. A file tree structured CONUS404/variable/conus404_variable_YEARMONDAY_HR.nc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45c4796-e4d8-421d-8fc1-f1acced99205",
   "metadata": {},
   "outputs": [],
   "source": [
    "### variables are just a list of variables  \n",
    "\n",
    "#variables = ['T2', 'U10', 'V10', 'TD2', 'SMOIS', 'PBLH', 'SBCAPE', 'MLCAPE', 'PREC_ACC_NC']\n",
    "variables = ['SMOIS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f78812dd-fa81-4203-b54d-43051dac97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### inputs to the conus404_grabber function are established here. \n",
    "### start_date, end_date, and output_dir = strings\n",
    "### hour_range = array of timesteps you would like to grab. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. Leave in numerics.\n",
    "\n",
    "start_date = '2016-01-01'\n",
    "end_date = '2016-01-01'\n",
    "hour_range = np.arange(0, 24, 6)\n",
    "output_dir = 'CONUS404'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06fe701-71f2-415c-bcee-c89187fc114e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2sfire/miniconda3/envs/noaa_s2s/lib/python3.12/site-packages/xarray/backends/plugins.py:80: RuntimeWarning: Engine 'gini' loading failed:\n",
      "Struct() takes at most 1 argument (3 given)\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all of 2016-01-01 00:00:00 has been saved\n",
      "CPU times: user 3.75 s, sys: 971 ms, total: 4.72 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Calling the CONUS404 grabber function to download all data of interest. The only item that is returned is a \n",
    "### list of files that does not exists (is missing or otherwise) for one to be aware of and double check on. \n",
    "\n",
    "non_exist_conus404 = conus404_grabber(start_date, end_date, hour_range, variables, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef678426-75ac-45d5-bd7d-6cb9687246a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# NCEP Reanalysis II "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38952678-e47f-44fe-927e-1e663f416f63",
   "metadata": {},
   "source": [
    "source: THREDDS\n",
    "<br>\n",
    "file type: netCDF\n",
    "<br>\n",
    "dates available from: 1979 to present; BY YEAR \n",
    "<br>\n",
    "domain: the domain is subset to 85N-0N, 180W-360W in this code to cover all of North America including Alaska and Hawai'i; global available\n",
    "<br>\n",
    "<br>\n",
    "Resources on NCEP Renalysis II:\n",
    "<br>\n",
    "https://www.ncei.noaa.gov/products/weather-climate-models/reanalysis-1-2\n",
    "<br>\n",
    "<br>\n",
    "Estimated time to run: 1-year, 4x daily, 7 vars, 1 level each = ~ 30min; ~130MB\n",
    "<br>\n",
    "<br>\n",
    "output:\n",
    "<br>\n",
    "1. A list of all missing or incomplete files\n",
    "2. A file tree structured NCEP/variable/ncep_variable_YEAR.nc or ncep_variable_YEAR_LEVELmb.nc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089563a-a376-4e8a-aa45-fcb233f333a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables are set up in a dictionary format as not all variables are stored with the same coordinates.\n",
    "### The keys represent the 'level' or coordinate each variable is found on and is used in determining the path \n",
    "### to obtain the data. Each variable of interest is stored in a list as the 'values' of the dictionary,\n",
    "### pairing to the level 'keys' of the dictionary. \n",
    "\n",
    "variables = {'pressure': ['air', 'uwnd', 'vwnd', 'rhum'],\n",
    "             'gaussian_grid': ['soilw.0-10cm.gauss', 'prate.sfc.gauss']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d32e7e-b2f2-46e6-bd1e-457634580b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### inputs to the conus404_grabber function are established here. \n",
    "### start_date, end_date, and output_dir = strings\n",
    "### domain == array of integers --> [N, S, E, W] presently set up for NA domain\n",
    "\n",
    "\n",
    "start_date = '1979-12-01'\n",
    "end_date = '2022-02-28'\n",
    "output_dir = 'NCEP'\n",
    "domain = [85, 0, 180, 360]\n",
    "\n",
    "### OPTIONAL ####\n",
    "### levels = list of float values. The default = None in the function which will return only the [1000] level for each year. \n",
    "### edit and uncomment the list of floats below to your desired levels. This is only for variables on pressure surfaces!! \n",
    "\n",
    "### levels = [1000.,  925.,  850.,  700.,  600.,  500.,  400.,  300.,  250.,  200., 150.,  100.,   70.,   50.,   30.,   20.,   10.] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff30c0-8398-4b0d-8c44-a2ba172d47d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Calling the NCEP grabber function to download all data of interest. The only item that is returned is a \n",
    "### list of files that does not exists (is missing or otherwise) for one to be aware of and double check on. \n",
    "### warning: the opendap process with this function can be particular or slow \n",
    "\n",
    "non_exist_ncep = ncep_grabber(start_date, end_date, variables, output_dir, domain, levels = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e2569-1c82-49f3-b64d-097dac61fd89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ERA5 on single levels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bde27-d748-4b2e-9fe3-bdc8fcc177e3",
   "metadata": {},
   "source": [
    "source: copernicus\n",
    "<br>\n",
    "file type: netCDF\n",
    "<br>\n",
    "dates available from: 1940 to present\n",
    "<br>\n",
    "hour time steps: 0 to 23 by 1 (all hours of day)\n",
    "<br>\n",
    "domain: the domain is subset to 85N-0N, 180W-360W in this code to cover all of North America including Alaska and Hawai'i; global available\n",
    "<br>\n",
    "<br>\n",
    "Resources on ERA5:\n",
    "<br>\n",
    "https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation\n",
    "<br>\n",
    "<br>\n",
    "Estimated time to run: 1-year, 4x daily, 11 vars = ~30 min download, 1 hour waiting; ~7.5GB\n",
    "<br>\n",
    "<br>\n",
    "output:\n",
    "<br>\n",
    "1. A file tree structured ERA5/era5_YEAR.nc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091de39-0a02-4389-98a7-76ad230a0d5d",
   "metadata": {},
   "source": [
    "## This process uses dask, multiprocess pool to expedite the process. \n",
    "The correct packages to align this may be difficult. \n",
    "<br>\n",
    "\n",
    "## You also need install the Copernicus client. \n",
    "\n",
    "Incorrect input will immediately lead to error. Another immediate error related to 'signing in' to the Copernicus CDS may pop up. If you click the link and sign in, then it should be fine. Follow more instructions below:\n",
    "<br>\n",
    "Linux: https://cds.climate.copernicus.eu/api-how-to\n",
    "<br>\n",
    "Mac: https://confluence.ecmwf.int/display/CKB/How+to+install+and+use+CDS+API+on+macOS\n",
    "<br>\n",
    "Windows: https://confluence.ecmwf.int/display/CKB/How+to+install+and+use+CDS+API+on+Windows\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2da502-2a31-4eb7-82ea-b23baf635c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### variables are just a list of variables\n",
    "\n",
    "variables = ['10m_u_component_of_wind', '10m_v_component_of_wind', '2m_dewpoint_temperature',\n",
    "    '2m_temperature'] \n",
    "\n",
    "#, 'boundary_layer_height', 'convective_available_potential_energy',\n",
    "    # 'convective_precipitation', 'instantaneous_10m_wind_gust', 'large_scale_rain_rate',\n",
    "    # 'total_precipitation', 'volumetric_soil_water_layer_1',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f3ba04-5311-43dd-b327-d1d3463b7f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### inputs to the conus404_grabber function are established here. \n",
    "### start_date, end_date, and output_dir = strings\n",
    "### hour_range = array of timesteps you would like to grab. For example, np.arange(0, 24, 6) will produce [00, 06, 12, 18] in the function. Leave in numerics.\n",
    "### domain == array of integers --> [N, S, E, W] presently set up for NA domain\n",
    "\n",
    "start_date = '1979-12-01'\n",
    "end_date = '2022-02-28'\n",
    "hour_range = np.arange(0, 24, 6)\n",
    "output_dir = 'ERA5'\n",
    "domain = [85, 0, 180*-1, 360]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c03c9-48be-4e58-bf12-1ea06c31c637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Calling the NCEP grabber function to download all data of interest. Nothing is returned, files are just saved.\n",
    "### warning: the download process with this function can be particular or slow at times \n",
    "### ''INFO Request is queued'' = you are waiting on their server\n",
    "\n",
    "era_grabber(start_date, end_date, hour_range, variables, output_dir, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9ed7f-a554-40e1-9356-fa0ae297536c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NOTE:\n",
    "This file tree is not like the others and is lumped into all variables per year. To have the files re-organized by variable_year.nc run the following function with the 'infile_vars'. These are the variables in the returned netcdf which have different names from the request variables. You might have to peak at one file for the accurate names or check the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec970b-de46-44d9-98c5-25038f0aede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def era_dir_maker(input_dir, variables):\n",
    "\n",
    "    for files in os.listdir(input_dir):\n",
    "        if files[0] == 'e':\n",
    "            print(files)\n",
    "                \n",
    "            for v in variables:\n",
    "                            \n",
    "                dir_maker(f'{input_dir}/{v}')                          \n",
    "    \n",
    "                filename = f'{files[0:4]}_{v}_{files[5:9]}.nc'\n",
    "    \n",
    "                if filename in os.listdir(f'{input_dir}/{v}'):\n",
    "    \n",
    "                    print(f'{filename}.nc has already been saved')\n",
    "    \n",
    "                else:\n",
    "\n",
    "                    tfile = xr.open_dataset(f'{input_dir}/{files}')\n",
    "    \n",
    "                    tfile[v].to_netcdf(f'{input_dir}/{v}/{filename}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a9773-ed8f-422b-925d-c681a8172807",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_vars = variables = ['u10', 'v10', 'd2m', 't2m', 'blh', 'cape', 'cp', 'i10fg', 'lsrr', 'tp', 'swvl1']\n",
    "era_dir_maker('ERA5', infile_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466f123-e37d-496d-b092-6cd4c144b7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080f840-aebc-42c4-b412-8fc59a00cc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
